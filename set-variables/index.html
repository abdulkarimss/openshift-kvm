<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://ibm.github.io/Ansible-OpenShift-Provisioning/set-variables/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>3 Set Variables - Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "3 Set Variables";
        var mkdocs_page_input_path = "set-variables.md";
        var mkdocs_page_url = "/Ansible-OpenShift-Provisioning/set-variables/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="..">
          <img src="../images/ansible-logo.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Read Me</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../before-you-begin/">Before You Begin</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../prerequisites/">Prerequisites</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Installation Instructions</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../get-the-repository/">1 Get the repository</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../get-pull-secret/">2 Get Pull Secret</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">3 Set Variables</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run-setup-playbook/">4 Run Setup Playbook</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run-the-playbooks/">5 Run the Playbooks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../first-time-login/">6 First-Time Login</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Misc</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../teardown/">Teardown</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../CHANGELOG/">Change Log</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../acknowledgements/">Acknowledgements</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Installation Instructions &raquo;</li><li>3 Set Variables</li>
    <li class="wy-breadcrumbs-aside">
        <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning/edit/main/docs/set-variables.md"
          class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="step-3-set-variables">Step 3: Set Variables<a class="headerlink" href="#step-3-set-variables" title="Permanent link">#</a></h1>
<ul>
<li>In a text editor of your choice, open the <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning/blob/main/inventories/default/group_vars/all.yaml">environment variables file</a>.</li>
<li>This is the master variables file and you will likely reference it many times throughout the process. The default inventory can be found at <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning/blob/main/inventories/default/group_vars/all.yaml">inventories/default/group_vars/all.yaml</a>.</li>
<li>Fill out the variables marked with <code>X</code> to match your specific installation. </li>
<li>This is the most important step in the process. Take the time to make sure everything here is correct.</li>
<li><u>Note on YAML syntax</u>: Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file env and z don't need to be filled in, but the cpc_name does. There are X's where input is required to help you with this.</li>
<li>Scroll table to the right to see examples for each variable.</li>
</ul>
<table>
<thead>
<tr>
<th align="left"><strong>Variable Name</strong></th>
<th align="left"><strong>Description</strong></th>
<th align="left"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong><u>Section 1 - IBM zSystems</u></strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>env.z.cpc_name</strong></td>
<td align="left">The name of the IBM zSystems / LinuxONE mainframe that you are creating a Red Hat OpenShift Container<br /> Platform cluster on. Can be found under the "Systems Management" tab of the Hardware Management<br /> Console (HMC).</td>
<td align="left">SYS1</td>
</tr>
<tr>
<td align="left"><strong>env.z.hmc.host</strong></td>
<td align="left">The IPv4 address of the HMC you will be connecting to in order to create a Logical Partition (LPAR)<br /> on which will act as the Kernel-based Virtual Machine (KVM) host aftering installing and setting up<br /> Red Hat Enterprise Linux (RHEL).</td>
<td align="left">192.168.10.1</td>
</tr>
<tr>
<td align="left"><strong>env.z.hmc.user</strong></td>
<td align="left">The username that the HMC API call will use to connect to the HMC. Must have access to create<br /> LPARs, attach storage groups and networking cards.</td>
<td align="left">hmc-user</td>
</tr>
<tr>
<td align="left"><strong>env.z.hmc.pass</strong></td>
<td align="left">The password that the HMC API call will use to connect to the HMC. Must have access to create<br /> LPARs, attach storage groups and networking cards.</td>
<td align="left">hmcPas$w0rd!</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.name</strong></td>
<td align="left">The name of the Logical Partition (LPAR) that you would like to create/target for the creation of<br /> your cluster. This LPAR will act as the KVM host, with RHEL installed natively.</td>
<td align="left">OCPKVM1</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.description</strong></td>
<td align="left">A short description of what this LPAR will be used for, will only be displayed in the HMC next to<br /> the LPAR name for identification purposes.</td>
<td align="left">KVM host LPAR for RHOCP cluster.</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.access.user</strong></td>
<td align="left">The username that will be created in RHEL when it is installed on the LPAR (the KVM host).</td>
<td align="left">kvm-admin</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.access.pass</strong></td>
<td align="left">The password for the user that will be created in RHEL when it is installed on the LPAR (the KVM host).</td>
<td align="left">ch4ngeMe!</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.root_pass</strong></td>
<td align="left">The root password for RHEL installed on the LPAR (the KVM host).</td>
<td align="left">$ecureP4ass!</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.ifl.count</strong></td>
<td align="left">Number of Integrated Facilities for Linux (IFL) processors will be assigned to this LPAR.<br /> 6 or more recommended.</td>
<td align="left">6</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.ifl.initial memory</strong></td>
<td align="left">Initial memory allocation for LPAR to have at start-up (in megabytes).</td>
<td align="left">55000</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.ifl.max_memory</strong></td>
<td align="left">The most amount of memory this LPAR can be using at any one time (in megabytes).</td>
<td align="left">99000</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.ifl.initial_weight</strong></td>
<td align="left">For LPAR load balancing purposes, the processing weight this LPAR will have at start-up (1-999).</td>
<td align="left">100</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.ifl.min_weight</strong></td>
<td align="left">For LPAR load balancing purposes, the minimum weight that this LPAR can have at any one time (1-999).</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.ifl.max_weight</strong></td>
<td align="left">For LPAR load balancing purposes, the maximum weight that this LPAR can have at any one time (1-999).</td>
<td align="left">500</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.hostname</strong></td>
<td align="left">The hostname of the LPAR with RHEL installed natively (the KVM host).</td>
<td align="left">kvm-host-01</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.ip</strong></td>
<td align="left">The IPv4 address of the LPAR with RHEL installed natively (the KVM host).</td>
<td align="left">192.168.10.2</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.subnetmask</strong></td>
<td align="left">The subnet that the LPAR resides in within your network.</td>
<td align="left">255.255.255.0</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.subnet</strong></td>
<td align="left">The same value as the above variable but in Classless Inter-Domain Routing (CIDR) notation.</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.gateway</strong></td>
<td align="left">The IPv4 address of the gateway to the network where the KVM host resides.</td>
<td align="left">192.168.10.0</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nameserver1</strong></td>
<td align="left">The IPv4 address from which the KVM host gets its hostname resolved.</td>
<td align="left">192.168.10.200</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nameserver2</strong></td>
<td align="left">(Optional) A second IPv4 address from which the KVM host can get its hostname<br /> resolved. Used for high availability.</td>
<td align="left">192.168.10.200</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.device1</strong></td>
<td align="left">The network interface card from Linux's perspective. Usually enc and then a number that comes<br /> from the dev_num of the network adapter.</td>
<td align="left">enc100</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.device2</strong></td>
<td align="left">(Optional) Another Linux network interface card. Usually enc and then a number that comes<br /> from the dev_num of the second network adapter.</td>
<td align="left">enc1</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card1.name</strong></td>
<td align="left">The logical name of the Network Interface Card (NIC) within the HMC. An arbitrary value<br /> that is human-readable<br /> that points to the NIC.</td>
<td align="left">SYS-NIC-01</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card1.adapter</strong></td>
<td align="left">The physical adapter name reference to the logical adapter for the LPAR.</td>
<td align="left">10Gb-A</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card1.port</strong></td>
<td align="left">The port number for the NIC.</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card1.dev_num</strong></td>
<td align="left">The logical device number for the NIC. In hex format.</td>
<td align="left">0x0100</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card2.name</strong></td>
<td align="left">(Optional, uncomment and fill-in if you want to attach a second storage group) The logical name of a<br /> second Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable<br /> that points to the NIC.</td>
<td align="left">SYS-NIC-02</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card2.adapter</strong></td>
<td align="left">(Optional, uncomment and fill-in if you want to attach a second storage group) The physical adapter<br /> name of a second NIC.</td>
<td align="left">10Gb-B</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card2.port</strong></td>
<td align="left">(Optional, uncomment and fill-in if you want to attach a second storage group) The port<br /> number for a second NIC.</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.networking.nic.card2.dev_num</strong></td>
<td align="left">(Optional, uncomment and fill-in if you want to attach a second storage group) The logical device<br /> number for a second NIC. In hex format.</td>
<td align="left">0x0001</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.storage_group.name</strong></td>
<td align="left">The name of the storage group that will be attached to the LPAR.</td>
<td align="left">OCP-storage-01</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.storage_group.type</strong></td>
<td align="left">Storage type. FCP is the only tested type as of now.</td>
<td align="left">fcp</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.storage_group.pool_path</strong></td>
<td align="left">Set the absolute path to the storage pool within Linux. Recommended /var/lib/libvirt/images</td>
<td align="left">/var/lib/libvirt/images</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.storage_group.storage_wwpn</strong></td>
<td align="left">World-wide port numbers for storage group. Use provided list formatting.</td>
<td align="left">500708680235c3f0<br />500708680235c3f1<br />500708680235c3f2<br />500708680235c3f3</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.storage_group.dev_num</strong></td>
<td align="left">The logical device number of the Host Bus Adapter (HBA) for the storage group.</td>
<td align="left">C001</td>
</tr>
<tr>
<td align="left"><strong>env.z.lpar.storage_group.lun_name</strong></td>
<td align="left">A list of Logical Unit Numbers (LUN) that point to specific virtual disk behind the WWPN. First in<br /> list will be used for boot.</td>
<td align="left">mpatha<br />mpathb<br />mpathc<br />mpathd</td>
</tr>
<tr>
<td align="left"><strong><u>Section 2 - FTP</u></strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>env.ftp.ip</strong></td>
<td align="left">IP address for the FTP server that will be used to pass config files and iso to KVM host LPAR and bastion<br /> VM during their first boot.</td>
<td align="left">192.168.10.201</td>
</tr>
<tr>
<td align="left"><strong>env.ftp.user</strong></td>
<td align="left">Username to connect to the FTP server.</td>
<td align="left">ftp-user</td>
</tr>
<tr>
<td align="left"><strong>env.ftp.pass</strong></td>
<td align="left">Password to connect to the FTP server as above user.</td>
<td align="left">FTPpa$s!</td>
</tr>
<tr>
<td align="left"><strong>env.ftp.iso_mount_dir</strong></td>
<td align="left">Directory path relative to FTP root where RHEL ISO is mounted. If FTP root is /var/ftp/pub<br /> and the ISO is mounted at /var/ftp/pub/RHEL/8.5 then this variable would be<br /> RHEL/8.5. No slash before or after.</td>
<td align="left">RHEL/8.5</td>
</tr>
<tr>
<td align="left"><strong>env.ftp.iso_mount_dir</strong></td>
<td align="left">Directory path relative to FTP root where configuration files can be stored. If FTP root is /var/ftp/pub<br /> and you would like to store the configs at /var/ftp/pub/ocpz-config then this variable would be<br /> ocpz-config. No slash before or after.</td>
<td align="left">ocpz-config</td>
</tr>
<tr>
<td align="left"><strong><u>Section 3 - RedHat</u></strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>env.redhat.username</strong></td>
<td align="left">Red Hat username with a valid license or free trial to Red Hat OpenShift Container Platform (RHOCP),<br /> which comes with necessary licenses for Red Hat Enterprise Linux (RHEL) and Red Hat CoreOS (RHCOS).</td>
<td align="left">redhat.user</td>
</tr>
<tr>
<td align="left"><strong>env.redhat.password</strong></td>
<td align="left">Password to Red Hat above user's account. Used to auto-attach necessary subscriptions to KVM Host,<br /> bastion VM, and pull live images for OpenShift.</td>
<td align="left">rEdHatPa$s!</td>
</tr>
<tr>
<td align="left"><strong>env.redhat.pull_secret</strong></td>
<td align="left">Pull secret for OpenShift, comes from Red Hat's <a href="https://console.redhat.com/openshift/install/ibmz/user-provisioned">Hybrid Cloud Console</a>. Make sure to enclose in 'single quotes'.<br /></td>
<td align="left">'{"auths":{"cloud.openshift<br />.com":{"auth":"b3Blb<br />...<br />4yQQ==","email":"redhat.<br />user@gmail.com"}}}'</td>
</tr>
<tr>
<td align="left"><strong><u>Section 4 - Bastion</u></strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>env.bastion.create</strong></td>
<td align="left">Would you like to create a bastion KVM guest to host essential infrastructure services like DNS,<br /> load balancer, firewall, etc? Highly recommended. Can de-select certain services with the env.bastion.options<br /> variables below. True or False (boolean).</td>
<td align="left">True</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.vm_name</strong></td>
<td align="left">Name of the bastion VM. Arbitrary value.</td>
<td align="left">bastion</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.resources.disk_size</strong></td>
<td align="left">How much of the storage pool would you like to allocate to the bastion (in<br /> Gigabytes)? Recommended 30 or more.</td>
<td align="left">30</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.resources.ram</strong></td>
<td align="left">How much memory would you like to allocate the bastion (in<br /> megabytes)? Recommended 4096 or more</td>
<td align="left">4096</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.resources.swap</strong></td>
<td align="left">How much swap storage would you like to allocate the bastion (in<br /> megabytes)? Recommended 4096 or more.</td>
<td align="left">4096</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.resources.vcpu</strong></td>
<td align="left">How many virtual CPUs would you like to allocate to the bastion? Recommended 4 or more.</td>
<td align="left">4</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.resources.os_variant</strong></td>
<td align="left">Version of Red Hat Enterprise Linux to use for the bastion's operating system.<br /> Recommended 8.4 and above. Must match version of mounted ISO on the FTP server.</td>
<td align="left">8.5</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.ip</strong></td>
<td align="left">IPv4 address for the bastion.</td>
<td align="left">192.168.10.3</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.hostname</strong></td>
<td align="left">Hostname of the bastion. Will be combined with<br /> env.bastion.networking.base_domain to create a Fully Qualified Domain Name (FQDN).</td>
<td align="left">ocpz-bastion</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.subnetmask</strong></td>
<td align="left">Subnet of the bastion.</td>
<td align="left">255.255.255.0</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.gateway</strong></td>
<td align="left">What will be the IPv4 address of the bastion's gateway server?</td>
<td align="left">192.168.10.0</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.nameserver1</strong></td>
<td align="left">IPv4 address of the server that resolves the bastion's hostname.</td>
<td align="left">192.168.10.200</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.nameserver2</strong></td>
<td align="left">(Optional) A second IPv4 address that resolves the bastion's hostname.</td>
<td align="left">192.168.10.201</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.interface</strong></td>
<td align="left">Name of the networking interface on the bastion from Linux's perspective. Most likely enc1.</td>
<td align="left">enc1</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.networking.base_domain</strong></td>
<td align="left">Base domain that, when combined with the hostname, creates a fully-qualified<br /> domain name (FQDN) for the bastion?</td>
<td align="left">ihost.com</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.access.user</strong></td>
<td align="left">What would you like the admin's username to be on the bastion?</td>
<td align="left">admin</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.access.pass</strong></td>
<td align="left">The password to the bastion's admin user.</td>
<td align="left">cH4ngeM3!</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.access.root_pass</strong></td>
<td align="left">The root password for the bastion.</td>
<td align="left">R0OtPa$s!</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.access.ocp_ssh_key_comment</strong></td>
<td align="left">Comment to describe the SSH key used for OCP. Arbitrary value.</td>
<td align="left">OCPZ-01 key</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.options.dns</strong></td>
<td align="left">Would you like the bastion to host the DNS information for the cluster? True or False (boolean).<br /> If false, resolution must come from elsewhere in your environment. Make sure to add IP addresses for<br /> KVM hosts, bastion, bootstrap, control, compute nodes, AND api, api-int and *.apps as described <a href="https://docs.openshift.com/container-platform/4.8/installing/installing_bare_metal/installing-bare-metal-network-customizations.html">here</a><br /> in section "User-provisioned DNS Requirements" Table 5. If True this will be done for<br /> you in the dns and check_dns roles.</td>
<td align="left">True</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.options.loadbalancer.on_bastion</strong></td>
<td align="left">Would you like the bastion to host the load balancer (HAProxy) for the cluster?<br /> True or False (boolean).<br /> If false, this service must be provided elsewhere in your environment, and public and<br /> private IP of the load balancer must be<br /> provided in the following two variables.</td>
<td align="left">True</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.options.loadbalancer.public_ip</strong></td>
<td align="left">(Only required if env.bastion.options.loadbalancer.on_bastion is True). The public IPv4<br /> address for your environment's loadbalancer. api, apps, *.apps must use this.</td>
<td align="left">192.168.10.50</td>
</tr>
<tr>
<td align="left"><strong>env.bastion.options.loadbalancer.private_ip</strong></td>
<td align="left">(Only required if env.bastion.options.loadbalancer.on_bastion is True). The private IPv4 address<br /> for your environment's loadbalancer. api-int must use this.</td>
<td align="left">10.24.17.12</td>
</tr>
<tr>
<td align="left"><strong><u>Section 5 - Cluster</u></strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>env.cluster.networking.metadata_name</strong></td>
<td align="left">Name to describe the cluster as a whole, can be anything if DNS will be hosted on the bastion. If<br /> DNS is not on the bastion, must match your DNS configuration. Will be combined with the base_domain<br /> and hostnames to create Fully Qualified Domain Names (FQDN).</td>
<td align="left">ocpz</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.networking.base_domain</strong></td>
<td align="left">The site name, where is the cluster being hosted? This will be combined with the metadata_name<br /> and hostnames to create FQDNs.</td>
<td align="left">ihost.com</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.networking.nameserver1</strong></td>
<td align="left">IPv4 address that the cluster get its hostname resolution from. If env.bastion.options.dns<br /> is True, this should be the IP address of the bastion.</td>
<td align="left">192.168.10.200</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.networking.nameserver2</strong></td>
<td align="left">(Optional) A second IPv4 address will the cluster get its hostname resolution from? If env.bastion.options.dns<br /> is True, this should be left commented out.</td>
<td align="left">192.168.10.201</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.networking.forwarder</strong></td>
<td align="left">What IPv4 address will be used to make external DNS calls? Can use 1.1.1.1 or 8.8.8.8 as defaults.</td>
<td align="left">8.8.8.8</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.bootstrap.disk_size</strong></td>
<td align="left">How much disk space do you want to allocate to the bootstrap node (in Gigabytes)? Bootstrap node<br /> is temporary and will be brought down automatically when its job completes. 120 or more recommended.</td>
<td align="left">120</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.bootstrap.ram</strong></td>
<td align="left">How much memory would you like to allocate to the temporary bootstrap node (in<br /> megabytes)? Recommended 16384 or more.</td>
<td align="left">16384</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.bootstrap.vcpu</strong></td>
<td align="left">How many virtual CPUs would you like to allocate to the temporary bootstrap node?<br /> Recommended 4 or more.</td>
<td align="left">4</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.bootstrap.vm_name</strong></td>
<td align="left">Name of the temporary bootstrap node VM. Arbitrary value.</td>
<td align="left">bootstrap</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.bootstrap.ip</strong></td>
<td align="left">IPv4 address of the temporary bootstrap node.</td>
<td align="left">192.168.10.4</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.bootstrap.hostname</strong></td>
<td align="left">Hostname of the temporary boostrap node. If DNS is hosted on the bastion, this can be anything.<br /> If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the<br /> metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN).</td>
<td align="left">bootstrap-ocpz</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.control.disk_size</strong></td>
<td align="left">How much disk space do you want to allocate to each control node (in Gigabytes)? 120 or more recommended.</td>
<td align="left">120</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.control.ram</strong></td>
<td align="left">How much memory would you like to allocate to the each control<br /> node (in megabytes)? Recommended 16384 or more.</td>
<td align="left">16384</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.control.vcpu</strong></td>
<td align="left">How many virtual CPUs would you like to allocate to each control node? Recommended 4 or more.</td>
<td align="left">4</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.control.vm_name</strong></td>
<td align="left">Name of the control node VMs. Arbitrary values. Usually no more or less than 3 are used. Must match<br /> the total number of IP addresses and hostnames for control nodes. Use provided list format.</td>
<td align="left">control-1<br />control-2<br />control-3</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.control.ip</strong></td>
<td align="left">IPv4 address of the control nodes. Use provided<br /> list formatting.</td>
<td align="left">192.168.10.5<br />192.168.10.6<br />192.168.10.7</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.control.hostname</strong></td>
<td align="left">Hostnames for control nodes. Must match the total number of IP addresses for control nodes<br /> (usually 3). If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere,<br /> this must match DNS definition. This will be combined with the metadata_name and<br /> base_domain to create a Fully Qualififed Domain Name (FQDN).</td>
<td align="left">control-01<br />control-02<br />control-03</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.compute.disk_size</strong></td>
<td align="left">How much disk space do you want to allocate to each compute<br /> node (in Gigabytes)? 120 or more recommended.</td>
<td align="left">120</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.compute.ram</strong></td>
<td align="left">How much memory would you like to allocate to the each compute<br /> node (in megabytes)? Recommended 16384 or more.</td>
<td align="left">16384</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.compute.vcpu</strong></td>
<td align="left">How many virtual CPUs would you like to allocate to each compute node? Recommended 2 or more.</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.compute.vm_name</strong></td>
<td align="left">Name of the compute node VMs. Arbitrary values. This list can be expanded to any<br /> number of nodes, minimum 2. Must match the total number of IP<br /> addresses and hostnames for compute nodes. Use provided list format.</td>
<td align="left">compute-1<br />compute-2</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.compute.ip</strong></td>
<td align="left">IPv4 address of the compute nodes. Must match the total number of VM names and<br /> hostnames for compute nodes. Use provided list formatting.</td>
<td align="left">192.168.10.8<br />192.168.10.9</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.compute.hostname</strong></td>
<td align="left">Hostnames for compute nodes. Must match the total number of IP addresses and<br /> VM names for compute nodes. If DNS is hosted on the bastion, this can be anything.<br /> If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the<br /> metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN).</td>
<td align="left">compute-01<br />compute-02</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.infra.disk_size</strong></td>
<td align="left">(Optional) Set up compute nodes that are made for infrastructure workloads (ingress,<br /> monitoring, logging)? How much disk space do you want to allocate to each infra node (in Gigabytes)?<br /> 120 or more recommended.</td>
<td align="left">120</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.infra.ram</strong></td>
<td align="left">(Optional) How much memory would you like to allocate to the each infra node (in<br /> megabytes)? Recommended 16384 or more.</td>
<td align="left">16384</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.infra.vcpu</strong></td>
<td align="left">(Optional) How many virtual CPUs would you like to allocate to each infra node?<br /> Recommended 2 or more.</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.infra.vm_name</strong></td>
<td align="left">(Optional) Name of additional infra node VMs. Arbitrary values. This list can be<br /> expanded to any number of nodes, minimum 2. Must match the total<br /> number of IP addresses and hostnames for infra nodes. Use provided list format.</td>
<td align="left">infra-1<br />infra-2</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.infra.ip</strong></td>
<td align="left">(Optional) IPv4 address of the infra nodes. This list can be expanded to any number of nodes,<br /> minimum 2. Use provided list formatting.</td>
<td align="left">192.168.10.8<br />192.168.10.9</td>
</tr>
<tr>
<td align="left"><strong>env.cluster.nodes.infra.hostname</strong></td>
<td align="left">(Optional) Hostnames for infra nodes. Must match the total number of IP addresses for infra nodes.<br /> If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match<br /> DNS definition. This will be combined with the metadata_name and base_domain<br /> to create a Fully Qualififed Domain Name (FQDN).</td>
<td align="left">infra-01<br />infra-02</td>
</tr>
<tr>
<td align="left"><strong><u>Section 6 - Misc Optional Settings</u></strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>env.language</strong></td>
<td align="left">What language would you like Red Hat Enterprise Linux to use? In UTF-8 language code.<br /> Available languages and their corresponding codes can be found <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html-single/international_language_support_guide/index">here</a>, in the "Locale" column of Table 2.1.</td>
<td align="left">en_US.UTF-8</td>
</tr>
<tr>
<td align="left"><strong>env.timezone</strong></td>
<td align="left">Which timezone would you like Red Hat Enterprise Linux to use? A list of available timezone<br /> options can be found <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">here</a>.</td>
<td align="left">America/New_York</td>
</tr>
<tr>
<td align="left"><strong>env.ansible_key_name</strong></td>
<td align="left">(Optional) Name of the SSH key that Ansible will use to connect to hosts.</td>
<td align="left">ansible-ocpz</td>
</tr>
<tr>
<td align="left"><strong>env.bridge_name</strong></td>
<td align="left">(Optional) Name of the macvtap bridge that will be created on the KVM host.</td>
<td align="left">macvtap-net</td>
</tr>
<tr>
<td align="left"><strong>env.pkgs.galaxy</strong></td>
<td align="left">A list of Ansible Galaxy collections that will be installed during the setup playbook. The<br /> collections listed are required. Feel free to add more as needed, just make sure to follow the same list format.</td>
<td align="left">community.general</td>
</tr>
<tr>
<td align="left"><strong>env.pkgs.workstation</strong></td>
<td align="left">A list of packages that will be installed on the workstation running Ansible during the setup<br /> playbook. Feel free to add more as needed, just make sure to follow the same list format.</td>
<td align="left">openssh</td>
</tr>
<tr>
<td align="left"><strong>env.pkgs.kvm</strong></td>
<td align="left">A list of packages that will be installed on the KVM Host during the setup_kvm_host playbook.<br /> Feel free to add more as needed, just make sure to follow the same list format.</td>
<td align="left">qemu-kvm</td>
</tr>
<tr>
<td align="left"><strong>env.pkgs.bastion</strong></td>
<td align="left">A list of packages that will be installed on the bastion during the setup_bastion playbook.<br /> Feel free to add more as needed, just make sure to follow the same list format.</td>
<td align="left">haproxy</td>
</tr>
<tr>
<td align="left"><strong>env.openshift.client</strong></td>
<td align="left">Link to the mirror for the OpenShift client from Red Hat. Feel free to change to a different version, but <br /> make sure it is for s390x architecture.<br /> Also make sure the client, installer and CoreOS versions (below) match.</td>
<td align="left">https://mirror.openshift.com<br />/pub/openshift-v4/s390x/clients<br />/ocp/stable/openshift-<br />client-linux.tar.gz</td>
</tr>
<tr>
<td align="left"><strong>env.openshift.installer</strong></td>
<td align="left">Link to the mirror for the OpenShift installer from Red Hat.<br /> Feel free to change to a different version, but make sure it is for s390x architecture.<br /> Also make sure the client, installer and CoreOS versions (below) match.</td>
<td align="left">https://mirror.openshift.com<br />/pub/openshift-v4/s390x<br />/clients/ocp/stable/openshift-<br />install-linux.tar.gz</td>
</tr>
<tr>
<td align="left"><strong>env.coreos.kernel</strong></td>
<td align="left">Link to the mirror of the CoreOS kernel to be used for the bootstrap, control and compute nodes.<br /> Feel free to change to a different version, but make sure it is for s390x architecture.<br /> Also make sure the OCP client and installer (above) and other CoreOS components versions match.</td>
<td align="left">https://mirror.openshift.com<br />/pub/openshift-v4/s390x<br />/dependencies/rhcos<br />/4.9/latest/rhcos-4.9.0-s390x-<br />live-kernel-s390x</td>
</tr>
<tr>
<td align="left"><strong>env.coreos.initramfs</strong></td>
<td align="left">Link to the mirror of the CoreOS initramfs to be used for the bootstrap, control and compute nodes.<br /> Feel free to change to a different version, but make sure it is for s390x architecture.<br /> Also make sure the OCP client and installer (above) and other CoreOS components versions match.</td>
<td align="left">https://mirror.openshift.com<br />/pub/openshift-v4<br />/s390x/dependencies/rhcos<br />/4.9/latest/rhcos-4.9.0-s390x-<br />live-initramfs.s390x.img</td>
</tr>
<tr>
<td align="left"><strong>env.coreos.rootfs</strong></td>
<td align="left">Link to the mirror of the CoreOS rootfs to be used for the bootstrap, control and compute nodes.<br /> Feel free to change to a different version, but make sure it is for s390x architecture.<br /> Also make sure the OCP client and installer (above) and other CoreOS components versions match.</td>
<td align="left">https://mirror.openshift.com<br />/pub/openshift-v4<br />/s390x/dependencies/rhcos<br />/4.9/latest/rhcos-4.9.0-<br />s390x-live-rootfs.s390x.img</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.api_version</strong></td>
<td align="left">Kubernetes API version for the cluster. These install_config variables will be passed to the OCP<br /> install_config file. This file is templated in the get_ocp role during the setup_bastion playbook.<br /> To make more fine-tuned adjustments to the install_config, you can find it at<br /> roles/get_ocp/templates/install-config.yaml.j2</td>
<td align="left">v1</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.compute.architecture</strong></td>
<td align="left">Computing architecture for the compute nodes. Must be s390x for clusters on IBM zSystems.</td>
<td align="left">s390x</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.compute.hyperthreading</strong></td>
<td align="left">Enable or disable hyperthreading on compute nodes. Recommended enabled.</td>
<td align="left">Enabled</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.control.architecture</strong></td>
<td align="left">Computing architecture for the control nodes. Must be s390x for clusters on IBM zSystems.</td>
<td align="left">s390x</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.control.hyperthreading</strong></td>
<td align="left">Enable or disable hyperthreading on control nodes. Recommended enabled.</td>
<td align="left">Enabled</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.cluster_network.cidr</strong></td>
<td align="left">IPv4 block in Internal cluster networking in Classless Inter-Domain<br /> Routing (CIDR) notation. Recommended to keep as is.</td>
<td align="left">10.128.0.0/14</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.cluster_network.host_prefix</strong></td>
<td align="left">The subnet prefix length to assign to each individual node. For example, if<br /> hostPrefix is set to 23 then each node is assigned a /23 subnet out of the given cidr. A hostPrefix<br /> value of 23 provides 510 (2^(32 - 23) - 2) pod IP addresses.</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.cluster_network.type</strong></td>
<td align="left">The cluster network provider Container Network Interface (CNI) plug-in to install.<br /> Either OpenShiftSDN (recommended) or OVNKubernetes.</td>
<td align="left">OpenShiftSDN</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.service_network</strong></td>
<td align="left">The IP address block for services. The default value is 172.30.0.0/16. The OpenShift SDN<br /> and OVN-Kubernetes network providers support only a single IP address block for the service<br /> network. An array with an IP address block in CIDR format.</td>
<td align="left">172.30.0.0/16</td>
</tr>
<tr>
<td align="left"><strong>env.install_config.fips</strong></td>
<td align="left">True or False (boolean) for whether or not to use the United States' Federal Information Processing<br /> Standards (FIPS). Not yet certified on IBM zSystems. Enclosed in 'single quotes'.</td>
<td align="left">'false'</td>
</tr>
<tr>
<td align="left"><strong>proxy_env.http_proxy</strong></td>
<td align="left">(Optional) A proxy URL to use for creating HTTP connections outside the cluster. Will be<br /> used in the install-config and applied to other Ansible hosts unless set otherwise in<br /> no_proxy below. Must follow this pattern: http://usernamepswd&gt;@ip:port</td>
<td align="left">http://ocp-admin:Pa$sw0rd@9.72.10.1:80</td>
</tr>
<tr>
<td align="left"><strong>proxy_env.https_proxy</strong></td>
<td align="left">(Optional) A proxy URL to use for creating HTTPS connections outside the cluster. Will be<br /> used in the install-config and applied to other Ansible hosts unless set otherwise in<br /> no_proxy below. Must follow this pattern: https://username:pswd@ip:port</td>
<td align="left">https://ocp-admin:Pa$sw0rd@9.72.10.1:80</td>
</tr>
<tr>
<td align="left"><strong>proxy_env.no_proxy</strong></td>
<td align="left">(Optional) A comma-separated list of destination domain names, IP addresses, or other<br /> network CIDRs to exclude from proxying. Preface a domain with . to match subdomains only.<br /> For example, .y.com matches x.y.com, but not y.com. Use * to bypass the proxy for all listed destinations.</td>
<td align="left">example.com, 192.168.10.1</td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../get-pull-secret/" class="btn btn-neutral float-left" title="2 Get Pull Secret"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../run-setup-playbook/" class="btn btn-neutral float-right" title="4 Run Setup Playbook">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright © 2022 IBM zSystems Washington Systems Center</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../get-pull-secret/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../run-setup-playbook/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
