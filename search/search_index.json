{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE # Overview # These Ansible Playbooks automate the setup and deployment of a Red Hat OpenShift Container Platform (RHOCP) cluster on IBM zSystems / LinuxONE with Kernel Virtual Machine (KVM) as the hypervisor. Ready to Start? # Use the left-hand panel to navigate the site. Start with the Before You Begin page. Need Help? # Contact Jacob Emery at jacob.emery@ibm.com","title":"Home"},{"location":"#ansible-automated-openshift-provisioning-on-kvm-on-ibm-zsystems-linuxone","text":"","title":"Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE"},{"location":"#overview","text":"These Ansible Playbooks automate the setup and deployment of a Red Hat OpenShift Container Platform (RHOCP) cluster on IBM zSystems / LinuxONE with Kernel Virtual Machine (KVM) as the hypervisor.","title":"Overview"},{"location":"#ready-to-start","text":"Use the left-hand panel to navigate the site. Start with the Before You Begin page.","title":"Ready to Start?"},{"location":"#need-help","text":"Contact Jacob Emery at jacob.emery@ibm.com","title":"Need Help?"},{"location":"CHANGELOG/","text":"Changelog # All notable changes to this project will be documented in this file. Roadmap # Add option to use a VPN to reduce # of IPs needed Add the ability to provision multiple LPARs for high availability Tag infrastructure nodes for specific operators Add air-gapped (disconnected) install option Add option for OpenShift to use a proxy server Add README\u2019s for each role Make ssh-copy-id role idempotent Documentation Overhaul # Version 0.3.1 Released: 2022-06-03 Summary # Moved documentation to GitHub Pages to be more reader-friendly. Automated KVM Host Provisioning # Version 0.3.0 Released: 2022-03-26 Summary # Now able to provision the KVM host via Ansible. Changed the structure of playbooks, variables, and inventories to use Ansible best practices. Added # Support for using IBM's zHMC Ansible modules to automate the creation of a logical partition (LPAR) profile, connect storage group and network card, boot from an FTP server, and then kickstart the installation of RHEL to serve as the KVM hypervisor for the cluster. Usage of Ansible vault to encrypt sensitive data. Playbooks must now be run with --ask-vault-pass, e.g. 'ansible-playbook playbooks/site.yaml --ask-vault-pass' Modified # Bastion boot method from cloud-init to FTP and kickstart. The structure of playbooks. The setup.yaml playbook still must be run before anything else, but now there is a master playbook - site.yaml which imports all other playbooks. This was done to be more user-friendly and in-line with best practices. Previously, everything was all in one playbook and relied on tags to start back from a given point. Relying solely on tags proved tedious. The structure for inventories, which allows for more flexibility with deployments and is more in-line with best practices. Now you can have multiple inventories and specify which you would like to use for a given run in the ansible.cfg file. The structure of variables, to allow for the separation of the bastion node from the rest of the cluster. This opens up many more possibilities for more complex deployments where, for example, the bastion node is already created. Infrastructure Nodes, Extra Apps, Security # Version: 0.2.1 Released: 2022-01-06 Summary # Now able to designate compute nodes as infrastructure nodes, and create optional RHEL VMs for additional non-cluster applications running on the KVM host. Made changes to SSH and SELinux tasks to be more secure. Added # Support for creating infrastructure nodes and extra apps. Added tcp port 53 to firewall. Setting of permissions and ownership of important configuration files to bastion admin user instead of root. Wheel to groups that bastion admin user is added to on boot. More rounds of checking cluster operators and CSRs in verification steps to ensure the playbook doesn't fail if it takes a long time for those steps to complete. Task to httpd to allow port 4443 because SELinux is no longer set to permissive (see ' Removed ' below). Modified # Formatting of README file to be prettier and more useful. env.yaml to have two sections separated by a comment block: one for variables that need to be filled out, the other for pre-filled variables that can be modified if desired. Ansible user from running as root to an admin user with sudo privileges. Removed # The need to run anything as root user for security reasons. set_selinux_permissive mode role for security reasons. Scaling # Version: 0.2.0 Released: 2021-12-09 Summary # Now supports any number of control and compute nodes to be provisioned in the cluster. This update heavily modifies the variable structure in env.yaml in order to make scaling work. Added # Support for scaling of control and compute nodes. Modified # Variable structure in env.yaml in order to support scaling. Tags to match their corresponding role. Every reference to a variable from env.yaml to match the new structure. Automated OCP Verification # Version: 0.1.1 Released: 2021-12-03 Summary # Fully automated all OCP verification steps. Cutting the number of steps nearly in half. The main playbook can now run completely hands-off from kicking it off all the way to an operational cluster. The last step provides the first-time login credentials. Added # 5 roles related to automating OCP verification steps: wait_for_bootstrap, approve_certs, check_nodes, wait_for_cluster_operators, and wait_for_install_complete. Role to check internal and external DNS configuration before continuing. Including checking to make sure the name resolves to the correct IP address. Modified # The mirrors for CoreOS versions to update to 4.9 and tested them. The acquisition method of RHEL qcow2 from downloading via ephemeral link to having the user download the file to their local machine as a pre-req. This was changed to avoid having to re-copy the link every time it expires. teardown.yaml and reset_files role to be fully idempotent when running the main playbook from the point where each type of teardown sets the user back to. Lots of small tweaks. Removed # Instructions in README for doing OCP verification steps manually Automated Bastion Install # Version: 0.1.0 Released: 2021-11-24 Summary # Fully automated bastion installation and configuration using cloud-init Added # Options in env.yaml for creating a DNS server on the bastion or not, and for automatically attaching Red Hat subscriptions Variables for bootstrap, bastion, control and compute nodes' specifications in env.yaml Node name variables in env.yaml Variable for network interface name in env.yaml Variable for DNS forwarder in env.yaml Templating of DNS configuration files so they don't have to be pre-provided Expect script to ssh_copy_id role so that the user doesn't have to type in ssh password when copying ssh key Templating of haproxy config file A boot_teardown tag in teardown.yaml to automate the teardown of bootstrap node Modified # create_bastion role to use cloud-init to fully automate configuration and installation of the bastion node teardown.yaml script to decrease complexity and work faster. Some tags to match their corresponding role names Lots of small improvements and tweaks Removed # Encryption of env.yaml as it was unnecessary and increased complexity First Working Build # Version: 0.0.1 Released: 2021-08-24 Initial Commit # Version: 0.0.0 Released: 2021-06-11","title":"Change Log"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file.","title":"Changelog"},{"location":"CHANGELOG/#roadmap","text":"Add option to use a VPN to reduce # of IPs needed Add the ability to provision multiple LPARs for high availability Tag infrastructure nodes for specific operators Add air-gapped (disconnected) install option Add option for OpenShift to use a proxy server Add README\u2019s for each role Make ssh-copy-id role idempotent","title":"Roadmap"},{"location":"CHANGELOG/#documentation-overhaul","text":"Version 0.3.1 Released: 2022-06-03","title":"Documentation Overhaul"},{"location":"CHANGELOG/#summary","text":"Moved documentation to GitHub Pages to be more reader-friendly.","title":"Summary"},{"location":"CHANGELOG/#automated-kvm-host-provisioning","text":"Version 0.3.0 Released: 2022-03-26","title":"Automated KVM Host Provisioning"},{"location":"CHANGELOG/#summary_1","text":"Now able to provision the KVM host via Ansible. Changed the structure of playbooks, variables, and inventories to use Ansible best practices.","title":"Summary"},{"location":"CHANGELOG/#added","text":"Support for using IBM's zHMC Ansible modules to automate the creation of a logical partition (LPAR) profile, connect storage group and network card, boot from an FTP server, and then kickstart the installation of RHEL to serve as the KVM hypervisor for the cluster. Usage of Ansible vault to encrypt sensitive data. Playbooks must now be run with --ask-vault-pass, e.g. 'ansible-playbook playbooks/site.yaml --ask-vault-pass'","title":"Added"},{"location":"CHANGELOG/#modified","text":"Bastion boot method from cloud-init to FTP and kickstart. The structure of playbooks. The setup.yaml playbook still must be run before anything else, but now there is a master playbook - site.yaml which imports all other playbooks. This was done to be more user-friendly and in-line with best practices. Previously, everything was all in one playbook and relied on tags to start back from a given point. Relying solely on tags proved tedious. The structure for inventories, which allows for more flexibility with deployments and is more in-line with best practices. Now you can have multiple inventories and specify which you would like to use for a given run in the ansible.cfg file. The structure of variables, to allow for the separation of the bastion node from the rest of the cluster. This opens up many more possibilities for more complex deployments where, for example, the bastion node is already created.","title":"Modified"},{"location":"CHANGELOG/#infrastructure-nodes-extra-apps-security","text":"Version: 0.2.1 Released: 2022-01-06","title":"Infrastructure Nodes, Extra Apps, Security"},{"location":"CHANGELOG/#summary_2","text":"Now able to designate compute nodes as infrastructure nodes, and create optional RHEL VMs for additional non-cluster applications running on the KVM host. Made changes to SSH and SELinux tasks to be more secure.","title":"Summary"},{"location":"CHANGELOG/#added_1","text":"Support for creating infrastructure nodes and extra apps. Added tcp port 53 to firewall. Setting of permissions and ownership of important configuration files to bastion admin user instead of root. Wheel to groups that bastion admin user is added to on boot. More rounds of checking cluster operators and CSRs in verification steps to ensure the playbook doesn't fail if it takes a long time for those steps to complete. Task to httpd to allow port 4443 because SELinux is no longer set to permissive (see ' Removed ' below).","title":"Added"},{"location":"CHANGELOG/#modified_1","text":"Formatting of README file to be prettier and more useful. env.yaml to have two sections separated by a comment block: one for variables that need to be filled out, the other for pre-filled variables that can be modified if desired. Ansible user from running as root to an admin user with sudo privileges.","title":"Modified"},{"location":"CHANGELOG/#removed","text":"The need to run anything as root user for security reasons. set_selinux_permissive mode role for security reasons.","title":"Removed"},{"location":"CHANGELOG/#scaling","text":"Version: 0.2.0 Released: 2021-12-09","title":"Scaling"},{"location":"CHANGELOG/#summary_3","text":"Now supports any number of control and compute nodes to be provisioned in the cluster. This update heavily modifies the variable structure in env.yaml in order to make scaling work.","title":"Summary"},{"location":"CHANGELOG/#added_2","text":"Support for scaling of control and compute nodes.","title":"Added"},{"location":"CHANGELOG/#modified_2","text":"Variable structure in env.yaml in order to support scaling. Tags to match their corresponding role. Every reference to a variable from env.yaml to match the new structure.","title":"Modified"},{"location":"CHANGELOG/#automated-ocp-verification","text":"Version: 0.1.1 Released: 2021-12-03","title":"Automated OCP Verification"},{"location":"CHANGELOG/#summary_4","text":"Fully automated all OCP verification steps. Cutting the number of steps nearly in half. The main playbook can now run completely hands-off from kicking it off all the way to an operational cluster. The last step provides the first-time login credentials.","title":"Summary"},{"location":"CHANGELOG/#added_3","text":"5 roles related to automating OCP verification steps: wait_for_bootstrap, approve_certs, check_nodes, wait_for_cluster_operators, and wait_for_install_complete. Role to check internal and external DNS configuration before continuing. Including checking to make sure the name resolves to the correct IP address.","title":"Added"},{"location":"CHANGELOG/#modified_3","text":"The mirrors for CoreOS versions to update to 4.9 and tested them. The acquisition method of RHEL qcow2 from downloading via ephemeral link to having the user download the file to their local machine as a pre-req. This was changed to avoid having to re-copy the link every time it expires. teardown.yaml and reset_files role to be fully idempotent when running the main playbook from the point where each type of teardown sets the user back to. Lots of small tweaks.","title":"Modified"},{"location":"CHANGELOG/#removed_1","text":"Instructions in README for doing OCP verification steps manually","title":"Removed"},{"location":"CHANGELOG/#automated-bastion-install","text":"Version: 0.1.0 Released: 2021-11-24","title":"Automated Bastion Install"},{"location":"CHANGELOG/#summary_5","text":"Fully automated bastion installation and configuration using cloud-init","title":"Summary"},{"location":"CHANGELOG/#added_4","text":"Options in env.yaml for creating a DNS server on the bastion or not, and for automatically attaching Red Hat subscriptions Variables for bootstrap, bastion, control and compute nodes' specifications in env.yaml Node name variables in env.yaml Variable for network interface name in env.yaml Variable for DNS forwarder in env.yaml Templating of DNS configuration files so they don't have to be pre-provided Expect script to ssh_copy_id role so that the user doesn't have to type in ssh password when copying ssh key Templating of haproxy config file A boot_teardown tag in teardown.yaml to automate the teardown of bootstrap node","title":"Added"},{"location":"CHANGELOG/#modified_4","text":"create_bastion role to use cloud-init to fully automate configuration and installation of the bastion node teardown.yaml script to decrease complexity and work faster. Some tags to match their corresponding role names Lots of small improvements and tweaks","title":"Modified"},{"location":"CHANGELOG/#removed_2","text":"Encryption of env.yaml as it was unnecessary and increased complexity","title":"Removed"},{"location":"CHANGELOG/#first-working-build","text":"Version: 0.0.1 Released: 2021-08-24","title":"First Working Build"},{"location":"CHANGELOG/#initial-commit","text":"Version: 0.0.0 Released: 2021-06-11","title":"Initial Commit"},{"location":"acknowledgements/","text":"Phillip Wilson Filipe Miranda Patrick Fruth Wasif Mohammad Stuart Tener Miao Zhang-Cohen","title":"Acknowledgements"},{"location":"before-you-begin/","text":"Before You Begin # Description # This project automates the User-Provisioned Infrastructure (UPI) method for deploying Red Hat OpenShift Container Platform (RHOCP) on IBM zSystems / LinuxONE using Kernel-based Virtual Machine (KVM) as the hypervisor. Support # This is an unofficial project created by IBMers. This installation method is not officially supported by either Red Hat or IBM. However, once installation is complete, the resulting cluster is supported by Red Hat. UPI is the only supported method for RHOCP on IBM zSystems. Difficulty # This process is much easier than doing so manually, but still not an easy task. You will likely encounter errors, but you will reach those errors quicker and understand the problem faster than if you were doing this process manually. After using these playbooks once, successive deployments will be much easier. A very basic understanding of what Ansible does is recommended. Advanced understanding is helpful for further customization of the playbooks. A basic understanding of the command-line is required. A basic understanding of git is recommended, especially for creating your organization's own fork of the repository for further customization. An advanced understanding of your computing environment is required for setting the environment variables. These Ansible Playbooks automate a User-Provisioned Infrastructure (UPI) deployment of Red Hat OpenShift Container Platform (RHOCP). This process, when done manually, is extremely tedious, time-consuming, and requires high levels of Linux AND IBM zSystems expertise. UPI is currently the only supported method for deploying RHOCP on IBM zSystems. Why Free and Open-Source? # Trust : IBM zSystems run some of the most highly-secure workloads in the world. Trust is paramount. Developing and using code transparently builds trust between developers and users, so that users feel safe using it on their highly sensitive systems. Customization : IBM zSystems exist in environments that can be highly complex and vary drastically from one datacenter to another. Using code that isn't in a proprietary black box allows you to see exactly what is being done so that you can change any part of it to meet your specific needs. Collaboration : If users encounter a problem, or have a feature request, they can get in contact with the developers directly. Submit an issue or pull request on GitHub or email jacob.emery@ibm.com. Collaboration is highly encouraged! Lower Barriers to Entry : The easier it is to get RHOCP on IBM zSystems up and running, the better - for you, IBM and Red Hat! It is free because RHOCP is an incredible product that should have the least amount of barriers to entry as possible. The world needs open-source, private, and hybrid cloud.","title":"Before You Begin"},{"location":"before-you-begin/#before-you-begin","text":"","title":"Before You Begin"},{"location":"before-you-begin/#description","text":"This project automates the User-Provisioned Infrastructure (UPI) method for deploying Red Hat OpenShift Container Platform (RHOCP) on IBM zSystems / LinuxONE using Kernel-based Virtual Machine (KVM) as the hypervisor.","title":"Description"},{"location":"before-you-begin/#support","text":"This is an unofficial project created by IBMers. This installation method is not officially supported by either Red Hat or IBM. However, once installation is complete, the resulting cluster is supported by Red Hat. UPI is the only supported method for RHOCP on IBM zSystems.","title":"Support"},{"location":"before-you-begin/#difficulty","text":"This process is much easier than doing so manually, but still not an easy task. You will likely encounter errors, but you will reach those errors quicker and understand the problem faster than if you were doing this process manually. After using these playbooks once, successive deployments will be much easier. A very basic understanding of what Ansible does is recommended. Advanced understanding is helpful for further customization of the playbooks. A basic understanding of the command-line is required. A basic understanding of git is recommended, especially for creating your organization's own fork of the repository for further customization. An advanced understanding of your computing environment is required for setting the environment variables. These Ansible Playbooks automate a User-Provisioned Infrastructure (UPI) deployment of Red Hat OpenShift Container Platform (RHOCP). This process, when done manually, is extremely tedious, time-consuming, and requires high levels of Linux AND IBM zSystems expertise. UPI is currently the only supported method for deploying RHOCP on IBM zSystems.","title":"Difficulty"},{"location":"before-you-begin/#why-free-and-open-source","text":"Trust : IBM zSystems run some of the most highly-secure workloads in the world. Trust is paramount. Developing and using code transparently builds trust between developers and users, so that users feel safe using it on their highly sensitive systems. Customization : IBM zSystems exist in environments that can be highly complex and vary drastically from one datacenter to another. Using code that isn't in a proprietary black box allows you to see exactly what is being done so that you can change any part of it to meet your specific needs. Collaboration : If users encounter a problem, or have a feature request, they can get in contact with the developers directly. Submit an issue or pull request on GitHub or email jacob.emery@ibm.com. Collaboration is highly encouraged! Lower Barriers to Entry : The easier it is to get RHOCP on IBM zSystems up and running, the better - for you, IBM and Red Hat! It is free because RHOCP is an incredible product that should have the least amount of barriers to entry as possible. The world needs open-source, private, and hybrid cloud.","title":"Why Free and Open-Source?"},{"location":"first-time-login/","text":"Step 6: First-Time Login # The last step of the main playbook will print a URL, username and temporary password for first-time login. Use a web-browser to type in the URL, which should take you to a sign-in page. Use the provided credentials to sign in. You will have to bypass a warning screen. Congratulations! Your OpenShift cluster installation is now complete.","title":"6 First-Time Login"},{"location":"first-time-login/#step-6-first-time-login","text":"The last step of the main playbook will print a URL, username and temporary password for first-time login. Use a web-browser to type in the URL, which should take you to a sign-in page. Use the provided credentials to sign in. You will have to bypass a warning screen. Congratulations! Your OpenShift cluster installation is now complete.","title":"Step 6: First-Time Login"},{"location":"get-pull-secret/","text":"Step 2: Get Pull Secret # In a web browser, navigate to Red Hat's Hybrid Cloud Console , click the text that says 'Copy pull secret' and save it for the next step.","title":"2 Get Pull Secret"},{"location":"get-pull-secret/#step-2-get-pull-secret","text":"In a web browser, navigate to Red Hat's Hybrid Cloud Console , click the text that says 'Copy pull secret' and save it for the next step.","title":"Step 2: Get Pull Secret"},{"location":"get-the-repository/","text":"Step 1: Get the Repository # Open the terminal On MacOS: cmd+space to open spotlight search, type in 'terminal' and hit enter Navigate to a folder (AKA directory) where you would like to store this project. Either do so graphically, or use the command-line. Here are some helpful commands for doing so: pwd to see what directory you're currently in ls to list child directories cd <folder-name> to change directories ( cd .. to go up to the parent directory) mkdir <new-folder-name> to create a new directory Copy/paste the following and hit enter: git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Open the newly created folder ( cd ) All in all, it will look something like this: $ pwd /Users/example-user $ mkdir ansible-project $ cd ansible-project/ $ git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Cloning into 'Ansible-OpenShift-Provisioning'... remote: Enumerating objects: 3472, done. remote: Counting objects: 100% (200/200), done. remote: Compressing objects: 100% (57/57), done. remote: Total 3472 (delta 152), reused 143 (delta 143), pack-reused 3272 Receiving objects: 100% (3472/3472), 506.29 KiB | 1.27 MiB/s, done. Resolving deltas: 100% (1699/1699), done. $ ls Ansible-OpenShift-Provisioning $ cd Ansible-OpenShift-Provisioning/ $ ls CHANGELOG.md README.md docs mkdocs.yaml roles LICENSE ansible.cfg inventories playbooks vault.yaml All you need to run Ansible is a terminal and a text editor. However, an IDE like VS Code is highly recommended for an integrated, user-friendly experience with helpful extensions like YAML .","title":"1 Get the repository"},{"location":"get-the-repository/#step-1-get-the-repository","text":"Open the terminal On MacOS: cmd+space to open spotlight search, type in 'terminal' and hit enter Navigate to a folder (AKA directory) where you would like to store this project. Either do so graphically, or use the command-line. Here are some helpful commands for doing so: pwd to see what directory you're currently in ls to list child directories cd <folder-name> to change directories ( cd .. to go up to the parent directory) mkdir <new-folder-name> to create a new directory Copy/paste the following and hit enter: git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Open the newly created folder ( cd ) All in all, it will look something like this: $ pwd /Users/example-user $ mkdir ansible-project $ cd ansible-project/ $ git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Cloning into 'Ansible-OpenShift-Provisioning'... remote: Enumerating objects: 3472, done. remote: Counting objects: 100% (200/200), done. remote: Compressing objects: 100% (57/57), done. remote: Total 3472 (delta 152), reused 143 (delta 143), pack-reused 3272 Receiving objects: 100% (3472/3472), 506.29 KiB | 1.27 MiB/s, done. Resolving deltas: 100% (1699/1699), done. $ ls Ansible-OpenShift-Provisioning $ cd Ansible-OpenShift-Provisioning/ $ ls CHANGELOG.md README.md docs mkdocs.yaml roles LICENSE ansible.cfg inventories playbooks vault.yaml All you need to run Ansible is a terminal and a text editor. However, an IDE like VS Code is highly recommended for an integrated, user-friendly experience with helpful extensions like YAML .","title":"Step 1: Get the Repository"},{"location":"prerequisites/","text":"Prerequisites # Red Hat # Account ( Sign Up ) License or free trial of Red Hat OpenShift Container Platform for IBM Z systems - s390x architecture (comes with the required licenses for Red Hat Enterprise Linux (RHEL) and CoreOS) IBM zSystems # Hardware Management Console (HMC) access on IBM zSystems or LinuxONE Must be in Dynamic Partition Manager (DPM) mode in order to use the playbook that automates the creation of the KVM host. If DPM mode is not an option for your environment, that playbook can be skipped, but a bare-metal RHEL server must be set-up on an LPAR manually (Filipe Miranda's how-to article ) before moving on. Once that is done, continue with the playbook that sets up the KVM host. For a minimum installation, at least: 6 Integrated Facilities for Linux (IFLs) with SMT2 enabled 85 GB of RAM An FCP storage group created with 1 TB of disk space 8 IPv4 addresses FTP Server # On the same network as your IBM zSystems / LinuxONE hardware. Red Hat Enterprise Linux (RHEL) 8 for s390x architecture mounted in an accessible folder (e.g. /home/ftpuser/rhel/) If you do not yet have RHEL for s390x, go to the Red Hat Customer Portal and download it. Under 'Product Variant' use the drop-down menu to select 'Red Hat Enterprise Linux for IBM z Systems' Double-check it's for version 8 and for s390x architecture Then scroll down to Red Hat Enterprise Linux 8.x Binary DVD and click on the 'Download Now' button. A folder to store config files (e.g. /home/ftpuser/ocp-config) Workstation # A computer/virtual machine with MacOS or Linux operating system Network access to your IBM zSystems / LinuxONE hardware Python3 installed: brew install python3 #MacOS, see note below sudo dnf install python3 #Fedora sudo apt install python3 #Debian Once Python3 is installed, you also need Ansible : pip3 install ansible Note: If you are using MacOS, you also need to have Homebrew package manager installed: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" and Xcode : xcode-select --install","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#red-hat","text":"Account ( Sign Up ) License or free trial of Red Hat OpenShift Container Platform for IBM Z systems - s390x architecture (comes with the required licenses for Red Hat Enterprise Linux (RHEL) and CoreOS)","title":"Red Hat"},{"location":"prerequisites/#ibm-zsystems","text":"Hardware Management Console (HMC) access on IBM zSystems or LinuxONE Must be in Dynamic Partition Manager (DPM) mode in order to use the playbook that automates the creation of the KVM host. If DPM mode is not an option for your environment, that playbook can be skipped, but a bare-metal RHEL server must be set-up on an LPAR manually (Filipe Miranda's how-to article ) before moving on. Once that is done, continue with the playbook that sets up the KVM host. For a minimum installation, at least: 6 Integrated Facilities for Linux (IFLs) with SMT2 enabled 85 GB of RAM An FCP storage group created with 1 TB of disk space 8 IPv4 addresses","title":"IBM zSystems"},{"location":"prerequisites/#ftp-server","text":"On the same network as your IBM zSystems / LinuxONE hardware. Red Hat Enterprise Linux (RHEL) 8 for s390x architecture mounted in an accessible folder (e.g. /home/ftpuser/rhel/) If you do not yet have RHEL for s390x, go to the Red Hat Customer Portal and download it. Under 'Product Variant' use the drop-down menu to select 'Red Hat Enterprise Linux for IBM z Systems' Double-check it's for version 8 and for s390x architecture Then scroll down to Red Hat Enterprise Linux 8.x Binary DVD and click on the 'Download Now' button. A folder to store config files (e.g. /home/ftpuser/ocp-config)","title":"FTP Server"},{"location":"prerequisites/#workstation","text":"A computer/virtual machine with MacOS or Linux operating system Network access to your IBM zSystems / LinuxONE hardware Python3 installed: brew install python3 #MacOS, see note below sudo dnf install python3 #Fedora sudo apt install python3 #Debian Once Python3 is installed, you also need Ansible : pip3 install ansible Note: If you are using MacOS, you also need to have Homebrew package manager installed: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" and Xcode : xcode-select --install","title":"Workstation"},{"location":"run-setup-playbook/","text":"Step 4: Run Setup Playbook # Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). Run this shell command: ansible-playbook playbooks/setup.yaml --ask-vault-pass","title":"4 Run Setup Playbook"},{"location":"run-setup-playbook/#step-4-run-setup-playbook","text":"Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). Run this shell command: ansible-playbook playbooks/setup.yaml --ask-vault-pass","title":"Step 4: Run Setup Playbook"},{"location":"run-the-playbooks/","text":"Step 5: Run the Playbooks # Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). To run all the playbooks at once, start the master playbook by running this shell command: ansible-playbook playbooks/site.yaml --ask-vault-pass Alternatively, run each part step-by-step by running one playbook at a time. Here's the list of playbooks to be run in order, also found in playbooks/site.yaml : create_kvm_host setup_kvm_host create_bastion setup_bastion create_nodes ocp_verification Watch Ansible as it completes the installation, correcting errors if they arise. To look at what tasks are running in detail, open the playbook or roles/role-name/tasks/main.yaml If the process fails in error: Go through the steps in the troubleshooting page. Use tags to selectively start from a certain point in the playbook. Each role has a corresponding tag for convenience. For example, to only run the httpd and get_ocp roles of the setup_bastion playbook: ansible-playbook playbooks/setup_bastion.yaml --tags 'httpd,get_ocp' --ask-vault-pass","title":"5 Run the Playbooks"},{"location":"run-the-playbooks/#step-5-run-the-playbooks","text":"Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). To run all the playbooks at once, start the master playbook by running this shell command: ansible-playbook playbooks/site.yaml --ask-vault-pass Alternatively, run each part step-by-step by running one playbook at a time. Here's the list of playbooks to be run in order, also found in playbooks/site.yaml : create_kvm_host setup_kvm_host create_bastion setup_bastion create_nodes ocp_verification Watch Ansible as it completes the installation, correcting errors if they arise. To look at what tasks are running in detail, open the playbook or roles/role-name/tasks/main.yaml If the process fails in error: Go through the steps in the troubleshooting page. Use tags to selectively start from a certain point in the playbook. Each role has a corresponding tag for convenience. For example, to only run the httpd and get_ocp roles of the setup_bastion playbook: ansible-playbook playbooks/setup_bastion.yaml --tags 'httpd,get_ocp' --ask-vault-pass","title":"Step 5: Run the Playbooks"},{"location":"set-variables/","text":"Step 3: Set Variables # In a text editor of your choice, open the environment variables file . This is the master variables file and you will likely reference it many times throughout the process. The default inventory can be found at inventories/default/group_vars/all.yaml . Fill out the variables marked with X to match your specific installation. This is the most important step in the process. Take the time to make sure everything here is correct.","title":"3 Set Variables"},{"location":"set-variables/#step-3-set-variables","text":"In a text editor of your choice, open the environment variables file . This is the master variables file and you will likely reference it many times throughout the process. The default inventory can be found at inventories/default/group_vars/all.yaml . Fill out the variables marked with X to match your specific installation. This is the most important step in the process. Take the time to make sure everything here is correct.","title":"Step 3: Set Variables"},{"location":"teardown/","text":"Teardown: # If you would like to teardown your VMs, first determine whether you would like to do a full or partial teardown, specified below. Full Teardown : To teardown all the OpenShift KVM guest virtual machines (will not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags full --ask-vault-pass Partial Teardown : To teardown all OpenShift KVM guest virtual machines except the bastion (will also not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags partial --ask-vault-pass To teardown the KVM host, use the delete_partition playbook .","title":"Teardown"},{"location":"teardown/#teardown","text":"If you would like to teardown your VMs, first determine whether you would like to do a full or partial teardown, specified below. Full Teardown : To teardown all the OpenShift KVM guest virtual machines (will not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags full --ask-vault-pass Partial Teardown : To teardown all OpenShift KVM guest virtual machines except the bastion (will also not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags partial --ask-vault-pass To teardown the KVM host, use the delete_partition playbook .","title":"Teardown:"},{"location":"troubleshooting/","text":"Troubleshooting # If you encounter errors while running the main playbook, there are a few things you can do: Double check your variables. Inspect the part that failed by opening the playbook or role at roles/role-name/tasks/main.yaml Google the specific error message. Re-run the role indivually with tags or with the verbosity '-v' option to get more debugging information (more v's give more info). For example: ansible-playbook main.yaml --tags get_ocp -vvv --ask-vault-pass Teardown the KVM host with the delete_partition.yaml playbook or teardown troublesome KVM guests with the teardown playbooks. E-mail Jacob Emery at jacob.emery@ibm.com If it's a problem with an OpenShift verification step: Open the cockpit to monitor the VMs. In a web browser, go to https://kvm-host-IP-here:9090 Sign-in with your credentials set in the variables file Enable administrative access in the top right. Open the 'Virtual Machines' tab from the left side toolbar. Sometimes it just takes a while, especially if it's lacking resources. Give it some time and then re-reun the playbook/role with tags . If that doesn't work, SSH into the bastion as root (\"ssh root@\\<bastion-ip-address-here>\") and then run, \"export KUBECONFIG=~/ocpinst/auth/kubeconfig\" and then \"oc whoami\" and make sure it ouputs \"system:admin\". Then run the shell command from the role you would like to check on manually: i.e. 'oc get nodes', 'oc get co', etc. Open the .openshift_install.log file for information on what happened and try to debug the issue.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If you encounter errors while running the main playbook, there are a few things you can do: Double check your variables. Inspect the part that failed by opening the playbook or role at roles/role-name/tasks/main.yaml Google the specific error message. Re-run the role indivually with tags or with the verbosity '-v' option to get more debugging information (more v's give more info). For example: ansible-playbook main.yaml --tags get_ocp -vvv --ask-vault-pass Teardown the KVM host with the delete_partition.yaml playbook or teardown troublesome KVM guests with the teardown playbooks. E-mail Jacob Emery at jacob.emery@ibm.com If it's a problem with an OpenShift verification step: Open the cockpit to monitor the VMs. In a web browser, go to https://kvm-host-IP-here:9090 Sign-in with your credentials set in the variables file Enable administrative access in the top right. Open the 'Virtual Machines' tab from the left side toolbar. Sometimes it just takes a while, especially if it's lacking resources. Give it some time and then re-reun the playbook/role with tags . If that doesn't work, SSH into the bastion as root (\"ssh root@\\<bastion-ip-address-here>\") and then run, \"export KUBECONFIG=~/ocpinst/auth/kubeconfig\" and then \"oc whoami\" and make sure it ouputs \"system:admin\". Then run the shell command from the role you would like to check on manually: i.e. 'oc get nodes', 'oc get co', etc. Open the .openshift_install.log file for information on what happened and try to debug the issue.","title":"Troubleshooting"},{"location":"vault-and-tags/","text":"Vault and Tags # A quick note explaining how to use tags and Ansible Vault. Tags # To be more selective with what parts of a playbook are run, use tags. To determine what part of a playbook you would like to run, open the playbook you'd like to run and find the roles parameter. Each role has a corresponding tag. There are also occasionally tags for sections of a playbook or within the role themselves. This is especially helpful for troubleshooting. You can add in tags under the name parameter for individual tasks you'd like to run. Here's an example of using a tag: ansible-playbook playbooks/setup_kvm_host.yaml --tags \"section_2,section_3\" This runs only the parts of the setup_kvm_host playbook marked with tags section_2 and section_3. To use more than one tag, they must be quoted (single or double) and comma-separated (with or without spaces between). Vault # For security purposes, the setup playbook transfers passwords and sensitive information entered into the environment variables file to the Ansible Vault and then encrypts them. To decrypt the Ansible Vault to view its contents, run the following command: ansible-playbook playbooks/vault.yaml --tags decrypt --ask-vault-pass To encrypt the Ansible Vault to re-secure its contents, run the following command: ansible-playbook playbooks/vault.yaml --tags encrypt --ask-vault-pass","title":"Vault and Tags"},{"location":"vault-and-tags/#vault-and-tags","text":"A quick note explaining how to use tags and Ansible Vault.","title":"Vault and Tags"},{"location":"vault-and-tags/#tags","text":"To be more selective with what parts of a playbook are run, use tags. To determine what part of a playbook you would like to run, open the playbook you'd like to run and find the roles parameter. Each role has a corresponding tag. There are also occasionally tags for sections of a playbook or within the role themselves. This is especially helpful for troubleshooting. You can add in tags under the name parameter for individual tasks you'd like to run. Here's an example of using a tag: ansible-playbook playbooks/setup_kvm_host.yaml --tags \"section_2,section_3\" This runs only the parts of the setup_kvm_host playbook marked with tags section_2 and section_3. To use more than one tag, they must be quoted (single or double) and comma-separated (with or without spaces between).","title":"Tags"},{"location":"vault-and-tags/#vault","text":"For security purposes, the setup playbook transfers passwords and sensitive information entered into the environment variables file to the Ansible Vault and then encrypts them. To decrypt the Ansible Vault to view its contents, run the following command: ansible-playbook playbooks/vault.yaml --tags decrypt --ask-vault-pass To encrypt the Ansible Vault to re-secure its contents, run the following command: ansible-playbook playbooks/vault.yaml --tags encrypt --ask-vault-pass","title":"Vault"}]}