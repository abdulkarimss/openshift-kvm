{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE # Overview # These Ansible Playbooks automate the setup and deployment of a Red Hat OpenShift Container Platform (RHOCP) cluster on IBM zSystems / LinuxONE with Kernel Virtual Machine (KVM) as the hypervisor. Ready to Start? # Use the left-hand panel to navigate the site. Start with the Before You Begin page. Need Help? # Contact Jacob Emery at jacob.emery@ibm.com","title":"Home"},{"location":"#ansible-automated-openshift-provisioning-on-kvm-on-ibm-zsystems-linuxone","text":"","title":"Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE"},{"location":"#overview","text":"These Ansible Playbooks automate the setup and deployment of a Red Hat OpenShift Container Platform (RHOCP) cluster on IBM zSystems / LinuxONE with Kernel Virtual Machine (KVM) as the hypervisor.","title":"Overview"},{"location":"#ready-to-start","text":"Use the left-hand panel to navigate the site. Start with the Before You Begin page.","title":"Ready to Start?"},{"location":"#need-help","text":"Contact Jacob Emery at jacob.emery@ibm.com","title":"Need Help?"},{"location":"CHANGELOG/","text":"Changelog # All notable changes to this project will be documented in this file. Roadmap # Add air-gapped (disconnected) deployment option Create podman container image to use as the Ansible Controller for consistency of deployment Automated setup of persistent storage Tag and assign infrastructure nodes to specific operators Add option to use a VPN to reduce # of IPs needed High Availability Deployments # Version 0.4.0 Released 2022-07-12 Summary # Added support for deploying across multiple Logical Partitions, for the sake of making the cluster more resilient and highly available. Added # Tasks to create_nodes playbook to support multi-LPAR deployment options. Single LPAR deployments are still possible. Pre-existing LPAR deployments have been clarified in terms of what variables are needed and not needed. Setup of OpenVPN server on bastion and clients on KVM hosts when it's a highly available deployment. This is so that the KVM hosts can communicate with each other. Ability to use custom names for each VM. Task that add a line to user's /etc/hosts file for easier first-time login. Support for using a proxy server for the Ansible environment and OCP install-config. Support for multiple nameservers. Support for using a load balancer off the bastion, asking for public and private IP of the LB. Detailed descriptions of what each playbook accomplishes (Thanks Trevor!) Tarballs of Ansible-Galaxy collections in anticipation of disconnected installs update. Separate set-variables documentation page for host_vars, new to high availability update. Controller packages and Ansible-Galaxy collections as pre-reqs, instead of waiting until setup playbook, which caused errors. Example KVM host host_vars files. Check to make sure host_vars are set properly for KVM host(s) Support for attaching 2 storage groups that then later are combined into one logical volume. Modified # API/apps DNS checks separated from bastion check, separated api-int check as well. Wait_for bootstrap complete tasks moved to create_nodes playbook so that compute nodes don't start coming up until control nodes are done bootstrapping. HAProxy config to use FQDNs instead of IPs (Thanks Wasif!) SSH copy id role to be idempotent (Thanks Nico!) Names of playbooks to include step-by-step numbering. Bastion kickstart to use HTTP instead of FTP because virt-install's use of FTP is inconsistent. Set_inventory role to use a template instead of tasks to creat the inventory to be cleaner and more consistent. OCP install directory from bastion admin user's home ~/ocpinst to /root/ocpinst Removed # Usage of Ansible Vault. Teardown playbook. Removed unnecessary task that add bastion admin user to libvirt and qemu groups. Option to store disks/storage pool for libvirt anywhere other than the default location (/var/lib/libvirt/images) Documentation Overhaul # Version 0.3.1 Released: 2022-06-03 Summary # Moved documentation to GitHub Pages to be more reader-friendly. Automated KVM Host Provisioning # Version 0.3.0 Released: 2022-03-26 Summary # Now able to provision the KVM host via Ansible. Changed the structure of playbooks, variables, and inventories to use Ansible best practices. Added # Support for using IBM's zHMC Ansible modules to automate the creation of a logical partition (LPAR) profile, connect storage group and network card, boot from an FTP server, and then kickstart the installation of RHEL to serve as the KVM hypervisor for the cluster. Usage of Ansible vault to encrypt sensitive data. Playbooks must now be run with --ask-vault-pass, e.g. 'ansible-playbook playbooks/site.yaml --ask-vault-pass' Modified # Bastion boot method from cloud-init to FTP and kickstart. The structure of playbooks. The setup.yaml playbook still must be run before anything else, but now there is a master playbook - site.yaml which imports all other playbooks. This was done to be more user-friendly and in-line with best practices. Previously, everything was all in one playbook and relied on tags to start back from a given point. Relying solely on tags proved tedious. The structure for inventories, which allows for more flexibility with deployments and is more in-line with best practices. Now you can have multiple inventories and specify which you would like to use for a given run in the ansible.cfg file. The structure of variables, to allow for the separation of the bastion node from the rest of the cluster. This opens up many more possibilities for more complex deployments where, for example, the bastion node is already created. Infrastructure Nodes, Extra Apps, Security # Version: 0.2.1 Released: 2022-01-06 Summary # Now able to designate compute nodes as infrastructure nodes, and create optional RHEL VMs for additional non-cluster applications running on the KVM host. Made changes to SSH and SELinux tasks to be more secure. Added # Support for creating infrastructure nodes and extra apps. Added tcp port 53 to firewall. Setting of permissions and ownership of important configuration files to bastion admin user instead of root. Wheel to groups that bastion admin user is added to on boot. More rounds of checking cluster operators and CSRs in verification steps to ensure the playbook doesn't fail if it takes a long time for those steps to complete. Task to httpd to allow port 4443 because SELinux is no longer set to permissive (see ' Removed ' below). Modified # Formatting of README file to be prettier and more useful. env.yaml to have two sections separated by a comment block: one for variables that need to be filled out, the other for pre-filled variables that can be modified if desired. Ansible user from running as root to an admin user with sudo privileges. Removed # The need to run anything as root user for security reasons. set_selinux_permissive mode role for security reasons. Scaling # Version: 0.2.0 Released: 2021-12-09 Summary # Now supports any number of control and compute nodes to be provisioned in the cluster. This update heavily modifies the variable structure in env.yaml in order to make scaling work. Added # Support for scaling of control and compute nodes. Modified # Variable structure in env.yaml in order to support scaling. Tags to match their corresponding role. Every reference to a variable from env.yaml to match the new structure. Automated OCP Verification # Version: 0.1.1 Released: 2021-12-03 Summary # Fully automated all OCP verification steps. Cutting the number of steps nearly in half. The main playbook can now run completely hands-off from kicking it off all the way to an operational cluster. The last step provides the first-time login credentials. Added # 5 roles related to automating OCP verification steps: wait_for_bootstrap, approve_certs, check_nodes, wait_for_cluster_operators, and wait_for_install_complete. Role to check internal and external DNS configuration before continuing. Including checking to make sure the name resolves to the correct IP address. Modified # The mirrors for CoreOS versions to update to 4.9 and tested them. The acquisition method of RHEL qcow2 from downloading via ephemeral link to having the user download the file to their local machine as a pre-req. This was changed to avoid having to re-copy the link every time it expires. teardown.yaml and reset_files role to be fully idempotent when running the main playbook from the point where each type of teardown sets the user back to. Lots of small tweaks. Removed # Instructions in README for doing OCP verification steps manually Automated Bastion Install # Version: 0.1.0 Released: 2021-11-24 Summary # Fully automated bastion installation and configuration using cloud-init Added # Options in env.yaml for creating a DNS server on the bastion or not, and for automatically attaching Red Hat subscriptions Variables for bootstrap, bastion, control and compute nodes' specifications in env.yaml Node name variables in env.yaml Variable for network interface name in env.yaml Variable for DNS forwarder in env.yaml Templating of DNS configuration files so they don't have to be pre-provided Expect script to ssh_copy_id role so that the user doesn't have to type in ssh password when copying ssh key Templating of haproxy config file A boot_teardown tag in teardown.yaml to automate the teardown of bootstrap node Modified # create_bastion role to use cloud-init to fully automate configuration and installation of the bastion node teardown.yaml script to decrease complexity and work faster. Some tags to match their corresponding role names Lots of small improvements and tweaks Removed # Encryption of env.yaml as it was unnecessary and increased complexity First Working Build # Version: 0.0.1 Released: 2021-08-24 Initial Commit # Version: 0.0.0 Released: 2021-06-11","title":"Change Log"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file.","title":"Changelog"},{"location":"CHANGELOG/#roadmap","text":"Add air-gapped (disconnected) deployment option Create podman container image to use as the Ansible Controller for consistency of deployment Automated setup of persistent storage Tag and assign infrastructure nodes to specific operators Add option to use a VPN to reduce # of IPs needed","title":"Roadmap"},{"location":"CHANGELOG/#high-availability-deployments","text":"Version 0.4.0 Released 2022-07-12","title":"High Availability Deployments"},{"location":"CHANGELOG/#summary","text":"Added support for deploying across multiple Logical Partitions, for the sake of making the cluster more resilient and highly available.","title":"Summary"},{"location":"CHANGELOG/#added","text":"Tasks to create_nodes playbook to support multi-LPAR deployment options. Single LPAR deployments are still possible. Pre-existing LPAR deployments have been clarified in terms of what variables are needed and not needed. Setup of OpenVPN server on bastion and clients on KVM hosts when it's a highly available deployment. This is so that the KVM hosts can communicate with each other. Ability to use custom names for each VM. Task that add a line to user's /etc/hosts file for easier first-time login. Support for using a proxy server for the Ansible environment and OCP install-config. Support for multiple nameservers. Support for using a load balancer off the bastion, asking for public and private IP of the LB. Detailed descriptions of what each playbook accomplishes (Thanks Trevor!) Tarballs of Ansible-Galaxy collections in anticipation of disconnected installs update. Separate set-variables documentation page for host_vars, new to high availability update. Controller packages and Ansible-Galaxy collections as pre-reqs, instead of waiting until setup playbook, which caused errors. Example KVM host host_vars files. Check to make sure host_vars are set properly for KVM host(s) Support for attaching 2 storage groups that then later are combined into one logical volume.","title":"Added"},{"location":"CHANGELOG/#modified","text":"API/apps DNS checks separated from bastion check, separated api-int check as well. Wait_for bootstrap complete tasks moved to create_nodes playbook so that compute nodes don't start coming up until control nodes are done bootstrapping. HAProxy config to use FQDNs instead of IPs (Thanks Wasif!) SSH copy id role to be idempotent (Thanks Nico!) Names of playbooks to include step-by-step numbering. Bastion kickstart to use HTTP instead of FTP because virt-install's use of FTP is inconsistent. Set_inventory role to use a template instead of tasks to creat the inventory to be cleaner and more consistent. OCP install directory from bastion admin user's home ~/ocpinst to /root/ocpinst","title":"Modified"},{"location":"CHANGELOG/#removed","text":"Usage of Ansible Vault. Teardown playbook. Removed unnecessary task that add bastion admin user to libvirt and qemu groups. Option to store disks/storage pool for libvirt anywhere other than the default location (/var/lib/libvirt/images)","title":"Removed"},{"location":"CHANGELOG/#documentation-overhaul","text":"Version 0.3.1 Released: 2022-06-03","title":"Documentation Overhaul"},{"location":"CHANGELOG/#summary_1","text":"Moved documentation to GitHub Pages to be more reader-friendly.","title":"Summary"},{"location":"CHANGELOG/#automated-kvm-host-provisioning","text":"Version 0.3.0 Released: 2022-03-26","title":"Automated KVM Host Provisioning"},{"location":"CHANGELOG/#summary_2","text":"Now able to provision the KVM host via Ansible. Changed the structure of playbooks, variables, and inventories to use Ansible best practices.","title":"Summary"},{"location":"CHANGELOG/#added_1","text":"Support for using IBM's zHMC Ansible modules to automate the creation of a logical partition (LPAR) profile, connect storage group and network card, boot from an FTP server, and then kickstart the installation of RHEL to serve as the KVM hypervisor for the cluster. Usage of Ansible vault to encrypt sensitive data. Playbooks must now be run with --ask-vault-pass, e.g. 'ansible-playbook playbooks/site.yaml --ask-vault-pass'","title":"Added"},{"location":"CHANGELOG/#modified_1","text":"Bastion boot method from cloud-init to FTP and kickstart. The structure of playbooks. The setup.yaml playbook still must be run before anything else, but now there is a master playbook - site.yaml which imports all other playbooks. This was done to be more user-friendly and in-line with best practices. Previously, everything was all in one playbook and relied on tags to start back from a given point. Relying solely on tags proved tedious. The structure for inventories, which allows for more flexibility with deployments and is more in-line with best practices. Now you can have multiple inventories and specify which you would like to use for a given run in the ansible.cfg file. The structure of variables, to allow for the separation of the bastion node from the rest of the cluster. This opens up many more possibilities for more complex deployments where, for example, the bastion node is already created.","title":"Modified"},{"location":"CHANGELOG/#infrastructure-nodes-extra-apps-security","text":"Version: 0.2.1 Released: 2022-01-06","title":"Infrastructure Nodes, Extra Apps, Security"},{"location":"CHANGELOG/#summary_3","text":"Now able to designate compute nodes as infrastructure nodes, and create optional RHEL VMs for additional non-cluster applications running on the KVM host. Made changes to SSH and SELinux tasks to be more secure.","title":"Summary"},{"location":"CHANGELOG/#added_2","text":"Support for creating infrastructure nodes and extra apps. Added tcp port 53 to firewall. Setting of permissions and ownership of important configuration files to bastion admin user instead of root. Wheel to groups that bastion admin user is added to on boot. More rounds of checking cluster operators and CSRs in verification steps to ensure the playbook doesn't fail if it takes a long time for those steps to complete. Task to httpd to allow port 4443 because SELinux is no longer set to permissive (see ' Removed ' below).","title":"Added"},{"location":"CHANGELOG/#modified_2","text":"Formatting of README file to be prettier and more useful. env.yaml to have two sections separated by a comment block: one for variables that need to be filled out, the other for pre-filled variables that can be modified if desired. Ansible user from running as root to an admin user with sudo privileges.","title":"Modified"},{"location":"CHANGELOG/#removed_1","text":"The need to run anything as root user for security reasons. set_selinux_permissive mode role for security reasons.","title":"Removed"},{"location":"CHANGELOG/#scaling","text":"Version: 0.2.0 Released: 2021-12-09","title":"Scaling"},{"location":"CHANGELOG/#summary_4","text":"Now supports any number of control and compute nodes to be provisioned in the cluster. This update heavily modifies the variable structure in env.yaml in order to make scaling work.","title":"Summary"},{"location":"CHANGELOG/#added_3","text":"Support for scaling of control and compute nodes.","title":"Added"},{"location":"CHANGELOG/#modified_3","text":"Variable structure in env.yaml in order to support scaling. Tags to match their corresponding role. Every reference to a variable from env.yaml to match the new structure.","title":"Modified"},{"location":"CHANGELOG/#automated-ocp-verification","text":"Version: 0.1.1 Released: 2021-12-03","title":"Automated OCP Verification"},{"location":"CHANGELOG/#summary_5","text":"Fully automated all OCP verification steps. Cutting the number of steps nearly in half. The main playbook can now run completely hands-off from kicking it off all the way to an operational cluster. The last step provides the first-time login credentials.","title":"Summary"},{"location":"CHANGELOG/#added_4","text":"5 roles related to automating OCP verification steps: wait_for_bootstrap, approve_certs, check_nodes, wait_for_cluster_operators, and wait_for_install_complete. Role to check internal and external DNS configuration before continuing. Including checking to make sure the name resolves to the correct IP address.","title":"Added"},{"location":"CHANGELOG/#modified_4","text":"The mirrors for CoreOS versions to update to 4.9 and tested them. The acquisition method of RHEL qcow2 from downloading via ephemeral link to having the user download the file to their local machine as a pre-req. This was changed to avoid having to re-copy the link every time it expires. teardown.yaml and reset_files role to be fully idempotent when running the main playbook from the point where each type of teardown sets the user back to. Lots of small tweaks.","title":"Modified"},{"location":"CHANGELOG/#removed_2","text":"Instructions in README for doing OCP verification steps manually","title":"Removed"},{"location":"CHANGELOG/#automated-bastion-install","text":"Version: 0.1.0 Released: 2021-11-24","title":"Automated Bastion Install"},{"location":"CHANGELOG/#summary_6","text":"Fully automated bastion installation and configuration using cloud-init","title":"Summary"},{"location":"CHANGELOG/#added_5","text":"Options in env.yaml for creating a DNS server on the bastion or not, and for automatically attaching Red Hat subscriptions Variables for bootstrap, bastion, control and compute nodes' specifications in env.yaml Node name variables in env.yaml Variable for network interface name in env.yaml Variable for DNS forwarder in env.yaml Templating of DNS configuration files so they don't have to be pre-provided Expect script to ssh_copy_id role so that the user doesn't have to type in ssh password when copying ssh key Templating of haproxy config file A boot_teardown tag in teardown.yaml to automate the teardown of bootstrap node","title":"Added"},{"location":"CHANGELOG/#modified_5","text":"create_bastion role to use cloud-init to fully automate configuration and installation of the bastion node teardown.yaml script to decrease complexity and work faster. Some tags to match their corresponding role names Lots of small improvements and tweaks","title":"Modified"},{"location":"CHANGELOG/#removed_3","text":"Encryption of env.yaml as it was unnecessary and increased complexity","title":"Removed"},{"location":"CHANGELOG/#first-working-build","text":"Version: 0.0.1 Released: 2021-08-24","title":"First Working Build"},{"location":"CHANGELOG/#initial-commit","text":"Version: 0.0.0 Released: 2021-06-11","title":"Initial Commit"},{"location":"acknowledgements/","text":"Phillip Wilson Filipe Miranda Patrick Fruth Wasif Mohammad Stuart Tener Fred Bader Ken Morse Nico Boehr Trevor Vardeman Matt Mondics Miao Zhang-Cohen","title":"Acknowledgements"},{"location":"before-you-begin/","text":"Before You Begin # Description # This project automates the User-Provisioned Infrastructure (UPI) method for deploying Red Hat OpenShift Container Platform (RHOCP) on IBM zSystems / LinuxONE using Kernel-based Virtual Machine (KVM) as the hypervisor. Support # This is an unofficial project created by IBMers. This installation method is not officially supported by either Red Hat or IBM. However, once installation is complete, the resulting cluster is supported by Red Hat. UPI is the only supported method for RHOCP on IBM zSystems. Difficulty # This process is much easier than doing so manually, but still not an easy task. You will likely encounter errors, but you will reach those errors quicker and understand the problem faster than if you were doing this process manually. After using these playbooks once, successive deployments will be much easier. A very basic understanding of what Ansible does is recommended. Advanced understanding is helpful for further customization of the playbooks. A basic understanding of the command-line is required. A basic understanding of git is recommended, especially for creating your organization's own fork of the repository for further customization. An advanced understanding of your computing environment is required for setting the environment variables. These Ansible Playbooks automate a User-Provisioned Infrastructure (UPI) deployment of Red Hat OpenShift Container Platform (RHOCP). This process, when done manually, is extremely tedious, time-consuming, and requires high levels of Linux AND IBM zSystems expertise. UPI is currently the only supported method for deploying RHOCP on IBM zSystems. Why Free and Open-Source? # Trust : IBM zSystems run some of the most highly-secure workloads in the world. Trust is paramount. Developing and using code transparently builds trust between developers and users, so that users feel safe using it on their highly sensitive systems. Customization : IBM zSystems exist in environments that can be highly complex and vary drastically from one datacenter to another. Using code that isn't in a proprietary black box allows you to see exactly what is being done so that you can change any part of it to meet your specific needs. Collaboration : If users encounter a problem, or have a feature request, they can get in contact with the developers directly. Submit an issue or pull request on GitHub or email jacob.emery@ibm.com. Collaboration is highly encouraged! Lower Barriers to Entry : The easier it is to get RHOCP on IBM zSystems up and running, the better - for you, IBM and Red Hat! It is free because RHOCP is an incredible product that should have the least amount of barriers to entry as possible. The world needs open-source, private, and hybrid cloud.","title":"Before You Begin"},{"location":"before-you-begin/#before-you-begin","text":"","title":"Before You Begin"},{"location":"before-you-begin/#description","text":"This project automates the User-Provisioned Infrastructure (UPI) method for deploying Red Hat OpenShift Container Platform (RHOCP) on IBM zSystems / LinuxONE using Kernel-based Virtual Machine (KVM) as the hypervisor.","title":"Description"},{"location":"before-you-begin/#support","text":"This is an unofficial project created by IBMers. This installation method is not officially supported by either Red Hat or IBM. However, once installation is complete, the resulting cluster is supported by Red Hat. UPI is the only supported method for RHOCP on IBM zSystems.","title":"Support"},{"location":"before-you-begin/#difficulty","text":"This process is much easier than doing so manually, but still not an easy task. You will likely encounter errors, but you will reach those errors quicker and understand the problem faster than if you were doing this process manually. After using these playbooks once, successive deployments will be much easier. A very basic understanding of what Ansible does is recommended. Advanced understanding is helpful for further customization of the playbooks. A basic understanding of the command-line is required. A basic understanding of git is recommended, especially for creating your organization's own fork of the repository for further customization. An advanced understanding of your computing environment is required for setting the environment variables. These Ansible Playbooks automate a User-Provisioned Infrastructure (UPI) deployment of Red Hat OpenShift Container Platform (RHOCP). This process, when done manually, is extremely tedious, time-consuming, and requires high levels of Linux AND IBM zSystems expertise. UPI is currently the only supported method for deploying RHOCP on IBM zSystems.","title":"Difficulty"},{"location":"before-you-begin/#why-free-and-open-source","text":"Trust : IBM zSystems run some of the most highly-secure workloads in the world. Trust is paramount. Developing and using code transparently builds trust between developers and users, so that users feel safe using it on their highly sensitive systems. Customization : IBM zSystems exist in environments that can be highly complex and vary drastically from one datacenter to another. Using code that isn't in a proprietary black box allows you to see exactly what is being done so that you can change any part of it to meet your specific needs. Collaboration : If users encounter a problem, or have a feature request, they can get in contact with the developers directly. Submit an issue or pull request on GitHub or email jacob.emery@ibm.com. Collaboration is highly encouraged! Lower Barriers to Entry : The easier it is to get RHOCP on IBM zSystems up and running, the better - for you, IBM and Red Hat! It is free because RHOCP is an incredible product that should have the least amount of barriers to entry as possible. The world needs open-source, private, and hybrid cloud.","title":"Why Free and Open-Source?"},{"location":"get-info/","text":"Step 1: Get Info # Get Repository # Open the terminal Navigate to a folder (AKA directory) where you would like to store this project. Either do so graphically, or use the command-line. Here are some helpful commands for doing so: pwd to see what directory you're currently in ls to list child directories cd <folder-name> to change directories ( cd .. to go up to the parent directory) mkdir <new-folder-name> to create a new directory Copy/paste the following and hit enter: git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Change into the newly created directory The commands and output should resemble the following example: $ pwd /Users/example-user $ mkdir ansible-project $ cd ansible-project/ $ git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Cloning into 'Ansible-OpenShift-Provisioning'... remote: Enumerating objects: 3472, done. remote: Counting objects: 100% (200/200), done. remote: Compressing objects: 100% (57/57), done. remote: Total 3472 (delta 152), reused 143 (delta 143), pack-reused 3272 Receiving objects: 100% (3472/3472), 506.29 KiB | 1.27 MiB/s, done. Resolving deltas: 100% (1699/1699), done. $ ls Ansible-OpenShift-Provisioning $ cd Ansible-OpenShift-Provisioning/ $ ls CHANGELOG.md README.md docs mkdocs.yaml roles LICENSE ansible.cfg inventories playbooks Get Pull Secret # In a web browser, navigate to Red Hat's Hybrid Cloud Console , click the text that says 'Copy pull secret' and save it for the next step. Gather Environment Information # You will need a lot of information about the environment this cluster will be set-up in. You will need the help of at least your IBM zSystems infrastructure team so they can provision you a storage group. You'll also need them to provide you with IP address range, hostnames, subnet, gateway, how much disk space you have to work with, etc. A full list of variables needed are found on the next page. Many of them are filled in with defaults or are optional. Please take your time. I would recommend having someone on stand-by in case you need more information or need to ask a question about the environment.","title":"1 Get Info"},{"location":"get-info/#step-1-get-info","text":"","title":"Step 1: Get Info"},{"location":"get-info/#get-repository","text":"Open the terminal Navigate to a folder (AKA directory) where you would like to store this project. Either do so graphically, or use the command-line. Here are some helpful commands for doing so: pwd to see what directory you're currently in ls to list child directories cd <folder-name> to change directories ( cd .. to go up to the parent directory) mkdir <new-folder-name> to create a new directory Copy/paste the following and hit enter: git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Change into the newly created directory The commands and output should resemble the following example: $ pwd /Users/example-user $ mkdir ansible-project $ cd ansible-project/ $ git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Cloning into 'Ansible-OpenShift-Provisioning'... remote: Enumerating objects: 3472, done. remote: Counting objects: 100% (200/200), done. remote: Compressing objects: 100% (57/57), done. remote: Total 3472 (delta 152), reused 143 (delta 143), pack-reused 3272 Receiving objects: 100% (3472/3472), 506.29 KiB | 1.27 MiB/s, done. Resolving deltas: 100% (1699/1699), done. $ ls Ansible-OpenShift-Provisioning $ cd Ansible-OpenShift-Provisioning/ $ ls CHANGELOG.md README.md docs mkdocs.yaml roles LICENSE ansible.cfg inventories playbooks","title":"Get Repository"},{"location":"get-info/#get-pull-secret","text":"In a web browser, navigate to Red Hat's Hybrid Cloud Console , click the text that says 'Copy pull secret' and save it for the next step.","title":"Get Pull Secret"},{"location":"get-info/#gather-environment-information","text":"You will need a lot of information about the environment this cluster will be set-up in. You will need the help of at least your IBM zSystems infrastructure team so they can provision you a storage group. You'll also need them to provide you with IP address range, hostnames, subnet, gateway, how much disk space you have to work with, etc. A full list of variables needed are found on the next page. Many of them are filled in with defaults or are optional. Please take your time. I would recommend having someone on stand-by in case you need more information or need to ask a question about the environment.","title":"Gather Environment Information"},{"location":"prerequisites/","text":"Prerequisites # Red Hat # Account ( Sign Up ) License or free trial of Red Hat OpenShift Container Platform for IBM Z systems - s390x architecture (comes with the required licenses for Red Hat Enterprise Linux (RHEL) and CoreOS) IBM zSystems # Hardware Management Console (HMC) access on IBM zSystems or LinuxONE Must be in Dynamic Partition Manager (DPM) mode in order to use the playbook that automates the creation of the KVM host. If DPM mode is not an option for your environment, that playbook can be skipped, but a bare-metal RHEL server must be set-up on an LPAR manually (Filipe Miranda's how-to article ) before moving on. Once that is done, continue with the playbook that sets up the KVM host. For a minimum installation, at least: 6 Integrated Facilities for Linux (IFLs) with SMT2 enabled 85 GB of RAM An FCP storage group created with 1 TB of disk space 8 IPv4 addresses FTP Server # On the same network as your IBM zSystems / LinuxONE hardware. Red Hat Enterprise Linux (RHEL) 8 for s390x architecture mounted in an accessible folder (e.g. /home/ftpuser/rhel/) If you do not yet have RHEL for s390x, go to the Red Hat Customer Portal and download it. Under 'Product Variant' use the drop-down menu to select 'Red Hat Enterprise Linux for IBM z Systems' Double-check it's for version 8 and for s390x architecture Then scroll down to Red Hat Enterprise Linux 8.x Binary DVD and click on the 'Download Now' button. A folder to store config files (e.g. /home/ftpuser/ocp-config) A user with sudo and SSH access. Ansible Controller # The computer/virtual machine running Ansible, sometimes referred to as localhost. Must be running on with MacOS or Linux operating systems. Network access to your IBM zSystems / LinuxONE hardware All you need to run Ansible is a terminal and a text editor. However, an IDE like VS Code is highly recommended for an integrated, user-friendly experience with helpful extensions like YAML . Python3 installed: brew install python3 #MacOS sudo dnf install python3 #Fedora sudo apt install python3 #Debian Once Python3 is installed, you also need Ansible version 2.9 or above: pip3 install ansible Once Ansible is installed, you will need a few collections from Ansible Galaxy. Use this template to install them via the terminal: ansible-galaxy collection install community.general community.crypto ansible.posix community.libvirt If you will be using these playbooks to automate the creation of the LPAR(s) that will act as KVM host(s) for the cluster, you will also need: pip3 install zhmcclient cryptography packaging PyYAML ansible-galaxy collection install ibm.ibm_zhmc If you are using MacOS, you also need to have Homebrew package manager installed: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" and Xcode : xcode-select --install","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#red-hat","text":"Account ( Sign Up ) License or free trial of Red Hat OpenShift Container Platform for IBM Z systems - s390x architecture (comes with the required licenses for Red Hat Enterprise Linux (RHEL) and CoreOS)","title":"Red Hat"},{"location":"prerequisites/#ibm-zsystems","text":"Hardware Management Console (HMC) access on IBM zSystems or LinuxONE Must be in Dynamic Partition Manager (DPM) mode in order to use the playbook that automates the creation of the KVM host. If DPM mode is not an option for your environment, that playbook can be skipped, but a bare-metal RHEL server must be set-up on an LPAR manually (Filipe Miranda's how-to article ) before moving on. Once that is done, continue with the playbook that sets up the KVM host. For a minimum installation, at least: 6 Integrated Facilities for Linux (IFLs) with SMT2 enabled 85 GB of RAM An FCP storage group created with 1 TB of disk space 8 IPv4 addresses","title":"IBM zSystems"},{"location":"prerequisites/#ftp-server","text":"On the same network as your IBM zSystems / LinuxONE hardware. Red Hat Enterprise Linux (RHEL) 8 for s390x architecture mounted in an accessible folder (e.g. /home/ftpuser/rhel/) If you do not yet have RHEL for s390x, go to the Red Hat Customer Portal and download it. Under 'Product Variant' use the drop-down menu to select 'Red Hat Enterprise Linux for IBM z Systems' Double-check it's for version 8 and for s390x architecture Then scroll down to Red Hat Enterprise Linux 8.x Binary DVD and click on the 'Download Now' button. A folder to store config files (e.g. /home/ftpuser/ocp-config) A user with sudo and SSH access.","title":"FTP Server"},{"location":"prerequisites/#ansible-controller","text":"The computer/virtual machine running Ansible, sometimes referred to as localhost. Must be running on with MacOS or Linux operating systems. Network access to your IBM zSystems / LinuxONE hardware All you need to run Ansible is a terminal and a text editor. However, an IDE like VS Code is highly recommended for an integrated, user-friendly experience with helpful extensions like YAML . Python3 installed: brew install python3 #MacOS sudo dnf install python3 #Fedora sudo apt install python3 #Debian Once Python3 is installed, you also need Ansible version 2.9 or above: pip3 install ansible Once Ansible is installed, you will need a few collections from Ansible Galaxy. Use this template to install them via the terminal: ansible-galaxy collection install community.general community.crypto ansible.posix community.libvirt If you will be using these playbooks to automate the creation of the LPAR(s) that will act as KVM host(s) for the cluster, you will also need: pip3 install zhmcclient cryptography packaging PyYAML ansible-galaxy collection install ibm.ibm_zhmc If you are using MacOS, you also need to have Homebrew package manager installed: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" and Xcode : xcode-select --install","title":"Ansible Controller"},{"location":"run-the-playbooks/","text":"Step 4: Run the Playbooks # Overview # Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). Run this shell command: ansible-playbook playbooks/0_setup.yaml Run each part step-by-step by running one playbook at a time, or all at once using playbooks/site.yaml . Here's the full list of playbooks to be run in order, full descriptions of each can be found further down the page: 0_setup.yaml ( code ) 1_create_lpar.yaml ( code ) 2_create_kvm_host.yaml ( code ) 3_setup_kvm_host.yaml ( code ) 4_create_bastion.yaml ( code ) 5_setup_bastion.yaml ( code ) 6_create_nodes.yaml ( code ) 7_ocp_verification.yaml ( code ) Watch Ansible as it completes the installation, correcting errors if they arise. To look at what tasks are running in detail, open the playbook or roles/role-name/tasks/main.yaml Alternatively, to run all the playbooks at once, start the master playbook by running this shell command: ansible-playbook playbooks/site.yaml If the process fails in error, go through the steps in the troubleshooting page. At the end of the the last playbook, follow the printed instructions for first-time login to the cluster. 0 Setup Playbook # Overview # First-time setup of the Ansible Controller, the machine running Ansible. Outcomes # Packages and Ansible Galaxy collections are confirmed to be installed properly. host_vars files are confirmed to match KVM host(s) hostnames. Ansible inventory is templated out and working properly. SSH key generated for Ansible passwordless authentication. SSH agent is setup on the Ansible Controller. Ansible SSH key is copied to the FTP server. Notes # You can use an existing SSH key as your Ansible key, or have Ansible create one for you. It is highly recommended to use one without a passphrase. 1 Create LPAR Playbook # Overview # Creation of one to three Logical Partitions (LPARs), depending on your configuration. Uses the Hardware Management Console (HMC) API, so your system must be in Dynamic Partition Manager (DPM) mode. Outcomes # One to three LPARs created. One to two Networking Interface Cards (NICs) attached per LPAR. One to two storage groups attached per LPAR. LPARs are in 'Stopped' state. Notes # Recommend opening the HMC via web-browser to watch the LPARs come up. 2 Create KVM Host Playbook # Overview # First-time start-up of Red Hat Enterprise Linux installed natively on the LPAR(s). Uses the Hardware Management Console (HMC) API, so your system must be in Dynamic Partition Manager (DPM) mode. Configuration files are passed to the FTP server and RHEL is booted and then kickstarted for fully automated setup. Outcomes # LPAR(s) started up in 'Active' state. Configuration files (cfg, ins, prm) for the KVM host(s) are on the FTP server in the provided configs directory. Notes # Recommended to open the HMC via web-browser to watch the Operating System Messages for each LPAR as they boot in order to debug any potential problems. 3 Setup KVM Host Playbook # Overview # Configures the RHEL server(s) installed natively on the LPAR(s) to act as virtualization hypervisor(s) to host the virtual machines that make up the eventual cluster. Outcomes # Ansible SSH key is copied to all KVM hosts for passwordless authentication. RHEL subscription is auto-attached to all KVM hosts. Software packages specified in group_vars/all.yaml have been installed. Cockpit console enabled for Graphical User Interface via web browser. Go to http://kvm-ip-here:9090 to view it. Libvirt is started and enabled. Logical volume group that was created during kickstart is extended to fill all available space. A macvtap bridge has been created on the host's networking interface. Notes # If you're using a pre-existing LPAR, take a look at roles/configure_storage/tasks/main.yaml to make sure that the commands that will be run to extend the logical volume will work. Storage configurations can vary widely. The values there are the defaults from using autopart during kickstart. Also be aware that if lpar.storage_group_2.auto_config is True, the role roles/configure_storage/tasks/main.yaml will be non-idempotent. Meaning, it will fail if you run it twice. 4 Create Bastion Playbook # Overview # Creates the bastion KVM guest on the first KVM host. The bastion hosts essential services for the cluster. If you already have a bastion server, that can be used instead of running this playbook. Outcomes # RHEL ISO is mounted to HTTP-accessible directory on the FTP server. Bastion configs are templated out to the FTP server. Bastion is booted using virt-install. Bastion is kickstarted for fully automated setup of the operating system. Notes # This can be a particularly sticky part of the process. If any of the variables used in the virt-install or kickstart are off, the bastion won't be able to boot. Recommend watching it come up from the first KVM host's cockpit. Go to http://kvm-ip-here:9090 via web-browser to view it. You'll have to sign in, enable administrative access (top right), and then click on the virtual machines tab on the left-hand toolbar. 5 Setup Bastion Playbook # Overview # Configuration of the bastion to host essential infrastructure services for the cluster. Can be first-time setup or use an existing server. Outcomes # Ansible SSH key copied to bastion for passwordless authentication. Software packages specified in group_vars/all.yaml have been installed. An OCP-specific SSH key is generated for passing into the install-config (then passed to the nodes). Firewall is configured to permit traffic through the necessary ports. Domain Name Server (DNS) configured to resolve cluster's IP addresses and APIs. Only done if env.bastion.options.dns is true. DNS is checked to make sure all the necessary Fully Qualified Domain Names, including APIs resolve properly. Also ensures outside access is working. High Availability Proxy (HAProxy) load balancer is configured. Only done if env.bastion.options.loadbalancer.on_bastion is true. If the the cluster is to be highly available (meaning spread across more than one LPAR), an OpenVPN server is setup on the bastion to allow for the KVM hosts to communicate between eachother. OpenVPN clients are configured on the KVM hosts. CoreOS roofts is pulled to the bastion if not already there. OCP client and installer are pulled down if not there already. oc, kubectl and openshift-install binaries are installed. OCP install-config is templated and backed up. Manfifests are created. OCP install directory found at /root/ocpinst/ is created and populated with necessary files. Ignition files for the bootstrap, control, and compute nodes are transferred to HTTP-accessible directory for booting nodes. Notes # The stickiest part is DNS setup and get_ocp role at the end. 6 Create Nodes Playbook # Overview # OCP cluster's nodes are created and the control plane is bootstrapped. Outcomes # CoreOS initramfs and kernel are pulled down. Control nodes are created and bootstrapped. Bootstrap has been created, done its job connecting the control plane, and is then destroyed. Compute nodes are created, as many as is specified in groups_vars/all.yaml. Infra nodes, if defined in group_vars/all.yaml have been created, but are at this point essentially just compute nodes. Notes # To watch the bootstrap do its job connecting the control plane: first, SSH to the bastion, then change to root (sudo -i), from there SSH to the bootstrap node as user 'core' (e.g. ssh core@bootstrap-ip). Once you're in the bootstrap run 'journalctl -b -f -u release-image.service -u bootkube.service'. Expect many errors as the control planes come up. You're waiting for the message 'bootkube.service complete' If the cluster is highly available, the bootstrap node will be created on the last (usually third) KVM host in the group. Since the bastion is on the first host, this was done to spread out the load. 7 OCP Verification Playbook # Overview # Final steps of waiting for and verifying the OpenShift cluster to complete its installation. Outcomes # Certificate Signing Requests (CSRs) have been approved. All nodes are in ready state. All cluster operators are available. OpenShift installation is verified to be complete. Temporary credentials and URL are printed to allow easy first-time login to the cluster. Notes # These steps may take a long time and the tasks are very repetitive because of that. If your cluster has a very large number of compute nodes or insufficient resources, more rounds of approvals and time may be needed for these tasks. If you made it this far, congratulations! To install a new cluster, copy your inventory directory, change the default in the ansible.cfg, change the variables, and start again. With all the customizations to the playbooks you made along the way still intact.","title":"4 Run the Playbooks"},{"location":"run-the-playbooks/#step-4-run-the-playbooks","text":"","title":"Step 4: Run the Playbooks"},{"location":"run-the-playbooks/#overview","text":"Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). Run this shell command: ansible-playbook playbooks/0_setup.yaml Run each part step-by-step by running one playbook at a time, or all at once using playbooks/site.yaml . Here's the full list of playbooks to be run in order, full descriptions of each can be found further down the page: 0_setup.yaml ( code ) 1_create_lpar.yaml ( code ) 2_create_kvm_host.yaml ( code ) 3_setup_kvm_host.yaml ( code ) 4_create_bastion.yaml ( code ) 5_setup_bastion.yaml ( code ) 6_create_nodes.yaml ( code ) 7_ocp_verification.yaml ( code ) Watch Ansible as it completes the installation, correcting errors if they arise. To look at what tasks are running in detail, open the playbook or roles/role-name/tasks/main.yaml Alternatively, to run all the playbooks at once, start the master playbook by running this shell command: ansible-playbook playbooks/site.yaml If the process fails in error, go through the steps in the troubleshooting page. At the end of the the last playbook, follow the printed instructions for first-time login to the cluster.","title":"Overview"},{"location":"run-the-playbooks/#0-setup-playbook","text":"","title":"0 Setup Playbook"},{"location":"run-the-playbooks/#overview_1","text":"First-time setup of the Ansible Controller, the machine running Ansible.","title":"Overview"},{"location":"run-the-playbooks/#outcomes","text":"Packages and Ansible Galaxy collections are confirmed to be installed properly. host_vars files are confirmed to match KVM host(s) hostnames. Ansible inventory is templated out and working properly. SSH key generated for Ansible passwordless authentication. SSH agent is setup on the Ansible Controller. Ansible SSH key is copied to the FTP server.","title":"Outcomes"},{"location":"run-the-playbooks/#notes","text":"You can use an existing SSH key as your Ansible key, or have Ansible create one for you. It is highly recommended to use one without a passphrase.","title":"Notes"},{"location":"run-the-playbooks/#1-create-lpar-playbook","text":"","title":"1 Create LPAR Playbook"},{"location":"run-the-playbooks/#overview_2","text":"Creation of one to three Logical Partitions (LPARs), depending on your configuration. Uses the Hardware Management Console (HMC) API, so your system must be in Dynamic Partition Manager (DPM) mode.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_1","text":"One to three LPARs created. One to two Networking Interface Cards (NICs) attached per LPAR. One to two storage groups attached per LPAR. LPARs are in 'Stopped' state.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_1","text":"Recommend opening the HMC via web-browser to watch the LPARs come up.","title":"Notes"},{"location":"run-the-playbooks/#2-create-kvm-host-playbook","text":"","title":"2 Create KVM Host Playbook"},{"location":"run-the-playbooks/#overview_3","text":"First-time start-up of Red Hat Enterprise Linux installed natively on the LPAR(s). Uses the Hardware Management Console (HMC) API, so your system must be in Dynamic Partition Manager (DPM) mode. Configuration files are passed to the FTP server and RHEL is booted and then kickstarted for fully automated setup.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_2","text":"LPAR(s) started up in 'Active' state. Configuration files (cfg, ins, prm) for the KVM host(s) are on the FTP server in the provided configs directory.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_2","text":"Recommended to open the HMC via web-browser to watch the Operating System Messages for each LPAR as they boot in order to debug any potential problems.","title":"Notes"},{"location":"run-the-playbooks/#3-setup-kvm-host-playbook","text":"","title":"3 Setup KVM Host Playbook"},{"location":"run-the-playbooks/#overview_4","text":"Configures the RHEL server(s) installed natively on the LPAR(s) to act as virtualization hypervisor(s) to host the virtual machines that make up the eventual cluster.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_3","text":"Ansible SSH key is copied to all KVM hosts for passwordless authentication. RHEL subscription is auto-attached to all KVM hosts. Software packages specified in group_vars/all.yaml have been installed. Cockpit console enabled for Graphical User Interface via web browser. Go to http://kvm-ip-here:9090 to view it. Libvirt is started and enabled. Logical volume group that was created during kickstart is extended to fill all available space. A macvtap bridge has been created on the host's networking interface.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_3","text":"If you're using a pre-existing LPAR, take a look at roles/configure_storage/tasks/main.yaml to make sure that the commands that will be run to extend the logical volume will work. Storage configurations can vary widely. The values there are the defaults from using autopart during kickstart. Also be aware that if lpar.storage_group_2.auto_config is True, the role roles/configure_storage/tasks/main.yaml will be non-idempotent. Meaning, it will fail if you run it twice.","title":"Notes"},{"location":"run-the-playbooks/#4-create-bastion-playbook","text":"","title":"4 Create Bastion Playbook"},{"location":"run-the-playbooks/#overview_5","text":"Creates the bastion KVM guest on the first KVM host. The bastion hosts essential services for the cluster. If you already have a bastion server, that can be used instead of running this playbook.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_4","text":"RHEL ISO is mounted to HTTP-accessible directory on the FTP server. Bastion configs are templated out to the FTP server. Bastion is booted using virt-install. Bastion is kickstarted for fully automated setup of the operating system.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_4","text":"This can be a particularly sticky part of the process. If any of the variables used in the virt-install or kickstart are off, the bastion won't be able to boot. Recommend watching it come up from the first KVM host's cockpit. Go to http://kvm-ip-here:9090 via web-browser to view it. You'll have to sign in, enable administrative access (top right), and then click on the virtual machines tab on the left-hand toolbar.","title":"Notes"},{"location":"run-the-playbooks/#5-setup-bastion-playbook","text":"","title":"5 Setup Bastion Playbook"},{"location":"run-the-playbooks/#overview_6","text":"Configuration of the bastion to host essential infrastructure services for the cluster. Can be first-time setup or use an existing server.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_5","text":"Ansible SSH key copied to bastion for passwordless authentication. Software packages specified in group_vars/all.yaml have been installed. An OCP-specific SSH key is generated for passing into the install-config (then passed to the nodes). Firewall is configured to permit traffic through the necessary ports. Domain Name Server (DNS) configured to resolve cluster's IP addresses and APIs. Only done if env.bastion.options.dns is true. DNS is checked to make sure all the necessary Fully Qualified Domain Names, including APIs resolve properly. Also ensures outside access is working. High Availability Proxy (HAProxy) load balancer is configured. Only done if env.bastion.options.loadbalancer.on_bastion is true. If the the cluster is to be highly available (meaning spread across more than one LPAR), an OpenVPN server is setup on the bastion to allow for the KVM hosts to communicate between eachother. OpenVPN clients are configured on the KVM hosts. CoreOS roofts is pulled to the bastion if not already there. OCP client and installer are pulled down if not there already. oc, kubectl and openshift-install binaries are installed. OCP install-config is templated and backed up. Manfifests are created. OCP install directory found at /root/ocpinst/ is created and populated with necessary files. Ignition files for the bootstrap, control, and compute nodes are transferred to HTTP-accessible directory for booting nodes.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_5","text":"The stickiest part is DNS setup and get_ocp role at the end.","title":"Notes"},{"location":"run-the-playbooks/#6-create-nodes-playbook","text":"","title":"6 Create Nodes Playbook"},{"location":"run-the-playbooks/#overview_7","text":"OCP cluster's nodes are created and the control plane is bootstrapped.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_6","text":"CoreOS initramfs and kernel are pulled down. Control nodes are created and bootstrapped. Bootstrap has been created, done its job connecting the control plane, and is then destroyed. Compute nodes are created, as many as is specified in groups_vars/all.yaml. Infra nodes, if defined in group_vars/all.yaml have been created, but are at this point essentially just compute nodes.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_6","text":"To watch the bootstrap do its job connecting the control plane: first, SSH to the bastion, then change to root (sudo -i), from there SSH to the bootstrap node as user 'core' (e.g. ssh core@bootstrap-ip). Once you're in the bootstrap run 'journalctl -b -f -u release-image.service -u bootkube.service'. Expect many errors as the control planes come up. You're waiting for the message 'bootkube.service complete' If the cluster is highly available, the bootstrap node will be created on the last (usually third) KVM host in the group. Since the bastion is on the first host, this was done to spread out the load.","title":"Notes"},{"location":"run-the-playbooks/#7-ocp-verification-playbook","text":"","title":"7 OCP Verification Playbook"},{"location":"run-the-playbooks/#overview_8","text":"Final steps of waiting for and verifying the OpenShift cluster to complete its installation.","title":"Overview"},{"location":"run-the-playbooks/#outcomes_7","text":"Certificate Signing Requests (CSRs) have been approved. All nodes are in ready state. All cluster operators are available. OpenShift installation is verified to be complete. Temporary credentials and URL are printed to allow easy first-time login to the cluster.","title":"Outcomes"},{"location":"run-the-playbooks/#notes_7","text":"These steps may take a long time and the tasks are very repetitive because of that. If your cluster has a very large number of compute nodes or insufficient resources, more rounds of approvals and time may be needed for these tasks. If you made it this far, congratulations! To install a new cluster, copy your inventory directory, change the default in the ansible.cfg, change the variables, and start again. With all the customizations to the playbooks you made along the way still intact.","title":"Notes"},{"location":"set-variables-group-vars/","text":"Step 2: Set Variables (group_vars) # Overview # In a text editor of your choice, open the template of the environment variables file . Make a copy of it called all.yaml and paste it into the same directory with its template. all.yaml is your master variables file and you will likely reference it many times throughout the process. The default inventory can be found at inventories/default . The variables marked with an X are required to be filled in. Many values are pre-filled or are optional. Optional values are commented out; in order to use them, remove the # and fill them in. This is the most important step in the process. Take the time to make sure everything here is correct. Note on YAML syntax : Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file env and z don't need to be filled in, but the cpc_name does. There are X's where input is required to help you with this. Scroll the table to the right to see examples for each variable. 1 - Controller # Variable Name Description Example env.controller.sudo_pass The password to the machine running Ansible (localhost). This will only be used for two things. To ensure you've installed the pre-requisite packages if you're on Linux, and to add the login URL to your /etc/hosts file. Pas$w0rd! 2 - LPAR(s) # Variable Name Description Example env.z.high_availability Is this cluster spread across three LPARs? If yes, mark True. If not (just in one LPAR), mark False True env.z.lpar1.create To have Ansible create an LPAR and install RHEL on it for the KVM host, mark True. If using a pre-existing LPAR with RHEL already installed, mark False. True env.z.lpar1.hostname The hostname of the KVM host. kvm-host-01 env.z.lpar1.ip The IPv4 address of the KVM host. 192.168.10.1 env.z.lpar1.user Username for Linux admin on KVM host 1. Recommended to run as a non-root user with sudo access. admin env.z.lpar1.pass The password for the user that will be created or exists on the KVM host. ch4ngeMe! env.z.lpar2.create To create a second LPAR and install RHEL on it to act as another KVM host, mark True. If using pre-existing LPAR(s) with RHEL already installed, mark False. True env.z.lpar2.hostname (Optional) The hostname of the second KVM host. kvm-host-02 env.z.lpar2.ip (Optional) The IPv4 address of the second KVM host. 192.168.10.2 env.z.lpar2.user Username for Linux admin on KVM host 2. Recommended to run as a non-root user with sudo access. admin env.z.lpar2.pass (Optional) The password for the admin user on the second KVM host. ch4ngeMe! env.z.lpar3.create To create a third LPAR and install RHEL on it to act as another KVM host, mark True. If using pre-existing LPAR(s) with RHEL already installed, mark False. True env.z.lpar3.hostname (Optional) The hostname of the third KVM host. kvm-host-03 env.z.lpar3.ip (Optional) The IPv4 address of the third KVM host. 192.168.10.3 env.z.lpar3.user Username for Linux admin on KVM host 3. Recommended to run as a non-root user with sudo access. admin env.z.lpar3.pass (Optional) The password for the admin user on the third KVM host. ch4ngeMe! 3 - FTP Server # Variable Name Description Example env.ftp.ip IPv4 address for the FTP server that will be used to pass config files and iso to KVM host LPAR(s) and bastion VM during their first boot. 192.168.10.201 env.ftp.user Username to connect to the FTP server. Must have sudo and SSH access. ftp-user env.ftp.pass Password to connect to the FTP server as above user. FTPpa$s! env.ftp.iso_mount_dir Directory path relative to FTP root where RHEL ISO is mounted. If FTP root is /var/ftp/pub and the ISO is mounted at /var/ftp/pub/RHEL/8.5 then this variable would be RHEL/8.5. No slash before or after. RHEL/8.5 env.ftp.cfgs_dir Directory path relative to FTP root where configuration files can be stored. If FTP root is /var/ftp/pub and you would like to store the configs at /var/ftp/pub/ocpz-config then this variable would be ocpz-config. No slash before or after. ocpz-config 4 - Red Hat Info # Variable Name Description Example env.redhat.username Red Hat username with a valid license or free trial to Red Hat OpenShift Container Platform (RHOCP), which comes with necessary licenses for Red Hat Enterprise Linux (RHEL) and Red Hat CoreOS (RHCOS). redhat.user env.redhat.password Password to Red Hat above user's account. Used to auto-attach necessary subscriptions to KVM Host, bastion VM, and pull live images for OpenShift. rEdHatPa$s! env.redhat.pull_secret Pull secret for OpenShift, comes from Red Hat's Hybrid Cloud Console . Make sure to enclose in 'single quotes'. '{\"auths\":{\"cloud.openshift .com\":{\"auth\":\"b3Blb ... 4yQQ==\",\"email\":\"redhat. user@gmail.com\"}}}' 5 - Bastion # Variable Name Description Example env.bastion.create True or False. Would you like to create a bastion KVM guest to host essential infrastructure services like DNS, load balancer, firewall, etc? Can de-select certain services with the env.bastion.options variables below. True env.bastion.vm_name Name of the bastion VM. Arbitrary value. bastion env.bastion.resources.disk_size How much of the storage pool would you like to allocate to the bastion (in Gigabytes)? Recommended 30 or more. 30 env.bastion.resources.ram How much memory would you like to allocate the bastion (in megabytes)? Recommended 4096 or more 4096 env.bastion.resources.swap How much swap storage would you like to allocate the bastion (in megabytes)? Recommended 4096 or more. 4096 env.bastion.resources.vcpu How many virtual CPUs would you like to allocate to the bastion? Recommended 4 or more. 4 env.bastion.resources.os_variant Version of Red Hat Enterprise Linux to use for the bastion's operating system. Recommended 8.4 and above. Must match version of mounted ISO on the FTP server. 8.5 env.bastion.networking.ip IPv4 address for the bastion. 192.168.10.3 env.bastion.networking.hostname Hostname of the bastion. Will be combined with env.bastion.networking.base_domain to create a Fully Qualified Domain Name (FQDN). ocpz-bastion env.bastion.networking. subnetmask Subnet of the bastion. 255.255.255.0 env.bastion.networking.gateway IPv4 of he bastion's gateway server. 192.168.10.0 env.bastion.networking.name server1 IPv4 address of the server that resolves the bastion's hostname. 192.168.10.200 env.bastion.networking.name server2 (Optional) A second IPv4 address that resolves the bastion's hostname. 192.168.10.201 env.bastion.networking.interface Name of the networking interface on the bastion from Linux's perspective. Most likely enc1. enc1 env.bastion.networking.base_ domain Base domain that, when combined with the hostname, creates a fully-qualified domain name (FQDN) for the bastion? ihost.com env.bastion.access.user What would you like the admin's username to be on the bastion? If root, make pass and root_pass vars the same. admin env.bastion.access.pass The password to the bastion's admin user. If using root, make pass and root_pass vars the same. cH4ngeM3! env.bastion.access.root_pass The root password for the bastion. If using root, make pass and root_pass vars the same. R0OtPa$s! env.bastion.options.dns Would you like the bastion to host the DNS information for the cluster? True or False. If false, resolution must come from elsewhere in your environment. Make sure to add IP addresses for KVM hosts, bastion, bootstrap, control, compute nodes, AND api, api-int and *.apps as described here in section \"User-provisioned DNS Requirements\" Table 5. If True this will be done for you in the dns and check_dns roles. True env.bastion.options.load balancer.on_bastion Would you like the bastion to host the load balancer (HAProxy) for the cluster? True or False (boolean). If false, this service must be provided elsewhere in your environment, and public and private IP of the load balancer must be provided in the following two variables. True env.bastion.options.load balancer.public_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The public IPv4 address for your environment's loadbalancer. api, apps, *.apps must use this. 192.168.10.50 env.bastion.options.load balancer.private_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The private IPv4 address for your environment's loadbalancer. api-int must use this. 10.24.17.12 6 - Cluster Networking # Variable Name Description Example env.cluster.networking.metadata_name Name to describe the cluster as a whole, can be anything if DNS will be hosted on the bastion. If DNS is not on the bastion, must match your DNS configuration. Will be combined with the base_domain and hostnames to create Fully Qualified Domain Names (FQDN). ocpz env.cluster.networking.base_domain The site name, where is the cluster being hosted? This will be combined with the metadata_name and hostnames to create FQDNs. ihost.com env.cluster.networking.nameserver1 IPv4 address that the cluster get its hostname resolution from. If env.bastion.options.dns is True, this should be the IP address of the bastion. 192.168.10.200 env.cluster.networking.nameserver2 (Optional) A second IPv4 address will the cluster get its hostname resolution from? If env.bastion.options.dns is True, this should be left commented out. 192.168.10.201 env.cluster.networking.forwarder What IPv4 address will be used to make external DNS calls? Can use 1.1.1.1 or 8.8.8.8 as defaults. 8.8.8.8 7 - Bootstrap Node # Variable Name Description Example env.cluster.nodes.bootstrap.disk_size How much disk space do you want to allocate to the bootstrap node (in Gigabytes)? Bootstrap node is temporary and will be brought down automatically when its job completes. 120 or more recommended. 120 env.cluster.nodes.bootstrap.ram How much memory would you like to allocate to the temporary bootstrap node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.bootstrap.vcpu How many virtual CPUs would you like to allocate to the temporary bootstrap node? Recommended 4 or more. 4 env.cluster.nodes.bootstrap.vm_name Name of the temporary bootstrap node VM. Arbitrary value. bootstrap env.cluster.nodes.bootstrap.ip IPv4 address of the temporary bootstrap node. 192.168.10.4 env.cluster.nodes.bootstrap.hostname Hostname of the temporary boostrap node. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). bootstrap-ocpz 8 - Control Nodes # Variable Name Description Example env.cluster.nodes.control.disk_size How much disk space do you want to allocate to each control node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.control.ram How much memory would you like to allocate to the each control node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.control.vcpu How many virtual CPUs would you like to allocate to each control node? Recommended 4 or more. 4 env.cluster.nodes.control.vm_name Name of the control node VMs. Arbitrary values. Usually no more or less than 3 are used. Must match the total number of IP addresses and hostnames for control nodes. Use provided list format. control-1 control-2 control-3 env.cluster.nodes.control.ip IPv4 address of the control nodes. Use provided list formatting. 192.168.10.5 192.168.10.6 192.168.10.7 env.cluster.nodes.control.hostname Hostnames for control nodes. Must match the total number of IP addresses for control nodes (usually 3). If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). control-01 control-02 control-03 9 - Compute Nodes # Variable Name Description Example env.cluster.nodes.compute.disk_size How much disk space do you want to allocate to each compute node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.compute.ram How much memory would you like to allocate to the each compute node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.compute.vcpu How many virtual CPUs would you like to allocate to each compute node? Recommended 2 or more. 2 env.cluster.nodes.compute.vm_name Name of the compute node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for compute nodes. Use provided list format. compute-1 compute-2 env.cluster.nodes.compute.ip IPv4 address of the compute nodes. Must match the total number of VM names and hostnames for compute nodes. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.compute.hostname Hostnames for compute nodes. Must match the total number of IP addresses and VM names for compute nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). compute-01 compute-02 10 - Infra Nodes # Variable Name Description Example env.cluster.nodes.infra.disk_size (Optional) Set up compute nodes that are made for infrastructure workloads (ingress, monitoring, logging)? How much disk space do you want to allocate to each infra node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.infra.ram (Optional) How much memory would you like to allocate to the each infra node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.infra.vcpu (Optional) How many virtual CPUs would you like to allocate to each infra node? Recommended 2 or more. 2 env.cluster.nodes.infra.vm_name (Optional) Name of additional infra node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for infra nodes. Use provided list format. infra-1 infra-2 env.cluster.nodes.infra.ip (Optional) IPv4 address of the infra nodes. This list can be expanded to any number of nodes, minimum 2. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.infra.hostname (Optional) Hostnames for infra nodes. Must match the total number of IP addresses for infra nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). infra-01 infra-02 11 - (Optional) Packages # Variable Name Description Example env.pkgs.galaxy A list of Ansible Galaxy collections that will be installed during the setup playbook. The collections listed are required. Feel free to add more as needed, just make sure to follow the same list format. community.general env.pkgs.controller A list of packages that will be installed on the machine running Ansible during the setup playbook. Feel free to add more as needed, just make sure to follow the same list format. openssh env.pkgs.kvm A list of packages that will be installed on the KVM Host during the setup_kvm_host playbook. Feel free to add more as needed, just make sure to follow the same list format. qemu-kvm env.pkgs.bastion A list of packages that will be installed on the bastion during the setup_bastion playbook. Feel free to add more as needed, just make sure to follow the same list format. haproxy 12 - (Optional) Mirror Links # Variable Name Description Example env.openshift.client Link to the mirror for the OpenShift client from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4/s390x/clients /ocp/stable/openshift- client-linux.tar.gz env.openshift.installer Link to the mirror for the OpenShift installer from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4/s390x /clients/ocp/stable/openshift- install-linux.tar.gz env.coreos.kernel Link to the mirror of the CoreOS kernel to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4/s390x /dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-kernel-s390x env.coreos.initramfs Link to the mirror of the CoreOS initramfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-initramfs.s390x.img env.coreos.rootfs Link to the mirror of the CoreOS rootfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0- s390x-live-rootfs.s390x.img 13 - (Optional) OCP Install Config # Variable Name Description Example env.install_config.api_version Kubernetes API version for the cluster. These install_config variables will be passed to the OCP install_config file. This file is templated in the get_ocp role during the setup_bastion playbook. To make more fine-tuned adjustments to the install_config, you can find it at roles/get_ocp/templates/install-config.yaml.j2 v1 env.install_config.compute.architecture Computing architecture for the compute nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.compute.hyperthreading Enable or disable hyperthreading on compute nodes. Recommended enabled. Enabled env.install_config.control.architecture Computing architecture for the control nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.control.hyperthreading Enable or disable hyperthreading on control nodes. Recommended enabled. Enabled env.install_config.cluster_network.cidr IPv4 block in Internal cluster networking in Classless Inter-Domain Routing (CIDR) notation. Recommended to keep as is. 10.128.0.0/14 env.install_config.cluster_network.host_prefix The subnet prefix length to assign to each individual node. For example, if hostPrefix is set to 23 then each node is assigned a /23 subnet out of the given cidr. A hostPrefix value of 23 provides 510 (2^(32 - 23) - 2) pod IP addresses. 23 env.install_config.cluster_network.type The cluster network provider Container Network Interface (CNI) plug-in to install. Either OpenShiftSDN (recommended) or OVNKubernetes. OpenShiftSDN env.install_config.service_network The IP address block for services. The default value is 172.30.0.0/16. The OpenShift SDN and OVN-Kubernetes network providers support only a single IP address block for the service network. An array with an IP address block in CIDR format. 172.30.0.0/16 env.install_config.fips True or False (boolean) for whether or not to use the United States' Federal Information Processing Standards (FIPS). Not yet certified on IBM zSystems. Enclosed in 'single quotes'. 'false' 14 - (Optional) Proxy # Variable Name Description Example proxy_env.http_proxy (Optional) A proxy URL to use for creating HTTP connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: http://username:pswd>@ip:port http://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.https_proxy (Optional) A proxy URL to use for creating HTTPS connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: https://username:pswd@ip:port https://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.no_proxy (Optional) A comma-separated list (no spaces) of destination domain names, IP addresses, or other network CIDRs to exclude from proxying. When using a proxy, all necessary IPs and domains for your cluster will be added automatically. See roles/get_ocp/templates/install-config.yaml.j2 for more details on the template. Preface a domain with . to match subdomains only. For example, .y.com matches x.y.com, but not y.com. Use * to bypass the proxy for all listed destinations. example.com,192.168.10.1 15 - (Optional) Misc # Variable Name Description Example env.language What language would you like Red Hat Enterprise Linux to use? In UTF-8 language code. Available languages and their corresponding codes can be found here , in the \"Locale\" column of Table 2.1. en_US.UTF-8 env.timezone Which timezone would you like Red Hat Enterprise Linux to use? A list of available timezone options can be found here . America/New_York env.ansible_key_name (Optional) Name of the SSH key that Ansible will use to connect to hosts. ansible-ocpz env.ocp_key_name Comment to describe the SSH key used for OCP. Arbitrary value. OCPZ-01 key env.bridge_name (Optional) Name of the macvtap bridge that will be created on the KVM host. macvtap-net","title":"2 Set Variables (group_vars)"},{"location":"set-variables-group-vars/#step-2-set-variables-group_vars","text":"","title":"Step 2: Set Variables (group_vars)"},{"location":"set-variables-group-vars/#overview","text":"In a text editor of your choice, open the template of the environment variables file . Make a copy of it called all.yaml and paste it into the same directory with its template. all.yaml is your master variables file and you will likely reference it many times throughout the process. The default inventory can be found at inventories/default . The variables marked with an X are required to be filled in. Many values are pre-filled or are optional. Optional values are commented out; in order to use them, remove the # and fill them in. This is the most important step in the process. Take the time to make sure everything here is correct. Note on YAML syntax : Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file env and z don't need to be filled in, but the cpc_name does. There are X's where input is required to help you with this. Scroll the table to the right to see examples for each variable.","title":"Overview"},{"location":"set-variables-group-vars/#1-controller","text":"Variable Name Description Example env.controller.sudo_pass The password to the machine running Ansible (localhost). This will only be used for two things. To ensure you've installed the pre-requisite packages if you're on Linux, and to add the login URL to your /etc/hosts file. Pas$w0rd!","title":"1 - Controller"},{"location":"set-variables-group-vars/#2-lpars","text":"Variable Name Description Example env.z.high_availability Is this cluster spread across three LPARs? If yes, mark True. If not (just in one LPAR), mark False True env.z.lpar1.create To have Ansible create an LPAR and install RHEL on it for the KVM host, mark True. If using a pre-existing LPAR with RHEL already installed, mark False. True env.z.lpar1.hostname The hostname of the KVM host. kvm-host-01 env.z.lpar1.ip The IPv4 address of the KVM host. 192.168.10.1 env.z.lpar1.user Username for Linux admin on KVM host 1. Recommended to run as a non-root user with sudo access. admin env.z.lpar1.pass The password for the user that will be created or exists on the KVM host. ch4ngeMe! env.z.lpar2.create To create a second LPAR and install RHEL on it to act as another KVM host, mark True. If using pre-existing LPAR(s) with RHEL already installed, mark False. True env.z.lpar2.hostname (Optional) The hostname of the second KVM host. kvm-host-02 env.z.lpar2.ip (Optional) The IPv4 address of the second KVM host. 192.168.10.2 env.z.lpar2.user Username for Linux admin on KVM host 2. Recommended to run as a non-root user with sudo access. admin env.z.lpar2.pass (Optional) The password for the admin user on the second KVM host. ch4ngeMe! env.z.lpar3.create To create a third LPAR and install RHEL on it to act as another KVM host, mark True. If using pre-existing LPAR(s) with RHEL already installed, mark False. True env.z.lpar3.hostname (Optional) The hostname of the third KVM host. kvm-host-03 env.z.lpar3.ip (Optional) The IPv4 address of the third KVM host. 192.168.10.3 env.z.lpar3.user Username for Linux admin on KVM host 3. Recommended to run as a non-root user with sudo access. admin env.z.lpar3.pass (Optional) The password for the admin user on the third KVM host. ch4ngeMe!","title":"2 - LPAR(s)"},{"location":"set-variables-group-vars/#3-ftp-server","text":"Variable Name Description Example env.ftp.ip IPv4 address for the FTP server that will be used to pass config files and iso to KVM host LPAR(s) and bastion VM during their first boot. 192.168.10.201 env.ftp.user Username to connect to the FTP server. Must have sudo and SSH access. ftp-user env.ftp.pass Password to connect to the FTP server as above user. FTPpa$s! env.ftp.iso_mount_dir Directory path relative to FTP root where RHEL ISO is mounted. If FTP root is /var/ftp/pub and the ISO is mounted at /var/ftp/pub/RHEL/8.5 then this variable would be RHEL/8.5. No slash before or after. RHEL/8.5 env.ftp.cfgs_dir Directory path relative to FTP root where configuration files can be stored. If FTP root is /var/ftp/pub and you would like to store the configs at /var/ftp/pub/ocpz-config then this variable would be ocpz-config. No slash before or after. ocpz-config","title":"3 - FTP Server"},{"location":"set-variables-group-vars/#4-red-hat-info","text":"Variable Name Description Example env.redhat.username Red Hat username with a valid license or free trial to Red Hat OpenShift Container Platform (RHOCP), which comes with necessary licenses for Red Hat Enterprise Linux (RHEL) and Red Hat CoreOS (RHCOS). redhat.user env.redhat.password Password to Red Hat above user's account. Used to auto-attach necessary subscriptions to KVM Host, bastion VM, and pull live images for OpenShift. rEdHatPa$s! env.redhat.pull_secret Pull secret for OpenShift, comes from Red Hat's Hybrid Cloud Console . Make sure to enclose in 'single quotes'. '{\"auths\":{\"cloud.openshift .com\":{\"auth\":\"b3Blb ... 4yQQ==\",\"email\":\"redhat. user@gmail.com\"}}}'","title":"4 - Red Hat Info"},{"location":"set-variables-group-vars/#5-bastion","text":"Variable Name Description Example env.bastion.create True or False. Would you like to create a bastion KVM guest to host essential infrastructure services like DNS, load balancer, firewall, etc? Can de-select certain services with the env.bastion.options variables below. True env.bastion.vm_name Name of the bastion VM. Arbitrary value. bastion env.bastion.resources.disk_size How much of the storage pool would you like to allocate to the bastion (in Gigabytes)? Recommended 30 or more. 30 env.bastion.resources.ram How much memory would you like to allocate the bastion (in megabytes)? Recommended 4096 or more 4096 env.bastion.resources.swap How much swap storage would you like to allocate the bastion (in megabytes)? Recommended 4096 or more. 4096 env.bastion.resources.vcpu How many virtual CPUs would you like to allocate to the bastion? Recommended 4 or more. 4 env.bastion.resources.os_variant Version of Red Hat Enterprise Linux to use for the bastion's operating system. Recommended 8.4 and above. Must match version of mounted ISO on the FTP server. 8.5 env.bastion.networking.ip IPv4 address for the bastion. 192.168.10.3 env.bastion.networking.hostname Hostname of the bastion. Will be combined with env.bastion.networking.base_domain to create a Fully Qualified Domain Name (FQDN). ocpz-bastion env.bastion.networking. subnetmask Subnet of the bastion. 255.255.255.0 env.bastion.networking.gateway IPv4 of he bastion's gateway server. 192.168.10.0 env.bastion.networking.name server1 IPv4 address of the server that resolves the bastion's hostname. 192.168.10.200 env.bastion.networking.name server2 (Optional) A second IPv4 address that resolves the bastion's hostname. 192.168.10.201 env.bastion.networking.interface Name of the networking interface on the bastion from Linux's perspective. Most likely enc1. enc1 env.bastion.networking.base_ domain Base domain that, when combined with the hostname, creates a fully-qualified domain name (FQDN) for the bastion? ihost.com env.bastion.access.user What would you like the admin's username to be on the bastion? If root, make pass and root_pass vars the same. admin env.bastion.access.pass The password to the bastion's admin user. If using root, make pass and root_pass vars the same. cH4ngeM3! env.bastion.access.root_pass The root password for the bastion. If using root, make pass and root_pass vars the same. R0OtPa$s! env.bastion.options.dns Would you like the bastion to host the DNS information for the cluster? True or False. If false, resolution must come from elsewhere in your environment. Make sure to add IP addresses for KVM hosts, bastion, bootstrap, control, compute nodes, AND api, api-int and *.apps as described here in section \"User-provisioned DNS Requirements\" Table 5. If True this will be done for you in the dns and check_dns roles. True env.bastion.options.load balancer.on_bastion Would you like the bastion to host the load balancer (HAProxy) for the cluster? True or False (boolean). If false, this service must be provided elsewhere in your environment, and public and private IP of the load balancer must be provided in the following two variables. True env.bastion.options.load balancer.public_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The public IPv4 address for your environment's loadbalancer. api, apps, *.apps must use this. 192.168.10.50 env.bastion.options.load balancer.private_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The private IPv4 address for your environment's loadbalancer. api-int must use this. 10.24.17.12","title":"5 - Bastion"},{"location":"set-variables-group-vars/#6-cluster-networking","text":"Variable Name Description Example env.cluster.networking.metadata_name Name to describe the cluster as a whole, can be anything if DNS will be hosted on the bastion. If DNS is not on the bastion, must match your DNS configuration. Will be combined with the base_domain and hostnames to create Fully Qualified Domain Names (FQDN). ocpz env.cluster.networking.base_domain The site name, where is the cluster being hosted? This will be combined with the metadata_name and hostnames to create FQDNs. ihost.com env.cluster.networking.nameserver1 IPv4 address that the cluster get its hostname resolution from. If env.bastion.options.dns is True, this should be the IP address of the bastion. 192.168.10.200 env.cluster.networking.nameserver2 (Optional) A second IPv4 address will the cluster get its hostname resolution from? If env.bastion.options.dns is True, this should be left commented out. 192.168.10.201 env.cluster.networking.forwarder What IPv4 address will be used to make external DNS calls? Can use 1.1.1.1 or 8.8.8.8 as defaults. 8.8.8.8","title":"6 - Cluster Networking"},{"location":"set-variables-group-vars/#7-bootstrap-node","text":"Variable Name Description Example env.cluster.nodes.bootstrap.disk_size How much disk space do you want to allocate to the bootstrap node (in Gigabytes)? Bootstrap node is temporary and will be brought down automatically when its job completes. 120 or more recommended. 120 env.cluster.nodes.bootstrap.ram How much memory would you like to allocate to the temporary bootstrap node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.bootstrap.vcpu How many virtual CPUs would you like to allocate to the temporary bootstrap node? Recommended 4 or more. 4 env.cluster.nodes.bootstrap.vm_name Name of the temporary bootstrap node VM. Arbitrary value. bootstrap env.cluster.nodes.bootstrap.ip IPv4 address of the temporary bootstrap node. 192.168.10.4 env.cluster.nodes.bootstrap.hostname Hostname of the temporary boostrap node. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). bootstrap-ocpz","title":"7 - Bootstrap Node"},{"location":"set-variables-group-vars/#8-control-nodes","text":"Variable Name Description Example env.cluster.nodes.control.disk_size How much disk space do you want to allocate to each control node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.control.ram How much memory would you like to allocate to the each control node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.control.vcpu How many virtual CPUs would you like to allocate to each control node? Recommended 4 or more. 4 env.cluster.nodes.control.vm_name Name of the control node VMs. Arbitrary values. Usually no more or less than 3 are used. Must match the total number of IP addresses and hostnames for control nodes. Use provided list format. control-1 control-2 control-3 env.cluster.nodes.control.ip IPv4 address of the control nodes. Use provided list formatting. 192.168.10.5 192.168.10.6 192.168.10.7 env.cluster.nodes.control.hostname Hostnames for control nodes. Must match the total number of IP addresses for control nodes (usually 3). If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). control-01 control-02 control-03","title":"8 - Control Nodes"},{"location":"set-variables-group-vars/#9-compute-nodes","text":"Variable Name Description Example env.cluster.nodes.compute.disk_size How much disk space do you want to allocate to each compute node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.compute.ram How much memory would you like to allocate to the each compute node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.compute.vcpu How many virtual CPUs would you like to allocate to each compute node? Recommended 2 or more. 2 env.cluster.nodes.compute.vm_name Name of the compute node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for compute nodes. Use provided list format. compute-1 compute-2 env.cluster.nodes.compute.ip IPv4 address of the compute nodes. Must match the total number of VM names and hostnames for compute nodes. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.compute.hostname Hostnames for compute nodes. Must match the total number of IP addresses and VM names for compute nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). compute-01 compute-02","title":"9 - Compute Nodes"},{"location":"set-variables-group-vars/#10-infra-nodes","text":"Variable Name Description Example env.cluster.nodes.infra.disk_size (Optional) Set up compute nodes that are made for infrastructure workloads (ingress, monitoring, logging)? How much disk space do you want to allocate to each infra node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.infra.ram (Optional) How much memory would you like to allocate to the each infra node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.infra.vcpu (Optional) How many virtual CPUs would you like to allocate to each infra node? Recommended 2 or more. 2 env.cluster.nodes.infra.vm_name (Optional) Name of additional infra node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for infra nodes. Use provided list format. infra-1 infra-2 env.cluster.nodes.infra.ip (Optional) IPv4 address of the infra nodes. This list can be expanded to any number of nodes, minimum 2. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.infra.hostname (Optional) Hostnames for infra nodes. Must match the total number of IP addresses for infra nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). infra-01 infra-02","title":"10 - Infra Nodes"},{"location":"set-variables-group-vars/#11-optional-packages","text":"Variable Name Description Example env.pkgs.galaxy A list of Ansible Galaxy collections that will be installed during the setup playbook. The collections listed are required. Feel free to add more as needed, just make sure to follow the same list format. community.general env.pkgs.controller A list of packages that will be installed on the machine running Ansible during the setup playbook. Feel free to add more as needed, just make sure to follow the same list format. openssh env.pkgs.kvm A list of packages that will be installed on the KVM Host during the setup_kvm_host playbook. Feel free to add more as needed, just make sure to follow the same list format. qemu-kvm env.pkgs.bastion A list of packages that will be installed on the bastion during the setup_bastion playbook. Feel free to add more as needed, just make sure to follow the same list format. haproxy","title":"11 - (Optional) Packages"},{"location":"set-variables-group-vars/#12-optional-mirror-links","text":"Variable Name Description Example env.openshift.client Link to the mirror for the OpenShift client from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4/s390x/clients /ocp/stable/openshift- client-linux.tar.gz env.openshift.installer Link to the mirror for the OpenShift installer from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4/s390x /clients/ocp/stable/openshift- install-linux.tar.gz env.coreos.kernel Link to the mirror of the CoreOS kernel to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4/s390x /dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-kernel-s390x env.coreos.initramfs Link to the mirror of the CoreOS initramfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-initramfs.s390x.img env.coreos.rootfs Link to the mirror of the CoreOS rootfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0- s390x-live-rootfs.s390x.img","title":"12 - (Optional) Mirror Links"},{"location":"set-variables-group-vars/#13-optional-ocp-install-config","text":"Variable Name Description Example env.install_config.api_version Kubernetes API version for the cluster. These install_config variables will be passed to the OCP install_config file. This file is templated in the get_ocp role during the setup_bastion playbook. To make more fine-tuned adjustments to the install_config, you can find it at roles/get_ocp/templates/install-config.yaml.j2 v1 env.install_config.compute.architecture Computing architecture for the compute nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.compute.hyperthreading Enable or disable hyperthreading on compute nodes. Recommended enabled. Enabled env.install_config.control.architecture Computing architecture for the control nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.control.hyperthreading Enable or disable hyperthreading on control nodes. Recommended enabled. Enabled env.install_config.cluster_network.cidr IPv4 block in Internal cluster networking in Classless Inter-Domain Routing (CIDR) notation. Recommended to keep as is. 10.128.0.0/14 env.install_config.cluster_network.host_prefix The subnet prefix length to assign to each individual node. For example, if hostPrefix is set to 23 then each node is assigned a /23 subnet out of the given cidr. A hostPrefix value of 23 provides 510 (2^(32 - 23) - 2) pod IP addresses. 23 env.install_config.cluster_network.type The cluster network provider Container Network Interface (CNI) plug-in to install. Either OpenShiftSDN (recommended) or OVNKubernetes. OpenShiftSDN env.install_config.service_network The IP address block for services. The default value is 172.30.0.0/16. The OpenShift SDN and OVN-Kubernetes network providers support only a single IP address block for the service network. An array with an IP address block in CIDR format. 172.30.0.0/16 env.install_config.fips True or False (boolean) for whether or not to use the United States' Federal Information Processing Standards (FIPS). Not yet certified on IBM zSystems. Enclosed in 'single quotes'. 'false'","title":"13 - (Optional) OCP Install Config"},{"location":"set-variables-group-vars/#14-optional-proxy","text":"Variable Name Description Example proxy_env.http_proxy (Optional) A proxy URL to use for creating HTTP connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: http://username:pswd>@ip:port http://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.https_proxy (Optional) A proxy URL to use for creating HTTPS connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: https://username:pswd@ip:port https://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.no_proxy (Optional) A comma-separated list (no spaces) of destination domain names, IP addresses, or other network CIDRs to exclude from proxying. When using a proxy, all necessary IPs and domains for your cluster will be added automatically. See roles/get_ocp/templates/install-config.yaml.j2 for more details on the template. Preface a domain with . to match subdomains only. For example, .y.com matches x.y.com, but not y.com. Use * to bypass the proxy for all listed destinations. example.com,192.168.10.1","title":"14 - (Optional) Proxy"},{"location":"set-variables-group-vars/#15-optional-misc","text":"Variable Name Description Example env.language What language would you like Red Hat Enterprise Linux to use? In UTF-8 language code. Available languages and their corresponding codes can be found here , in the \"Locale\" column of Table 2.1. en_US.UTF-8 env.timezone Which timezone would you like Red Hat Enterprise Linux to use? A list of available timezone options can be found here . America/New_York env.ansible_key_name (Optional) Name of the SSH key that Ansible will use to connect to hosts. ansible-ocpz env.ocp_key_name Comment to describe the SSH key used for OCP. Arbitrary value. OCPZ-01 key env.bridge_name (Optional) Name of the macvtap bridge that will be created on the KVM host. macvtap-net","title":"15 - (Optional) Misc"},{"location":"set-variables-host-vars/","text":"Step 3: Set Variables (host_vars) # Overview # Similar to the group_vars file, the host_vars files for each LPAR (KVM host) must be filled in. For each KVM host to be acted upon with Ansible, you must have a corresponding host_vars file named <kvm-hostname>.yaml (i.e. ocpz1.yaml, ocpz2.yaml, ocpz3.yaml), so you must copy and rename the templates found in the host_vars folder accordingly. The variables marked with an X are required to be filled in. Many values are pre-filled or are optional. Optional values are commented out; in order to use them, remove the # and fill them in. Many of the variables in these host_vars files are only required if you are NOT using pre-existing LPARs with RHEL installed. See the Important Note below this first section for more details. This is the most important step in the process. Take the time to make sure everything here is correct. Note on YAML syntax : Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file networking does not need to be filled in, but the hostname does. There are X's where input is required to help you with this. Scroll the table to the right to see examples for each variable. 1 - KVM Host # Variable Name Description Example networking.hostname The hostname of the LPAR with RHEL installed natively (the KVM host). kvm-host-01 networking.ip The IPv4 address of the LPAR with RHEL installed natively (the KVM host). 192.168.10.2 networking.subnetmask The subnet that the LPAR resides in within your network. 255.255.255.0 networking.gateway The IPv4 address of the gateway to the network where the KVM host resides. 192.168.10.0 networking.nameserver1 The IPv4 address from which the KVM host gets its hostname resolved. 192.168.10.200 networking.nameserver2 (Optional) A second IPv4 address from which the KVM host can get its hostname resolved. Used for high availability. 192.168.10.201 networking.device1 The network interface card from Linux's perspective. Usually enc and then a number that comes from the dev_num of the network adapter. enc100 networking.device2 (Optional) Another Linux network interface card. Usually enc and then a number that comes from the dev_num of the second network adapter. enc1 Important Note # You can skip the rest of the variables on this page IF you are using existing LPAR(s) that has RHEL already installed. Since this is how most production deployments on-prem are done on IBM zSystems, these variables have been marked as optional. With pre-existing LPARs with RHEL installed, you can also skip 1_create_lpar.yaml and 2_create_kvm_host.yaml playbooks. Make sure to still do 0_setup.yaml first though, then skip to 3_setup_kvm_host.yaml 2 - (Optional) CPC & HMC # Variable Name Description Example cpc_name The name of the IBM zSystems / LinuxONE mainframe that you are creating a Red Hat OpenShift Container Platform cluster on. Can be found under the \"Systems Management\" tab of the Hardware Management Console (HMC). SYS1 hmc.host The IPv4 address of the HMC you will be connecting to in order to create a Logical Partition (LPAR) on which will act as the Kernel-based Virtual Machine (KVM) host aftering installing and setting up Red Hat Enterprise Linux (RHEL). 192.168.10.1 hmc.user The username that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmc-user hmc.pass The password that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmcPas$w0rd! 3 - (Optional) LPAR # Variable Name Description Example lpar.name The name of the Logical Partition (LPAR) that you would like to create/target for the creation of your cluster. This LPAR will act as the KVM host, with RHEL installed natively. OCPKVM1 lpar.description A short description of what this LPAR will be used for, will only be displayed in the HMC next to the LPAR name for identification purposes. KVM host LPAR for RHOCP cluster. lpar.access.user The username that will be created in RHEL when it is installed on the LPAR (the KVM host). kvm-admin lpar.access.pass The password for the user that will be created in RHEL when it is installed on the LPAR (the KVM host). ch4ngeMe! lpar.root_pass The root password for RHEL installed on the LPAR (the KVM host). $ecureP4ass! 4 - (Optional) IFL & Memory # Variable Name Description Example lpar.ifl.count Number of Integrated Facilities for Linux (IFL) processors will be assigned to this LPAR. 6 or more recommended. 6 lpar.ifl.initial memory Initial memory allocation for LPAR to have at start-up (in megabytes). 55000 lpar.ifl.max_memory The most amount of memory this LPAR can be using at any one time (in megabytes). 99000 lpar.ifl.initial_weight For LPAR load balancing purposes, the processing weight this LPAR will have at start-up (1-999). 100 lpar.ifl.min_weight For LPAR load balancing purposes, the minimum weight that this LPAR can have at any one time (1-999). 50 lpar.ifl.max_weight For LPAR load balancing purposes, the maximum weight that this LPAR can have at any one time (1-999). 500 5 - (Optional) Networking # Variable Name Description Example lpar.networking.subnet_cidr The same value as the above variable but in Classless Inter- Domain Routing (CIDR) notation. 23 lpar.networking.nic.card1.name The logical name of the Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-01 lpar.networking.nic.card1.adapter The physical adapter name reference to the logical adapter for the LPAR. 10Gb-A lpar.networking.nic.card1.port The port number for the NIC. 0 lpar.networking.nic.card1.dev_num The logical device number for the NIC. In hex format. 0x0100 lpar.networking.nic.card2.name (Optional) The logical name of a second Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-02 lpar.networking.nic.card2.adapter (Optional) The physical adapter name of a second NIC. 10Gb-B lpar.networking.nic.card2.port (Optional) The port number for a second NIC. 1 lpar.networking.nic.card2.dev_num (Optional) The logical device number for a second NIC. In hex format. 0x0001 6 - (Optional) Storage # Variable Name Description Example lpar.storage_group_1.name The name of the storage group that will be attached to the LPAR. OCP-storage-01 lpar.storage_group_1.type Storage type. FCP is the only tested type as of now. fcp lpar.storage_group_1.storage_wwpn World-wide port numbers for storage group. Use provided list formatting. 500708680235c3f0 500708680235c3f1 500708680235c3f2 500708680235c3f3 lpar.storage_group_1.dev_num The logical device number of the Host Bus Adapter (HBA) for the storage group. C001 lpar.storage_group_1.lun_name The Logical Unit Numbers (LUN) that points to a specific virtual disk behind the WWPN. 4200569309ahhd240000000000000c001 lpar.storage_group_2.name (Optional) The name of the storage group that will be attached to the LPAR. OCP-storage-01 lpar.storage_group_2.auto_config (Optional) Attempt to automate the addition of the disk space to the existing logical volume. Check out roles/configure_storage/tasks/main.yaml to ensure this will work properly with your setup. True lpar.storage_group_2.type (Optional) Storage type. FCP is the only tested type as of now. fcp lpar.storage_group_2_.storage_wwpn (Optional) World-wide port numbers for storage group. Use provided list formatting. 500708680235c3f0 500708680235c3f1 500708680235c3f2 500708680235c3f3 lpar.storage_group_2_.dev_num (Optional) The logical device number of the Host Bus Adapter (HBA) for the storage group. C001 lpar.storage_group_2_.lun_name (Optional) he Logical Unit Numbers (LUN) that points to a specific virtual disk behind the WWPN. 4200569309ahhd240000000000000c001","title":"3 Set Variables (host_vars)"},{"location":"set-variables-host-vars/#step-3-set-variables-host_vars","text":"","title":"Step 3: Set Variables (host_vars)"},{"location":"set-variables-host-vars/#overview","text":"Similar to the group_vars file, the host_vars files for each LPAR (KVM host) must be filled in. For each KVM host to be acted upon with Ansible, you must have a corresponding host_vars file named <kvm-hostname>.yaml (i.e. ocpz1.yaml, ocpz2.yaml, ocpz3.yaml), so you must copy and rename the templates found in the host_vars folder accordingly. The variables marked with an X are required to be filled in. Many values are pre-filled or are optional. Optional values are commented out; in order to use them, remove the # and fill them in. Many of the variables in these host_vars files are only required if you are NOT using pre-existing LPARs with RHEL installed. See the Important Note below this first section for more details. This is the most important step in the process. Take the time to make sure everything here is correct. Note on YAML syntax : Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file networking does not need to be filled in, but the hostname does. There are X's where input is required to help you with this. Scroll the table to the right to see examples for each variable.","title":"Overview"},{"location":"set-variables-host-vars/#1-kvm-host","text":"Variable Name Description Example networking.hostname The hostname of the LPAR with RHEL installed natively (the KVM host). kvm-host-01 networking.ip The IPv4 address of the LPAR with RHEL installed natively (the KVM host). 192.168.10.2 networking.subnetmask The subnet that the LPAR resides in within your network. 255.255.255.0 networking.gateway The IPv4 address of the gateway to the network where the KVM host resides. 192.168.10.0 networking.nameserver1 The IPv4 address from which the KVM host gets its hostname resolved. 192.168.10.200 networking.nameserver2 (Optional) A second IPv4 address from which the KVM host can get its hostname resolved. Used for high availability. 192.168.10.201 networking.device1 The network interface card from Linux's perspective. Usually enc and then a number that comes from the dev_num of the network adapter. enc100 networking.device2 (Optional) Another Linux network interface card. Usually enc and then a number that comes from the dev_num of the second network adapter. enc1","title":"1 - KVM Host"},{"location":"set-variables-host-vars/#important-note","text":"You can skip the rest of the variables on this page IF you are using existing LPAR(s) that has RHEL already installed. Since this is how most production deployments on-prem are done on IBM zSystems, these variables have been marked as optional. With pre-existing LPARs with RHEL installed, you can also skip 1_create_lpar.yaml and 2_create_kvm_host.yaml playbooks. Make sure to still do 0_setup.yaml first though, then skip to 3_setup_kvm_host.yaml","title":"Important Note"},{"location":"set-variables-host-vars/#2-optional-cpc-hmc","text":"Variable Name Description Example cpc_name The name of the IBM zSystems / LinuxONE mainframe that you are creating a Red Hat OpenShift Container Platform cluster on. Can be found under the \"Systems Management\" tab of the Hardware Management Console (HMC). SYS1 hmc.host The IPv4 address of the HMC you will be connecting to in order to create a Logical Partition (LPAR) on which will act as the Kernel-based Virtual Machine (KVM) host aftering installing and setting up Red Hat Enterprise Linux (RHEL). 192.168.10.1 hmc.user The username that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmc-user hmc.pass The password that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmcPas$w0rd!","title":"2 - (Optional) CPC &amp; HMC"},{"location":"set-variables-host-vars/#3-optional-lpar","text":"Variable Name Description Example lpar.name The name of the Logical Partition (LPAR) that you would like to create/target for the creation of your cluster. This LPAR will act as the KVM host, with RHEL installed natively. OCPKVM1 lpar.description A short description of what this LPAR will be used for, will only be displayed in the HMC next to the LPAR name for identification purposes. KVM host LPAR for RHOCP cluster. lpar.access.user The username that will be created in RHEL when it is installed on the LPAR (the KVM host). kvm-admin lpar.access.pass The password for the user that will be created in RHEL when it is installed on the LPAR (the KVM host). ch4ngeMe! lpar.root_pass The root password for RHEL installed on the LPAR (the KVM host). $ecureP4ass!","title":"3 - (Optional) LPAR"},{"location":"set-variables-host-vars/#4-optional-ifl-memory","text":"Variable Name Description Example lpar.ifl.count Number of Integrated Facilities for Linux (IFL) processors will be assigned to this LPAR. 6 or more recommended. 6 lpar.ifl.initial memory Initial memory allocation for LPAR to have at start-up (in megabytes). 55000 lpar.ifl.max_memory The most amount of memory this LPAR can be using at any one time (in megabytes). 99000 lpar.ifl.initial_weight For LPAR load balancing purposes, the processing weight this LPAR will have at start-up (1-999). 100 lpar.ifl.min_weight For LPAR load balancing purposes, the minimum weight that this LPAR can have at any one time (1-999). 50 lpar.ifl.max_weight For LPAR load balancing purposes, the maximum weight that this LPAR can have at any one time (1-999). 500","title":"4 - (Optional) IFL &amp; Memory"},{"location":"set-variables-host-vars/#5-optional-networking","text":"Variable Name Description Example lpar.networking.subnet_cidr The same value as the above variable but in Classless Inter- Domain Routing (CIDR) notation. 23 lpar.networking.nic.card1.name The logical name of the Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-01 lpar.networking.nic.card1.adapter The physical adapter name reference to the logical adapter for the LPAR. 10Gb-A lpar.networking.nic.card1.port The port number for the NIC. 0 lpar.networking.nic.card1.dev_num The logical device number for the NIC. In hex format. 0x0100 lpar.networking.nic.card2.name (Optional) The logical name of a second Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-02 lpar.networking.nic.card2.adapter (Optional) The physical adapter name of a second NIC. 10Gb-B lpar.networking.nic.card2.port (Optional) The port number for a second NIC. 1 lpar.networking.nic.card2.dev_num (Optional) The logical device number for a second NIC. In hex format. 0x0001","title":"5 - (Optional) Networking"},{"location":"set-variables-host-vars/#6-optional-storage","text":"Variable Name Description Example lpar.storage_group_1.name The name of the storage group that will be attached to the LPAR. OCP-storage-01 lpar.storage_group_1.type Storage type. FCP is the only tested type as of now. fcp lpar.storage_group_1.storage_wwpn World-wide port numbers for storage group. Use provided list formatting. 500708680235c3f0 500708680235c3f1 500708680235c3f2 500708680235c3f3 lpar.storage_group_1.dev_num The logical device number of the Host Bus Adapter (HBA) for the storage group. C001 lpar.storage_group_1.lun_name The Logical Unit Numbers (LUN) that points to a specific virtual disk behind the WWPN. 4200569309ahhd240000000000000c001 lpar.storage_group_2.name (Optional) The name of the storage group that will be attached to the LPAR. OCP-storage-01 lpar.storage_group_2.auto_config (Optional) Attempt to automate the addition of the disk space to the existing logical volume. Check out roles/configure_storage/tasks/main.yaml to ensure this will work properly with your setup. True lpar.storage_group_2.type (Optional) Storage type. FCP is the only tested type as of now. fcp lpar.storage_group_2_.storage_wwpn (Optional) World-wide port numbers for storage group. Use provided list formatting. 500708680235c3f0 500708680235c3f1 500708680235c3f2 500708680235c3f3 lpar.storage_group_2_.dev_num (Optional) The logical device number of the Host Bus Adapter (HBA) for the storage group. C001 lpar.storage_group_2_.lun_name (Optional) he Logical Unit Numbers (LUN) that points to a specific virtual disk behind the WWPN. 4200569309ahhd240000000000000c001","title":"6 - (Optional) Storage"},{"location":"troubleshooting/","text":"Troubleshooting # If you encounter errors while running the main playbook, there are a few things you can do: Double check your variables. Inspect the part that failed by opening the playbook or role at roles/role-name/tasks/main.yaml Google the specific error message. Re-run the role with the verbosity '-v' option to get more debugging information (more v's give more info). For example: ansible-playbook playbooks/setup_bastion.yaml -vvv Use tags To be more selective with what parts of a playbook are run, use tags. To determine what part of a playbook you would like to run, open the playbook you'd like to run and find the roles parameter. Each role has a corresponding tag. There are also occasionally tags for sections of a playbook or within the role themselves. This is especially helpful for troubleshooting. You can add in tags under the name parameter for individual tasks you'd like to run. Here's an example of using a tag: ansible-playbook playbooks/setup_kvm_host.yaml --tags \"section_2,section_3\" This runs only the parts of the setup_kvm_host playbook marked with tags section_2 and section_3. To use more than one tag, they must be quoted (single or double) and comma-separated (with or without spaces between). E-mail Jacob Emery at jacob.emery@ibm.com If it's a problem with an OpenShift verification step: Open the cockpit to monitor the VMs. In a web browser, go to https://kvm-host-IP-here:9090 Sign-in with your credentials set in the variables file Enable administrative access in the top right. Open the 'Virtual Machines' tab from the left side toolbar. Sometimes it just takes a while, especially if it's lacking resources. Give it some time and then re-reun the playbook/role with tags. If that doesn't work, SSH into the bastion as root (\"ssh root@\\<bastion-ip-address-here>\") and then run, \"export KUBECONFIG=/root/ocpinst/auth/kubeconfig\" and then \"oc whoami\" and make sure it ouputs \"system:admin\". Then run the shell command from the role you would like to check on manually: i.e. 'oc get nodes', 'oc get co', etc. Open the .openshift_install.log file for information on what happened and try to debug the issue.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If you encounter errors while running the main playbook, there are a few things you can do: Double check your variables. Inspect the part that failed by opening the playbook or role at roles/role-name/tasks/main.yaml Google the specific error message. Re-run the role with the verbosity '-v' option to get more debugging information (more v's give more info). For example: ansible-playbook playbooks/setup_bastion.yaml -vvv Use tags To be more selective with what parts of a playbook are run, use tags. To determine what part of a playbook you would like to run, open the playbook you'd like to run and find the roles parameter. Each role has a corresponding tag. There are also occasionally tags for sections of a playbook or within the role themselves. This is especially helpful for troubleshooting. You can add in tags under the name parameter for individual tasks you'd like to run. Here's an example of using a tag: ansible-playbook playbooks/setup_kvm_host.yaml --tags \"section_2,section_3\" This runs only the parts of the setup_kvm_host playbook marked with tags section_2 and section_3. To use more than one tag, they must be quoted (single or double) and comma-separated (with or without spaces between). E-mail Jacob Emery at jacob.emery@ibm.com If it's a problem with an OpenShift verification step: Open the cockpit to monitor the VMs. In a web browser, go to https://kvm-host-IP-here:9090 Sign-in with your credentials set in the variables file Enable administrative access in the top right. Open the 'Virtual Machines' tab from the left side toolbar. Sometimes it just takes a while, especially if it's lacking resources. Give it some time and then re-reun the playbook/role with tags. If that doesn't work, SSH into the bastion as root (\"ssh root@\\<bastion-ip-address-here>\") and then run, \"export KUBECONFIG=/root/ocpinst/auth/kubeconfig\" and then \"oc whoami\" and make sure it ouputs \"system:admin\". Then run the shell command from the role you would like to check on manually: i.e. 'oc get nodes', 'oc get co', etc. Open the .openshift_install.log file for information on what happened and try to debug the issue.","title":"Troubleshooting"}]}