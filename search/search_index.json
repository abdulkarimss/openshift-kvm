{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE # Overview # These Ansible Playbooks automate the setup and deployment of a Red Hat OpenShift Container Platform (RHOCP) cluster on IBM zSystems / LinuxONE with Kernel Virtual Machine (KVM) as the hypervisor. Ready to Start? # Use the left-hand panel to navigate the site. Start with the Before You Begin page. Need Help? # Contact Jacob Emery at jacob.emery@ibm.com","title":"Home"},{"location":"#ansible-automated-openshift-provisioning-on-kvm-on-ibm-zsystems-linuxone","text":"","title":"Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE"},{"location":"#overview","text":"These Ansible Playbooks automate the setup and deployment of a Red Hat OpenShift Container Platform (RHOCP) cluster on IBM zSystems / LinuxONE with Kernel Virtual Machine (KVM) as the hypervisor.","title":"Overview"},{"location":"#ready-to-start","text":"Use the left-hand panel to navigate the site. Start with the Before You Begin page.","title":"Ready to Start?"},{"location":"#need-help","text":"Contact Jacob Emery at jacob.emery@ibm.com","title":"Need Help?"},{"location":"CHANGELOG/","text":"Changelog # All notable changes to this project will be documented in this file. Roadmap # Add option to use a VPN to reduce # of IPs needed Add the ability to provision multiple LPARs for high availability Tag infrastructure nodes for specific operators Add air-gapped (disconnected) install option Add option for OpenShift to use a proxy server Add README\u2019s for each role Make ssh-copy-id role idempotent Documentation Overhaul # Version 0.3.1 Released: 2022-06-03 Summary # Moved documentation to GitHub Pages to be more reader-friendly. Automated KVM Host Provisioning # Version 0.3.0 Released: 2022-03-26 Summary # Now able to provision the KVM host via Ansible. Changed the structure of playbooks, variables, and inventories to use Ansible best practices. Added # Support for using IBM's zHMC Ansible modules to automate the creation of a logical partition (LPAR) profile, connect storage group and network card, boot from an FTP server, and then kickstart the installation of RHEL to serve as the KVM hypervisor for the cluster. Usage of Ansible vault to encrypt sensitive data. Playbooks must now be run with --ask-vault-pass, e.g. 'ansible-playbook playbooks/site.yaml --ask-vault-pass' Modified # Bastion boot method from cloud-init to FTP and kickstart. The structure of playbooks. The setup.yaml playbook still must be run before anything else, but now there is a master playbook - site.yaml which imports all other playbooks. This was done to be more user-friendly and in-line with best practices. Previously, everything was all in one playbook and relied on tags to start back from a given point. Relying solely on tags proved tedious. The structure for inventories, which allows for more flexibility with deployments and is more in-line with best practices. Now you can have multiple inventories and specify which you would like to use for a given run in the ansible.cfg file. The structure of variables, to allow for the separation of the bastion node from the rest of the cluster. This opens up many more possibilities for more complex deployments where, for example, the bastion node is already created. Infrastructure Nodes, Extra Apps, Security # Version: 0.2.1 Released: 2022-01-06 Summary # Now able to designate compute nodes as infrastructure nodes, and create optional RHEL VMs for additional non-cluster applications running on the KVM host. Made changes to SSH and SELinux tasks to be more secure. Added # Support for creating infrastructure nodes and extra apps. Added tcp port 53 to firewall. Setting of permissions and ownership of important configuration files to bastion admin user instead of root. Wheel to groups that bastion admin user is added to on boot. More rounds of checking cluster operators and CSRs in verification steps to ensure the playbook doesn't fail if it takes a long time for those steps to complete. Task to httpd to allow port 4443 because SELinux is no longer set to permissive (see ' Removed ' below). Modified # Formatting of README file to be prettier and more useful. env.yaml to have two sections separated by a comment block: one for variables that need to be filled out, the other for pre-filled variables that can be modified if desired. Ansible user from running as root to an admin user with sudo privileges. Removed # The need to run anything as root user for security reasons. set_selinux_permissive mode role for security reasons. Scaling # Version: 0.2.0 Released: 2021-12-09 Summary # Now supports any number of control and compute nodes to be provisioned in the cluster. This update heavily modifies the variable structure in env.yaml in order to make scaling work. Added # Support for scaling of control and compute nodes. Modified # Variable structure in env.yaml in order to support scaling. Tags to match their corresponding role. Every reference to a variable from env.yaml to match the new structure. Automated OCP Verification # Version: 0.1.1 Released: 2021-12-03 Summary # Fully automated all OCP verification steps. Cutting the number of steps nearly in half. The main playbook can now run completely hands-off from kicking it off all the way to an operational cluster. The last step provides the first-time login credentials. Added # 5 roles related to automating OCP verification steps: wait_for_bootstrap, approve_certs, check_nodes, wait_for_cluster_operators, and wait_for_install_complete. Role to check internal and external DNS configuration before continuing. Including checking to make sure the name resolves to the correct IP address. Modified # The mirrors for CoreOS versions to update to 4.9 and tested them. The acquisition method of RHEL qcow2 from downloading via ephemeral link to having the user download the file to their local machine as a pre-req. This was changed to avoid having to re-copy the link every time it expires. teardown.yaml and reset_files role to be fully idempotent when running the main playbook from the point where each type of teardown sets the user back to. Lots of small tweaks. Removed # Instructions in README for doing OCP verification steps manually Automated Bastion Install # Version: 0.1.0 Released: 2021-11-24 Summary # Fully automated bastion installation and configuration using cloud-init Added # Options in env.yaml for creating a DNS server on the bastion or not, and for automatically attaching Red Hat subscriptions Variables for bootstrap, bastion, control and compute nodes' specifications in env.yaml Node name variables in env.yaml Variable for network interface name in env.yaml Variable for DNS forwarder in env.yaml Templating of DNS configuration files so they don't have to be pre-provided Expect script to ssh_copy_id role so that the user doesn't have to type in ssh password when copying ssh key Templating of haproxy config file A boot_teardown tag in teardown.yaml to automate the teardown of bootstrap node Modified # create_bastion role to use cloud-init to fully automate configuration and installation of the bastion node teardown.yaml script to decrease complexity and work faster. Some tags to match their corresponding role names Lots of small improvements and tweaks Removed # Encryption of env.yaml as it was unnecessary and increased complexity First Working Build # Version: 0.0.1 Released: 2021-08-24 Initial Commit # Version: 0.0.0 Released: 2021-06-11","title":"Change Log"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file.","title":"Changelog"},{"location":"CHANGELOG/#roadmap","text":"Add option to use a VPN to reduce # of IPs needed Add the ability to provision multiple LPARs for high availability Tag infrastructure nodes for specific operators Add air-gapped (disconnected) install option Add option for OpenShift to use a proxy server Add README\u2019s for each role Make ssh-copy-id role idempotent","title":"Roadmap"},{"location":"CHANGELOG/#documentation-overhaul","text":"Version 0.3.1 Released: 2022-06-03","title":"Documentation Overhaul"},{"location":"CHANGELOG/#summary","text":"Moved documentation to GitHub Pages to be more reader-friendly.","title":"Summary"},{"location":"CHANGELOG/#automated-kvm-host-provisioning","text":"Version 0.3.0 Released: 2022-03-26","title":"Automated KVM Host Provisioning"},{"location":"CHANGELOG/#summary_1","text":"Now able to provision the KVM host via Ansible. Changed the structure of playbooks, variables, and inventories to use Ansible best practices.","title":"Summary"},{"location":"CHANGELOG/#added","text":"Support for using IBM's zHMC Ansible modules to automate the creation of a logical partition (LPAR) profile, connect storage group and network card, boot from an FTP server, and then kickstart the installation of RHEL to serve as the KVM hypervisor for the cluster. Usage of Ansible vault to encrypt sensitive data. Playbooks must now be run with --ask-vault-pass, e.g. 'ansible-playbook playbooks/site.yaml --ask-vault-pass'","title":"Added"},{"location":"CHANGELOG/#modified","text":"Bastion boot method from cloud-init to FTP and kickstart. The structure of playbooks. The setup.yaml playbook still must be run before anything else, but now there is a master playbook - site.yaml which imports all other playbooks. This was done to be more user-friendly and in-line with best practices. Previously, everything was all in one playbook and relied on tags to start back from a given point. Relying solely on tags proved tedious. The structure for inventories, which allows for more flexibility with deployments and is more in-line with best practices. Now you can have multiple inventories and specify which you would like to use for a given run in the ansible.cfg file. The structure of variables, to allow for the separation of the bastion node from the rest of the cluster. This opens up many more possibilities for more complex deployments where, for example, the bastion node is already created.","title":"Modified"},{"location":"CHANGELOG/#infrastructure-nodes-extra-apps-security","text":"Version: 0.2.1 Released: 2022-01-06","title":"Infrastructure Nodes, Extra Apps, Security"},{"location":"CHANGELOG/#summary_2","text":"Now able to designate compute nodes as infrastructure nodes, and create optional RHEL VMs for additional non-cluster applications running on the KVM host. Made changes to SSH and SELinux tasks to be more secure.","title":"Summary"},{"location":"CHANGELOG/#added_1","text":"Support for creating infrastructure nodes and extra apps. Added tcp port 53 to firewall. Setting of permissions and ownership of important configuration files to bastion admin user instead of root. Wheel to groups that bastion admin user is added to on boot. More rounds of checking cluster operators and CSRs in verification steps to ensure the playbook doesn't fail if it takes a long time for those steps to complete. Task to httpd to allow port 4443 because SELinux is no longer set to permissive (see ' Removed ' below).","title":"Added"},{"location":"CHANGELOG/#modified_1","text":"Formatting of README file to be prettier and more useful. env.yaml to have two sections separated by a comment block: one for variables that need to be filled out, the other for pre-filled variables that can be modified if desired. Ansible user from running as root to an admin user with sudo privileges.","title":"Modified"},{"location":"CHANGELOG/#removed","text":"The need to run anything as root user for security reasons. set_selinux_permissive mode role for security reasons.","title":"Removed"},{"location":"CHANGELOG/#scaling","text":"Version: 0.2.0 Released: 2021-12-09","title":"Scaling"},{"location":"CHANGELOG/#summary_3","text":"Now supports any number of control and compute nodes to be provisioned in the cluster. This update heavily modifies the variable structure in env.yaml in order to make scaling work.","title":"Summary"},{"location":"CHANGELOG/#added_2","text":"Support for scaling of control and compute nodes.","title":"Added"},{"location":"CHANGELOG/#modified_2","text":"Variable structure in env.yaml in order to support scaling. Tags to match their corresponding role. Every reference to a variable from env.yaml to match the new structure.","title":"Modified"},{"location":"CHANGELOG/#automated-ocp-verification","text":"Version: 0.1.1 Released: 2021-12-03","title":"Automated OCP Verification"},{"location":"CHANGELOG/#summary_4","text":"Fully automated all OCP verification steps. Cutting the number of steps nearly in half. The main playbook can now run completely hands-off from kicking it off all the way to an operational cluster. The last step provides the first-time login credentials.","title":"Summary"},{"location":"CHANGELOG/#added_3","text":"5 roles related to automating OCP verification steps: wait_for_bootstrap, approve_certs, check_nodes, wait_for_cluster_operators, and wait_for_install_complete. Role to check internal and external DNS configuration before continuing. Including checking to make sure the name resolves to the correct IP address.","title":"Added"},{"location":"CHANGELOG/#modified_3","text":"The mirrors for CoreOS versions to update to 4.9 and tested them. The acquisition method of RHEL qcow2 from downloading via ephemeral link to having the user download the file to their local machine as a pre-req. This was changed to avoid having to re-copy the link every time it expires. teardown.yaml and reset_files role to be fully idempotent when running the main playbook from the point where each type of teardown sets the user back to. Lots of small tweaks.","title":"Modified"},{"location":"CHANGELOG/#removed_1","text":"Instructions in README for doing OCP verification steps manually","title":"Removed"},{"location":"CHANGELOG/#automated-bastion-install","text":"Version: 0.1.0 Released: 2021-11-24","title":"Automated Bastion Install"},{"location":"CHANGELOG/#summary_5","text":"Fully automated bastion installation and configuration using cloud-init","title":"Summary"},{"location":"CHANGELOG/#added_4","text":"Options in env.yaml for creating a DNS server on the bastion or not, and for automatically attaching Red Hat subscriptions Variables for bootstrap, bastion, control and compute nodes' specifications in env.yaml Node name variables in env.yaml Variable for network interface name in env.yaml Variable for DNS forwarder in env.yaml Templating of DNS configuration files so they don't have to be pre-provided Expect script to ssh_copy_id role so that the user doesn't have to type in ssh password when copying ssh key Templating of haproxy config file A boot_teardown tag in teardown.yaml to automate the teardown of bootstrap node","title":"Added"},{"location":"CHANGELOG/#modified_4","text":"create_bastion role to use cloud-init to fully automate configuration and installation of the bastion node teardown.yaml script to decrease complexity and work faster. Some tags to match their corresponding role names Lots of small improvements and tweaks","title":"Modified"},{"location":"CHANGELOG/#removed_2","text":"Encryption of env.yaml as it was unnecessary and increased complexity","title":"Removed"},{"location":"CHANGELOG/#first-working-build","text":"Version: 0.0.1 Released: 2021-08-24","title":"First Working Build"},{"location":"CHANGELOG/#initial-commit","text":"Version: 0.0.0 Released: 2021-06-11","title":"Initial Commit"},{"location":"acknowledgements/","text":"Phillip Wilson Filipe Miranda Patrick Fruth Wasif Mohammad Stuart Tener Fred Bader Ken Morse Matt Mondics Miao Zhang-Cohen","title":"Acknowledgements"},{"location":"before-you-begin/","text":"Before You Begin # Description # This project automates the User-Provisioned Infrastructure (UPI) method for deploying Red Hat OpenShift Container Platform (RHOCP) on IBM zSystems / LinuxONE using Kernel-based Virtual Machine (KVM) as the hypervisor. Support # This is an unofficial project created by IBMers. This installation method is not officially supported by either Red Hat or IBM. However, once installation is complete, the resulting cluster is supported by Red Hat. UPI is the only supported method for RHOCP on IBM zSystems. Difficulty # This process is much easier than doing so manually, but still not an easy task. You will likely encounter errors, but you will reach those errors quicker and understand the problem faster than if you were doing this process manually. After using these playbooks once, successive deployments will be much easier. A very basic understanding of what Ansible does is recommended. Advanced understanding is helpful for further customization of the playbooks. A basic understanding of the command-line is required. A basic understanding of git is recommended, especially for creating your organization's own fork of the repository for further customization. An advanced understanding of your computing environment is required for setting the environment variables. These Ansible Playbooks automate a User-Provisioned Infrastructure (UPI) deployment of Red Hat OpenShift Container Platform (RHOCP). This process, when done manually, is extremely tedious, time-consuming, and requires high levels of Linux AND IBM zSystems expertise. UPI is currently the only supported method for deploying RHOCP on IBM zSystems. Why Free and Open-Source? # Trust : IBM zSystems run some of the most highly-secure workloads in the world. Trust is paramount. Developing and using code transparently builds trust between developers and users, so that users feel safe using it on their highly sensitive systems. Customization : IBM zSystems exist in environments that can be highly complex and vary drastically from one datacenter to another. Using code that isn't in a proprietary black box allows you to see exactly what is being done so that you can change any part of it to meet your specific needs. Collaboration : If users encounter a problem, or have a feature request, they can get in contact with the developers directly. Submit an issue or pull request on GitHub or email jacob.emery@ibm.com. Collaboration is highly encouraged! Lower Barriers to Entry : The easier it is to get RHOCP on IBM zSystems up and running, the better - for you, IBM and Red Hat! It is free because RHOCP is an incredible product that should have the least amount of barriers to entry as possible. The world needs open-source, private, and hybrid cloud.","title":"Before You Begin"},{"location":"before-you-begin/#before-you-begin","text":"","title":"Before You Begin"},{"location":"before-you-begin/#description","text":"This project automates the User-Provisioned Infrastructure (UPI) method for deploying Red Hat OpenShift Container Platform (RHOCP) on IBM zSystems / LinuxONE using Kernel-based Virtual Machine (KVM) as the hypervisor.","title":"Description"},{"location":"before-you-begin/#support","text":"This is an unofficial project created by IBMers. This installation method is not officially supported by either Red Hat or IBM. However, once installation is complete, the resulting cluster is supported by Red Hat. UPI is the only supported method for RHOCP on IBM zSystems.","title":"Support"},{"location":"before-you-begin/#difficulty","text":"This process is much easier than doing so manually, but still not an easy task. You will likely encounter errors, but you will reach those errors quicker and understand the problem faster than if you were doing this process manually. After using these playbooks once, successive deployments will be much easier. A very basic understanding of what Ansible does is recommended. Advanced understanding is helpful for further customization of the playbooks. A basic understanding of the command-line is required. A basic understanding of git is recommended, especially for creating your organization's own fork of the repository for further customization. An advanced understanding of your computing environment is required for setting the environment variables. These Ansible Playbooks automate a User-Provisioned Infrastructure (UPI) deployment of Red Hat OpenShift Container Platform (RHOCP). This process, when done manually, is extremely tedious, time-consuming, and requires high levels of Linux AND IBM zSystems expertise. UPI is currently the only supported method for deploying RHOCP on IBM zSystems.","title":"Difficulty"},{"location":"before-you-begin/#why-free-and-open-source","text":"Trust : IBM zSystems run some of the most highly-secure workloads in the world. Trust is paramount. Developing and using code transparently builds trust between developers and users, so that users feel safe using it on their highly sensitive systems. Customization : IBM zSystems exist in environments that can be highly complex and vary drastically from one datacenter to another. Using code that isn't in a proprietary black box allows you to see exactly what is being done so that you can change any part of it to meet your specific needs. Collaboration : If users encounter a problem, or have a feature request, they can get in contact with the developers directly. Submit an issue or pull request on GitHub or email jacob.emery@ibm.com. Collaboration is highly encouraged! Lower Barriers to Entry : The easier it is to get RHOCP on IBM zSystems up and running, the better - for you, IBM and Red Hat! It is free because RHOCP is an incredible product that should have the least amount of barriers to entry as possible. The world needs open-source, private, and hybrid cloud.","title":"Why Free and Open-Source?"},{"location":"first-time-login/","text":"Step 6: First-Time Login # The last step of the main playbook will print a URL, username and temporary password for first-time login. Use a web-browser to type in the URL, which should take you to a sign-in page. Use the provided credentials to sign in. You will have to bypass a warning screen. Congratulations! Your OpenShift cluster installation is now complete.","title":"6 First-Time Login"},{"location":"first-time-login/#step-6-first-time-login","text":"The last step of the main playbook will print a URL, username and temporary password for first-time login. Use a web-browser to type in the URL, which should take you to a sign-in page. Use the provided credentials to sign in. You will have to bypass a warning screen. Congratulations! Your OpenShift cluster installation is now complete.","title":"Step 6: First-Time Login"},{"location":"get-pull-secret/","text":"Step 2: Get Pull Secret # In a web browser, navigate to Red Hat's Hybrid Cloud Console , click the text that says 'Copy pull secret' and save it for the next step.","title":"2 Get Pull Secret"},{"location":"get-pull-secret/#step-2-get-pull-secret","text":"In a web browser, navigate to Red Hat's Hybrid Cloud Console , click the text that says 'Copy pull secret' and save it for the next step.","title":"Step 2: Get Pull Secret"},{"location":"get-the-repository/","text":"Step 1: Get the Repository # Open the terminal On MacOS: cmd+space to open spotlight search, type in 'terminal' and hit enter Navigate to a folder (AKA directory) where you would like to store this project. Either do so graphically, or use the command-line. Here are some helpful commands for doing so: pwd to see what directory you're currently in ls to list child directories cd <folder-name> to change directories ( cd .. to go up to the parent directory) mkdir <new-folder-name> to create a new directory Copy/paste the following and hit enter: git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Open the newly created folder ( cd ) All in all, it will look something like this: $ pwd /Users/example-user $ mkdir ansible-project $ cd ansible-project/ $ git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Cloning into 'Ansible-OpenShift-Provisioning'... remote: Enumerating objects: 3472, done. remote: Counting objects: 100% (200/200), done. remote: Compressing objects: 100% (57/57), done. remote: Total 3472 (delta 152), reused 143 (delta 143), pack-reused 3272 Receiving objects: 100% (3472/3472), 506.29 KiB | 1.27 MiB/s, done. Resolving deltas: 100% (1699/1699), done. $ ls Ansible-OpenShift-Provisioning $ cd Ansible-OpenShift-Provisioning/ $ ls CHANGELOG.md README.md docs mkdocs.yaml roles LICENSE ansible.cfg inventories playbooks All you need to run Ansible is a terminal and a text editor. However, an IDE like VS Code is highly recommended for an integrated, user-friendly experience with helpful extensions like YAML .","title":"1 Get the repository"},{"location":"get-the-repository/#step-1-get-the-repository","text":"Open the terminal On MacOS: cmd+space to open spotlight search, type in 'terminal' and hit enter Navigate to a folder (AKA directory) where you would like to store this project. Either do so graphically, or use the command-line. Here are some helpful commands for doing so: pwd to see what directory you're currently in ls to list child directories cd <folder-name> to change directories ( cd .. to go up to the parent directory) mkdir <new-folder-name> to create a new directory Copy/paste the following and hit enter: git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Open the newly created folder ( cd ) All in all, it will look something like this: $ pwd /Users/example-user $ mkdir ansible-project $ cd ansible-project/ $ git clone https://github.com/IBM/Ansible-OpenShift-Provisioning.git Cloning into 'Ansible-OpenShift-Provisioning'... remote: Enumerating objects: 3472, done. remote: Counting objects: 100% (200/200), done. remote: Compressing objects: 100% (57/57), done. remote: Total 3472 (delta 152), reused 143 (delta 143), pack-reused 3272 Receiving objects: 100% (3472/3472), 506.29 KiB | 1.27 MiB/s, done. Resolving deltas: 100% (1699/1699), done. $ ls Ansible-OpenShift-Provisioning $ cd Ansible-OpenShift-Provisioning/ $ ls CHANGELOG.md README.md docs mkdocs.yaml roles LICENSE ansible.cfg inventories playbooks All you need to run Ansible is a terminal and a text editor. However, an IDE like VS Code is highly recommended for an integrated, user-friendly experience with helpful extensions like YAML .","title":"Step 1: Get the Repository"},{"location":"prerequisites/","text":"Prerequisites # Red Hat # Account ( Sign Up ) License or free trial of Red Hat OpenShift Container Platform for IBM Z systems - s390x architecture (comes with the required licenses for Red Hat Enterprise Linux (RHEL) and CoreOS) IBM zSystems # Hardware Management Console (HMC) access on IBM zSystems or LinuxONE Must be in Dynamic Partition Manager (DPM) mode in order to use the playbook that automates the creation of the KVM host. If DPM mode is not an option for your environment, that playbook can be skipped, but a bare-metal RHEL server must be set-up on an LPAR manually (Filipe Miranda's how-to article ) before moving on. Once that is done, continue with the playbook that sets up the KVM host. For a minimum installation, at least: 6 Integrated Facilities for Linux (IFLs) with SMT2 enabled 85 GB of RAM An FCP storage group created with 1 TB of disk space 8 IPv4 addresses FTP Server # On the same network as your IBM zSystems / LinuxONE hardware. Red Hat Enterprise Linux (RHEL) 8 for s390x architecture mounted in an accessible folder (e.g. /home/ftpuser/rhel/) If you do not yet have RHEL for s390x, go to the Red Hat Customer Portal and download it. Under 'Product Variant' use the drop-down menu to select 'Red Hat Enterprise Linux for IBM z Systems' Double-check it's for version 8 and for s390x architecture Then scroll down to Red Hat Enterprise Linux 8.x Binary DVD and click on the 'Download Now' button. A folder to store config files (e.g. /home/ftpuser/ocp-config) Workstation # A computer/virtual machine with MacOS or Linux operating system Network access to your IBM zSystems / LinuxONE hardware Python3 installed: brew install python3 #MacOS, see note below sudo dnf install python3 #Fedora sudo apt install python3 #Debian Once Python3 is installed, you also need Ansible : pip3 install ansible Note: If you are using MacOS, you also need to have Homebrew package manager installed: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" and Xcode : xcode-select --install","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#red-hat","text":"Account ( Sign Up ) License or free trial of Red Hat OpenShift Container Platform for IBM Z systems - s390x architecture (comes with the required licenses for Red Hat Enterprise Linux (RHEL) and CoreOS)","title":"Red Hat"},{"location":"prerequisites/#ibm-zsystems","text":"Hardware Management Console (HMC) access on IBM zSystems or LinuxONE Must be in Dynamic Partition Manager (DPM) mode in order to use the playbook that automates the creation of the KVM host. If DPM mode is not an option for your environment, that playbook can be skipped, but a bare-metal RHEL server must be set-up on an LPAR manually (Filipe Miranda's how-to article ) before moving on. Once that is done, continue with the playbook that sets up the KVM host. For a minimum installation, at least: 6 Integrated Facilities for Linux (IFLs) with SMT2 enabled 85 GB of RAM An FCP storage group created with 1 TB of disk space 8 IPv4 addresses","title":"IBM zSystems"},{"location":"prerequisites/#ftp-server","text":"On the same network as your IBM zSystems / LinuxONE hardware. Red Hat Enterprise Linux (RHEL) 8 for s390x architecture mounted in an accessible folder (e.g. /home/ftpuser/rhel/) If you do not yet have RHEL for s390x, go to the Red Hat Customer Portal and download it. Under 'Product Variant' use the drop-down menu to select 'Red Hat Enterprise Linux for IBM z Systems' Double-check it's for version 8 and for s390x architecture Then scroll down to Red Hat Enterprise Linux 8.x Binary DVD and click on the 'Download Now' button. A folder to store config files (e.g. /home/ftpuser/ocp-config)","title":"FTP Server"},{"location":"prerequisites/#workstation","text":"A computer/virtual machine with MacOS or Linux operating system Network access to your IBM zSystems / LinuxONE hardware Python3 installed: brew install python3 #MacOS, see note below sudo dnf install python3 #Fedora sudo apt install python3 #Debian Once Python3 is installed, you also need Ansible : pip3 install ansible Note: If you are using MacOS, you also need to have Homebrew package manager installed: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" and Xcode : xcode-select --install","title":"Workstation"},{"location":"run-setup-playbook/","text":"Step 4: Run Setup Playbook # Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). Run this shell command: ansible-playbook playbooks/setup.yaml","title":"4 Run Setup Playbook"},{"location":"run-setup-playbook/#step-4-run-setup-playbook","text":"Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). Run this shell command: ansible-playbook playbooks/setup.yaml","title":"Step 4: Run Setup Playbook"},{"location":"run-the-playbooks/","text":"Step 5: Run the Playbooks # Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). To run all the playbooks at once, start the master playbook by running this shell command: ansible-playbook playbooks/site.yaml Alternatively, run each part step-by-step by running one playbook at a time. Here's the list of playbooks to be run in order, also found in playbooks/site.yaml : create_kvm_host setup_kvm_host create_bastion setup_bastion create_nodes ocp_verification Watch Ansible as it completes the installation, correcting errors if they arise. To look at what tasks are running in detail, open the playbook or roles/role-name/tasks/main.yaml If the process fails in error, go through the steps in the troubleshooting page. ansible-playbook playbooks/setup_bastion.yaml --tags 'httpd,get_ocp'","title":"5 Run the Playbooks"},{"location":"run-the-playbooks/#step-5-run-the-playbooks","text":"Navigate to the root folder of the cloned Git repository in your terminal ( ls should show ansible.cfg ). To run all the playbooks at once, start the master playbook by running this shell command: ansible-playbook playbooks/site.yaml Alternatively, run each part step-by-step by running one playbook at a time. Here's the list of playbooks to be run in order, also found in playbooks/site.yaml : create_kvm_host setup_kvm_host create_bastion setup_bastion create_nodes ocp_verification Watch Ansible as it completes the installation, correcting errors if they arise. To look at what tasks are running in detail, open the playbook or roles/role-name/tasks/main.yaml If the process fails in error, go through the steps in the troubleshooting page. ansible-playbook playbooks/setup_bastion.yaml --tags 'httpd,get_ocp'","title":"Step 5: Run the Playbooks"},{"location":"set-variables/","text":"Step 3: Set Variables # In a text editor of your choice, open the environment variables file . This is the master variables file and you will likely reference it many times throughout the process. The default inventory can be found at inventories/default/group_vars/all.yaml . Fill out the variables marked with X to match your specific installation. This is the most important step in the process. Take the time to make sure everything here is correct. Note on YAML syntax : Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file env and z don't need to be filled in, but the cpc_name does. There are X's where input is required to help you with this. Scroll table to the right to see examples for each variable. Variable Name Description Example Section 1 - IBM zSystems env.z.cpc_name The name of the IBM zSystems / LinuxONE mainframe that you are creating a Red Hat OpenShift Container Platform cluster on. Can be found under the \"Systems Management\" tab of the Hardware Management Console (HMC). SYS1 env.z.hmc.host The IPv4 address of the HMC you will be connecting to in order to create a Logical Partition (LPAR) on which will act as the Kernel-based Virtual Machine (KVM) host aftering installing and setting up Red Hat Enterprise Linux (RHEL). 192.168.10.1 env.z.hmc.user The username that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmc-user env.z.hmc.pass The password that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmcPas$w0rd! env.z.lpar.name The name of the Logical Partition (LPAR) that you would like to create/target for the creation of your cluster. This LPAR will act as the KVM host, with RHEL installed natively. OCPKVM1 env.z.lpar.description A short description of what this LPAR will be used for, will only be displayed in the HMC next to the LPAR name for identification purposes. KVM host LPAR for RHOCP cluster. env.z.lpar.access.user The username that will be created in RHEL when it is installed on the LPAR (the KVM host). kvm-admin env.z.lpar.access.pass The password for the user that will be created in RHEL when it is installed on the LPAR (the KVM host). ch4ngeMe! env.z.lpar.root_pass The root password for RHEL installed on the LPAR (the KVM host). $ecureP4ass! env.z.lpar.ifl.count Number of Integrated Facilities for Linux (IFL) processors will be assigned to this LPAR. 6 or more recommended. 6 env.z.lpar.ifl.initial memory Initial memory allocation for LPAR to have at start-up (in megabytes). 55000 env.z.lpar.ifl.max_memory The most amount of memory this LPAR can be using at any one time (in megabytes). 99000 env.z.lpar.ifl.initial_weight For LPAR load balancing purposes, the processing weight this LPAR will have at start-up (1-999). 100 env.z.lpar.ifl.min_weight For LPAR load balancing purposes, the minimum weight that this LPAR can have at any one time (1-999). 50 env.z.lpar.ifl.max_weight For LPAR load balancing purposes, the maximum weight that this LPAR can have at any one time (1-999). 500 env.z.lpar.networking.hostname The hostname of the LPAR with RHEL installed natively (the KVM host). kvm-host-01 env.z.lpar.networking.ip The IPv4 address of the LPAR with RHEL installed natively (the KVM host). 192.168.10.2 env.z.lpar.networking.subnetmask The subnet that the LPAR resides in within your network. 255.255.255.0 env.z.lpar.networking.subnet The same value as the above variable but in Classless Inter-Domain Routing (CIDR) notation. 23 env.z.lpar.networking.gateway The IPv4 address of the gateway to the network where the KVM host resides. 192.168.10.0 env.z.lpar.networking.nameserver1 The IPv4 address from which the KVM host gets its hostname resolved. 192.168.10.200 env.z.lpar.networking.nameserver2 (Optional) A second IPv4 address from which the KVM host can get its hostname resolved. Used for high availability. 192.168.10.200 env.z.lpar.networking.device1 The network interface card from Linux's perspective. Usually enc and then a number that comes from the dev_num of the network adapter. enc100 env.z.lpar.networking.device2 (Optional) Another Linux network interface card. Usually enc and then a number that comes from the dev_num of the second network adapter. enc1 env.z.lpar.networking.nic.card1.name The logical name of the Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-01 env.z.lpar.networking.nic.card1.adapter The physical adapter name reference to the logical adapter for the LPAR. 10Gb-A env.z.lpar.networking.nic.card1.port The port number for the NIC. 0 env.z.lpar.networking.nic.card1.dev_num The logical device number for the NIC. In hex format. 0x0100 env.z.lpar.networking.nic.card2.name (Optional, uncomment and fill-in if you want to attach a second storage group) The logical name of a second Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-02 env.z.lpar.networking.nic.card2.adapter (Optional, uncomment and fill-in if you want to attach a second storage group) The physical adapter name of a second NIC. 10Gb-B env.z.lpar.networking.nic.card2.port (Optional, uncomment and fill-in if you want to attach a second storage group) The port number for a second NIC. 1 env.z.lpar.networking.nic.card2.dev_num (Optional, uncomment and fill-in if you want to attach a second storage group) The logical device number for a second NIC. In hex format. 0x0001 env.z.lpar.storage_group.name The name of the storage group that will be attached to the LPAR. OCP-storage-01 env.z.lpar.storage_group.type Storage type. FCP is the only tested type as of now. fcp env.z.lpar.storage_group.pool_path Set the absolute path to the storage pool within Linux. Recommended /var/lib/libvirt/images /var/lib/libvirt/images env.z.lpar.storage_group.storage_wwpn World-wide port numbers for storage group. Use provided list formatting. 500708680235c3f0 500708680235c3f1 500708680235c3f2 500708680235c3f3 env.z.lpar.storage_group.dev_num The logical device number of the Host Bus Adapter (HBA) for the storage group. C001 env.z.lpar.storage_group.lun_name A list of Logical Unit Numbers (LUN) that point to specific virtual disk behind the WWPN. First in list will be used for boot. mpatha mpathb mpathc mpathd Section 2 - FTP env.ftp.ip IP address for the FTP server that will be used to pass config files and iso to KVM host LPAR and bastion VM during their first boot. 192.168.10.201 env.ftp.user Username to connect to the FTP server. ftp-user env.ftp.pass Password to connect to the FTP server as above user. FTPpa$s! env.ftp.iso_mount_dir Directory path relative to FTP root where RHEL ISO is mounted. If FTP root is /var/ftp/pub and the ISO is mounted at /var/ftp/pub/RHEL/8.5 then this variable would be RHEL/8.5. No slash before or after. RHEL/8.5 env.ftp.iso_mount_dir Directory path relative to FTP root where configuration files can be stored. If FTP root is /var/ftp/pub and you would like to store the configs at /var/ftp/pub/ocpz-config then this variable would be ocpz-config. No slash before or after. ocpz-config Section 3 - RedHat env.redhat.username Red Hat username with a valid license or free trial to Red Hat OpenShift Container Platform (RHOCP), which comes with necessary licenses for Red Hat Enterprise Linux (RHEL) and Red Hat CoreOS (RHCOS). redhat.user env.redhat.password Password to Red Hat above user's account. Used to auto-attach necessary subscriptions to KVM Host, bastion VM, and pull live images for OpenShift. rEdHatPa$s! env.redhat.pull_secret Pull secret for OpenShift, comes from Red Hat's Hybrid Cloud Console . Make sure to enclose in 'single quotes'. '{\"auths\":{\"cloud.openshift .com\":{\"auth\":\"b3Blb ... 4yQQ==\",\"email\":\"redhat. user@gmail.com\"}}}' Section 4 - Bastion env.bastion.create Would you like to create a bastion KVM guest to host essential infrastructure services like DNS, load balancer, firewall, etc? Highly recommended. Can de-select certain services with the env.bastion.options variables below. True or False (boolean). True env.bastion.vm_name Name of the bastion VM. Arbitrary value. bastion env.bastion.resources.disk_size How much of the storage pool would you like to allocate to the bastion (in Gigabytes)? Recommended 30 or more. 30 env.bastion.resources.ram How much memory would you like to allocate the bastion (in megabytes)? Recommended 4096 or more 4096 env.bastion.resources.swap How much swap storage would you like to allocate the bastion (in megabytes)? Recommended 4096 or more. 4096 env.bastion.resources.vcpu How many virtual CPUs would you like to allocate to the bastion? Recommended 4 or more. 4 env.bastion.resources.os_variant Version of Red Hat Enterprise Linux to use for the bastion's operating system. Recommended 8.4 and above. Must match version of mounted ISO on the FTP server. 8.5 env.bastion.networking.ip IPv4 address for the bastion. 192.168.10.3 env.bastion.networking.hostname Hostname of the bastion. Will be combined with env.bastion.networking.base_domain to create a Fully Qualified Domain Name (FQDN). ocpz-bastion env.bastion.networking.subnetmask Subnet of the bastion. 255.255.255.0 env.bastion.networking.gateway What will be the IPv4 address of the bastion's gateway server? 192.168.10.0 env.bastion.networking.nameserver1 IPv4 address of the server that resolves the bastion's hostname. 192.168.10.200 env.bastion.networking.nameserver2 (Optional) A second IPv4 address that resolves the bastion's hostname. 192.168.10.201 env.bastion.networking.interface Name of the networking interface on the bastion from Linux's perspective. Most likely enc1. enc1 env.bastion.networking.base_domain Base domain that, when combined with the hostname, creates a fully-qualified domain name (FQDN) for the bastion? ihost.com env.bastion.access.user What would you like the admin's username to be on the bastion? admin env.bastion.access.pass The password to the bastion's admin user. cH4ngeM3! env.bastion.access.root_pass The root password for the bastion. R0OtPa$s! env.bastion.access.ocp_ssh_key_comment Comment to describe the SSH key used for OCP. Arbitrary value. OCPZ-01 key env.bastion.options.dns Would you like the bastion to host the DNS information for the cluster? True or False (boolean). If false, resolution must come from elsewhere in your environment. Make sure to add IP addresses for KVM hosts, bastion, bootstrap, control, compute nodes, AND api, api-int and *.apps as described here in section \"User-provisioned DNS Requirements\" Table 5. If True this will be done for you in the dns and check_dns roles. True env.bastion.options.loadbalancer.on_bastion Would you like the bastion to host the load balancer (HAProxy) for the cluster? True or False (boolean). If false, this service must be provided elsewhere in your environment, and public and private IP of the load balancer must be provided in the following two variables. True env.bastion.options.loadbalancer.public_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The public IPv4 address for your environment's loadbalancer. api, apps, *.apps must use this. 192.168.10.50 env.bastion.options.loadbalancer.private_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The private IPv4 address for your environment's loadbalancer. api-int must use this. 10.24.17.12 Section 5 - Cluster env.cluster.networking.metadata_name Name to describe the cluster as a whole, can be anything if DNS will be hosted on the bastion. If DNS is not on the bastion, must match your DNS configuration. Will be combined with the base_domain and hostnames to create Fully Qualified Domain Names (FQDN). ocpz env.cluster.networking.base_domain The site name, where is the cluster being hosted? This will be combined with the metadata_name and hostnames to create FQDNs. ihost.com env.cluster.networking.nameserver1 IPv4 address that the cluster get its hostname resolution from. If env.bastion.options.dns is True, this should be the IP address of the bastion. 192.168.10.200 env.cluster.networking.nameserver2 (Optional) A second IPv4 address will the cluster get its hostname resolution from? If env.bastion.options.dns is True, this should be left commented out. 192.168.10.201 env.cluster.networking.forwarder What IPv4 address will be used to make external DNS calls? Can use 1.1.1.1 or 8.8.8.8 as defaults. 8.8.8.8 env.cluster.nodes.bootstrap.disk_size How much disk space do you want to allocate to the bootstrap node (in Gigabytes)? Bootstrap node is temporary and will be brought down automatically when its job completes. 120 or more recommended. 120 env.cluster.nodes.bootstrap.ram How much memory would you like to allocate to the temporary bootstrap node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.bootstrap.vcpu How many virtual CPUs would you like to allocate to the temporary bootstrap node? Recommended 4 or more. 4 env.cluster.nodes.bootstrap.vm_name Name of the temporary bootstrap node VM. Arbitrary value. bootstrap env.cluster.nodes.bootstrap.ip IPv4 address of the temporary bootstrap node. 192.168.10.4 env.cluster.nodes.bootstrap.hostname Hostname of the temporary boostrap node. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). bootstrap-ocpz env.cluster.nodes.control.disk_size How much disk space do you want to allocate to each control node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.control.ram How much memory would you like to allocate to the each control node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.control.vcpu How many virtual CPUs would you like to allocate to each control node? Recommended 4 or more. 4 env.cluster.nodes.control.vm_name Name of the control node VMs. Arbitrary values. Usually no more or less than 3 are used. Must match the total number of IP addresses and hostnames for control nodes. Use provided list format. control-1 control-2 control-3 env.cluster.nodes.control.ip IPv4 address of the control nodes. Use provided list formatting. 192.168.10.5 192.168.10.6 192.168.10.7 env.cluster.nodes.control.hostname Hostnames for control nodes. Must match the total number of IP addresses for control nodes (usually 3). If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). control-01 control-02 control-03 env.cluster.nodes.compute.disk_size How much disk space do you want to allocate to each compute node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.compute.ram How much memory would you like to allocate to the each compute node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.compute.vcpu How many virtual CPUs would you like to allocate to each compute node? Recommended 2 or more. 2 env.cluster.nodes.compute.vm_name Name of the compute node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for compute nodes. Use provided list format. compute-1 compute-2 env.cluster.nodes.compute.ip IPv4 address of the compute nodes. Must match the total number of VM names and hostnames for compute nodes. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.compute.hostname Hostnames for compute nodes. Must match the total number of IP addresses and VM names for compute nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). compute-01 compute-02 env.cluster.nodes.infra.disk_size (Optional) Set up compute nodes that are made for infrastructure workloads (ingress, monitoring, logging)? How much disk space do you want to allocate to each infra node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.infra.ram (Optional) How much memory would you like to allocate to the each infra node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.infra.vcpu (Optional) How many virtual CPUs would you like to allocate to each infra node? Recommended 2 or more. 2 env.cluster.nodes.infra.vm_name (Optional) Name of additional infra node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for infra nodes. Use provided list format. infra-1 infra-2 env.cluster.nodes.infra.ip (Optional) IPv4 address of the infra nodes. This list can be expanded to any number of nodes, minimum 2. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.infra.hostname (Optional) Hostnames for infra nodes. Must match the total number of IP addresses for infra nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). infra-01 infra-02 Section 6 - Misc Optional Settings env.language What language would you like Red Hat Enterprise Linux to use? In UTF-8 language code. Available languages and their corresponding codes can be found here , in the \"Locale\" column of Table 2.1. en_US.UTF-8 env.timezone Which timezone would you like Red Hat Enterprise Linux to use? A list of available timezone options can be found here . America/New_York env.ansible_key_name (Optional) Name of the SSH key that Ansible will use to connect to hosts. ansible-ocpz env.bridge_name (Optional) Name of the macvtap bridge that will be created on the KVM host. macvtap-net env.pkgs.galaxy A list of Ansible Galaxy collections that will be installed during the setup playbook. The collections listed are required. Feel free to add more as needed, just make sure to follow the same list format. community.general env.pkgs.workstation A list of packages that will be installed on the workstation running Ansible during the setup playbook. Feel free to add more as needed, just make sure to follow the same list format. openssh env.pkgs.kvm A list of packages that will be installed on the KVM Host during the setup_kvm_host playbook. Feel free to add more as needed, just make sure to follow the same list format. qemu-kvm env.pkgs.bastion A list of packages that will be installed on the bastion during the setup_bastion playbook. Feel free to add more as needed, just make sure to follow the same list format. haproxy env.openshift.client Link to the mirror for the OpenShift client from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the client, installer and CoreOS versions (below) match. https://mirror.openshift.com /pub/openshift-v4/s390x/clients /ocp/stable/openshift- client-linux.tar.gz env.openshift.installer Link to the mirror for the OpenShift installer from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the client, installer and CoreOS versions (below) match. https://mirror.openshift.com /pub/openshift-v4/s390x /clients/ocp/stable/openshift- install-linux.tar.gz env.coreos.kernel Link to the mirror of the CoreOS kernel to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the OCP client and installer (above) and other CoreOS components versions match. https://mirror.openshift.com /pub/openshift-v4/s390x /dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-kernel-s390x env.coreos.initramfs Link to the mirror of the CoreOS initramfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the OCP client and installer (above) and other CoreOS components versions match. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-initramfs.s390x.img env.coreos.rootfs Link to the mirror of the CoreOS rootfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the OCP client and installer (above) and other CoreOS components versions match. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0- s390x-live-rootfs.s390x.img env.install_config.api_version Kubernetes API version for the cluster. These install_config variables will be passed to the OCP install_config file. This file is templated in the get_ocp role during the setup_bastion playbook. To make more fine-tuned adjustments to the install_config, you can find it at roles/get_ocp/templates/install-config.yaml.j2 v1 env.install_config.compute.architecture Computing architecture for the compute nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.compute.hyperthreading Enable or disable hyperthreading on compute nodes. Recommended enabled. Enabled env.install_config.control.architecture Computing architecture for the control nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.control.hyperthreading Enable or disable hyperthreading on control nodes. Recommended enabled. Enabled env.install_config.cluster_network.cidr IPv4 block in Internal cluster networking in Classless Inter-Domain Routing (CIDR) notation. Recommended to keep as is. 10.128.0.0/14 env.install_config.cluster_network.host_prefix The subnet prefix length to assign to each individual node. For example, if hostPrefix is set to 23 then each node is assigned a /23 subnet out of the given cidr. A hostPrefix value of 23 provides 510 (2^(32 - 23) - 2) pod IP addresses. 23 env.install_config.cluster_network.type The cluster network provider Container Network Interface (CNI) plug-in to install. Either OpenShiftSDN (recommended) or OVNKubernetes. OpenShiftSDN env.install_config.service_network The IP address block for services. The default value is 172.30.0.0/16. The OpenShift SDN and OVN-Kubernetes network providers support only a single IP address block for the service network. An array with an IP address block in CIDR format. 172.30.0.0/16 env.install_config.fips True or False (boolean) for whether or not to use the United States' Federal Information Processing Standards (FIPS). Not yet certified on IBM zSystems. Enclosed in 'single quotes'. 'false' proxy_env.http_proxy (Optional) A proxy URL to use for creating HTTP connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: http://usernamepswd>@ip:port http://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.https_proxy (Optional) A proxy URL to use for creating HTTPS connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: https://username:pswd@ip:port https://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.no_proxy (Optional) A comma-separated list of destination domain names, IP addresses, or other network CIDRs to exclude from proxying. Preface a domain with . to match subdomains only. For example, .y.com matches x.y.com, but not y.com. Use * to bypass the proxy for all listed destinations. example.com, 192.168.10.1","title":"3 Set Variables"},{"location":"set-variables/#step-3-set-variables","text":"In a text editor of your choice, open the environment variables file . This is the master variables file and you will likely reference it many times throughout the process. The default inventory can be found at inventories/default/group_vars/all.yaml . Fill out the variables marked with X to match your specific installation. This is the most important step in the process. Take the time to make sure everything here is correct. Note on YAML syntax : Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file env and z don't need to be filled in, but the cpc_name does. There are X's where input is required to help you with this. Scroll table to the right to see examples for each variable. Variable Name Description Example Section 1 - IBM zSystems env.z.cpc_name The name of the IBM zSystems / LinuxONE mainframe that you are creating a Red Hat OpenShift Container Platform cluster on. Can be found under the \"Systems Management\" tab of the Hardware Management Console (HMC). SYS1 env.z.hmc.host The IPv4 address of the HMC you will be connecting to in order to create a Logical Partition (LPAR) on which will act as the Kernel-based Virtual Machine (KVM) host aftering installing and setting up Red Hat Enterprise Linux (RHEL). 192.168.10.1 env.z.hmc.user The username that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmc-user env.z.hmc.pass The password that the HMC API call will use to connect to the HMC. Must have access to create LPARs, attach storage groups and networking cards. hmcPas$w0rd! env.z.lpar.name The name of the Logical Partition (LPAR) that you would like to create/target for the creation of your cluster. This LPAR will act as the KVM host, with RHEL installed natively. OCPKVM1 env.z.lpar.description A short description of what this LPAR will be used for, will only be displayed in the HMC next to the LPAR name for identification purposes. KVM host LPAR for RHOCP cluster. env.z.lpar.access.user The username that will be created in RHEL when it is installed on the LPAR (the KVM host). kvm-admin env.z.lpar.access.pass The password for the user that will be created in RHEL when it is installed on the LPAR (the KVM host). ch4ngeMe! env.z.lpar.root_pass The root password for RHEL installed on the LPAR (the KVM host). $ecureP4ass! env.z.lpar.ifl.count Number of Integrated Facilities for Linux (IFL) processors will be assigned to this LPAR. 6 or more recommended. 6 env.z.lpar.ifl.initial memory Initial memory allocation for LPAR to have at start-up (in megabytes). 55000 env.z.lpar.ifl.max_memory The most amount of memory this LPAR can be using at any one time (in megabytes). 99000 env.z.lpar.ifl.initial_weight For LPAR load balancing purposes, the processing weight this LPAR will have at start-up (1-999). 100 env.z.lpar.ifl.min_weight For LPAR load balancing purposes, the minimum weight that this LPAR can have at any one time (1-999). 50 env.z.lpar.ifl.max_weight For LPAR load balancing purposes, the maximum weight that this LPAR can have at any one time (1-999). 500 env.z.lpar.networking.hostname The hostname of the LPAR with RHEL installed natively (the KVM host). kvm-host-01 env.z.lpar.networking.ip The IPv4 address of the LPAR with RHEL installed natively (the KVM host). 192.168.10.2 env.z.lpar.networking.subnetmask The subnet that the LPAR resides in within your network. 255.255.255.0 env.z.lpar.networking.subnet The same value as the above variable but in Classless Inter-Domain Routing (CIDR) notation. 23 env.z.lpar.networking.gateway The IPv4 address of the gateway to the network where the KVM host resides. 192.168.10.0 env.z.lpar.networking.nameserver1 The IPv4 address from which the KVM host gets its hostname resolved. 192.168.10.200 env.z.lpar.networking.nameserver2 (Optional) A second IPv4 address from which the KVM host can get its hostname resolved. Used for high availability. 192.168.10.200 env.z.lpar.networking.device1 The network interface card from Linux's perspective. Usually enc and then a number that comes from the dev_num of the network adapter. enc100 env.z.lpar.networking.device2 (Optional) Another Linux network interface card. Usually enc and then a number that comes from the dev_num of the second network adapter. enc1 env.z.lpar.networking.nic.card1.name The logical name of the Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-01 env.z.lpar.networking.nic.card1.adapter The physical adapter name reference to the logical adapter for the LPAR. 10Gb-A env.z.lpar.networking.nic.card1.port The port number for the NIC. 0 env.z.lpar.networking.nic.card1.dev_num The logical device number for the NIC. In hex format. 0x0100 env.z.lpar.networking.nic.card2.name (Optional, uncomment and fill-in if you want to attach a second storage group) The logical name of a second Network Interface Card (NIC) within the HMC. An arbitrary value that is human-readable that points to the NIC. SYS-NIC-02 env.z.lpar.networking.nic.card2.adapter (Optional, uncomment and fill-in if you want to attach a second storage group) The physical adapter name of a second NIC. 10Gb-B env.z.lpar.networking.nic.card2.port (Optional, uncomment and fill-in if you want to attach a second storage group) The port number for a second NIC. 1 env.z.lpar.networking.nic.card2.dev_num (Optional, uncomment and fill-in if you want to attach a second storage group) The logical device number for a second NIC. In hex format. 0x0001 env.z.lpar.storage_group.name The name of the storage group that will be attached to the LPAR. OCP-storage-01 env.z.lpar.storage_group.type Storage type. FCP is the only tested type as of now. fcp env.z.lpar.storage_group.pool_path Set the absolute path to the storage pool within Linux. Recommended /var/lib/libvirt/images /var/lib/libvirt/images env.z.lpar.storage_group.storage_wwpn World-wide port numbers for storage group. Use provided list formatting. 500708680235c3f0 500708680235c3f1 500708680235c3f2 500708680235c3f3 env.z.lpar.storage_group.dev_num The logical device number of the Host Bus Adapter (HBA) for the storage group. C001 env.z.lpar.storage_group.lun_name A list of Logical Unit Numbers (LUN) that point to specific virtual disk behind the WWPN. First in list will be used for boot. mpatha mpathb mpathc mpathd Section 2 - FTP env.ftp.ip IP address for the FTP server that will be used to pass config files and iso to KVM host LPAR and bastion VM during their first boot. 192.168.10.201 env.ftp.user Username to connect to the FTP server. ftp-user env.ftp.pass Password to connect to the FTP server as above user. FTPpa$s! env.ftp.iso_mount_dir Directory path relative to FTP root where RHEL ISO is mounted. If FTP root is /var/ftp/pub and the ISO is mounted at /var/ftp/pub/RHEL/8.5 then this variable would be RHEL/8.5. No slash before or after. RHEL/8.5 env.ftp.iso_mount_dir Directory path relative to FTP root where configuration files can be stored. If FTP root is /var/ftp/pub and you would like to store the configs at /var/ftp/pub/ocpz-config then this variable would be ocpz-config. No slash before or after. ocpz-config Section 3 - RedHat env.redhat.username Red Hat username with a valid license or free trial to Red Hat OpenShift Container Platform (RHOCP), which comes with necessary licenses for Red Hat Enterprise Linux (RHEL) and Red Hat CoreOS (RHCOS). redhat.user env.redhat.password Password to Red Hat above user's account. Used to auto-attach necessary subscriptions to KVM Host, bastion VM, and pull live images for OpenShift. rEdHatPa$s! env.redhat.pull_secret Pull secret for OpenShift, comes from Red Hat's Hybrid Cloud Console . Make sure to enclose in 'single quotes'. '{\"auths\":{\"cloud.openshift .com\":{\"auth\":\"b3Blb ... 4yQQ==\",\"email\":\"redhat. user@gmail.com\"}}}' Section 4 - Bastion env.bastion.create Would you like to create a bastion KVM guest to host essential infrastructure services like DNS, load balancer, firewall, etc? Highly recommended. Can de-select certain services with the env.bastion.options variables below. True or False (boolean). True env.bastion.vm_name Name of the bastion VM. Arbitrary value. bastion env.bastion.resources.disk_size How much of the storage pool would you like to allocate to the bastion (in Gigabytes)? Recommended 30 or more. 30 env.bastion.resources.ram How much memory would you like to allocate the bastion (in megabytes)? Recommended 4096 or more 4096 env.bastion.resources.swap How much swap storage would you like to allocate the bastion (in megabytes)? Recommended 4096 or more. 4096 env.bastion.resources.vcpu How many virtual CPUs would you like to allocate to the bastion? Recommended 4 or more. 4 env.bastion.resources.os_variant Version of Red Hat Enterprise Linux to use for the bastion's operating system. Recommended 8.4 and above. Must match version of mounted ISO on the FTP server. 8.5 env.bastion.networking.ip IPv4 address for the bastion. 192.168.10.3 env.bastion.networking.hostname Hostname of the bastion. Will be combined with env.bastion.networking.base_domain to create a Fully Qualified Domain Name (FQDN). ocpz-bastion env.bastion.networking.subnetmask Subnet of the bastion. 255.255.255.0 env.bastion.networking.gateway What will be the IPv4 address of the bastion's gateway server? 192.168.10.0 env.bastion.networking.nameserver1 IPv4 address of the server that resolves the bastion's hostname. 192.168.10.200 env.bastion.networking.nameserver2 (Optional) A second IPv4 address that resolves the bastion's hostname. 192.168.10.201 env.bastion.networking.interface Name of the networking interface on the bastion from Linux's perspective. Most likely enc1. enc1 env.bastion.networking.base_domain Base domain that, when combined with the hostname, creates a fully-qualified domain name (FQDN) for the bastion? ihost.com env.bastion.access.user What would you like the admin's username to be on the bastion? admin env.bastion.access.pass The password to the bastion's admin user. cH4ngeM3! env.bastion.access.root_pass The root password for the bastion. R0OtPa$s! env.bastion.access.ocp_ssh_key_comment Comment to describe the SSH key used for OCP. Arbitrary value. OCPZ-01 key env.bastion.options.dns Would you like the bastion to host the DNS information for the cluster? True or False (boolean). If false, resolution must come from elsewhere in your environment. Make sure to add IP addresses for KVM hosts, bastion, bootstrap, control, compute nodes, AND api, api-int and *.apps as described here in section \"User-provisioned DNS Requirements\" Table 5. If True this will be done for you in the dns and check_dns roles. True env.bastion.options.loadbalancer.on_bastion Would you like the bastion to host the load balancer (HAProxy) for the cluster? True or False (boolean). If false, this service must be provided elsewhere in your environment, and public and private IP of the load balancer must be provided in the following two variables. True env.bastion.options.loadbalancer.public_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The public IPv4 address for your environment's loadbalancer. api, apps, *.apps must use this. 192.168.10.50 env.bastion.options.loadbalancer.private_ip (Only required if env.bastion.options.loadbalancer.on_bastion is True). The private IPv4 address for your environment's loadbalancer. api-int must use this. 10.24.17.12 Section 5 - Cluster env.cluster.networking.metadata_name Name to describe the cluster as a whole, can be anything if DNS will be hosted on the bastion. If DNS is not on the bastion, must match your DNS configuration. Will be combined with the base_domain and hostnames to create Fully Qualified Domain Names (FQDN). ocpz env.cluster.networking.base_domain The site name, where is the cluster being hosted? This will be combined with the metadata_name and hostnames to create FQDNs. ihost.com env.cluster.networking.nameserver1 IPv4 address that the cluster get its hostname resolution from. If env.bastion.options.dns is True, this should be the IP address of the bastion. 192.168.10.200 env.cluster.networking.nameserver2 (Optional) A second IPv4 address will the cluster get its hostname resolution from? If env.bastion.options.dns is True, this should be left commented out. 192.168.10.201 env.cluster.networking.forwarder What IPv4 address will be used to make external DNS calls? Can use 1.1.1.1 or 8.8.8.8 as defaults. 8.8.8.8 env.cluster.nodes.bootstrap.disk_size How much disk space do you want to allocate to the bootstrap node (in Gigabytes)? Bootstrap node is temporary and will be brought down automatically when its job completes. 120 or more recommended. 120 env.cluster.nodes.bootstrap.ram How much memory would you like to allocate to the temporary bootstrap node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.bootstrap.vcpu How many virtual CPUs would you like to allocate to the temporary bootstrap node? Recommended 4 or more. 4 env.cluster.nodes.bootstrap.vm_name Name of the temporary bootstrap node VM. Arbitrary value. bootstrap env.cluster.nodes.bootstrap.ip IPv4 address of the temporary bootstrap node. 192.168.10.4 env.cluster.nodes.bootstrap.hostname Hostname of the temporary boostrap node. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). bootstrap-ocpz env.cluster.nodes.control.disk_size How much disk space do you want to allocate to each control node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.control.ram How much memory would you like to allocate to the each control node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.control.vcpu How many virtual CPUs would you like to allocate to each control node? Recommended 4 or more. 4 env.cluster.nodes.control.vm_name Name of the control node VMs. Arbitrary values. Usually no more or less than 3 are used. Must match the total number of IP addresses and hostnames for control nodes. Use provided list format. control-1 control-2 control-3 env.cluster.nodes.control.ip IPv4 address of the control nodes. Use provided list formatting. 192.168.10.5 192.168.10.6 192.168.10.7 env.cluster.nodes.control.hostname Hostnames for control nodes. Must match the total number of IP addresses for control nodes (usually 3). If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). control-01 control-02 control-03 env.cluster.nodes.compute.disk_size How much disk space do you want to allocate to each compute node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.compute.ram How much memory would you like to allocate to the each compute node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.compute.vcpu How many virtual CPUs would you like to allocate to each compute node? Recommended 2 or more. 2 env.cluster.nodes.compute.vm_name Name of the compute node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for compute nodes. Use provided list format. compute-1 compute-2 env.cluster.nodes.compute.ip IPv4 address of the compute nodes. Must match the total number of VM names and hostnames for compute nodes. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.compute.hostname Hostnames for compute nodes. Must match the total number of IP addresses and VM names for compute nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). compute-01 compute-02 env.cluster.nodes.infra.disk_size (Optional) Set up compute nodes that are made for infrastructure workloads (ingress, monitoring, logging)? How much disk space do you want to allocate to each infra node (in Gigabytes)? 120 or more recommended. 120 env.cluster.nodes.infra.ram (Optional) How much memory would you like to allocate to the each infra node (in megabytes)? Recommended 16384 or more. 16384 env.cluster.nodes.infra.vcpu (Optional) How many virtual CPUs would you like to allocate to each infra node? Recommended 2 or more. 2 env.cluster.nodes.infra.vm_name (Optional) Name of additional infra node VMs. Arbitrary values. This list can be expanded to any number of nodes, minimum 2. Must match the total number of IP addresses and hostnames for infra nodes. Use provided list format. infra-1 infra-2 env.cluster.nodes.infra.ip (Optional) IPv4 address of the infra nodes. This list can be expanded to any number of nodes, minimum 2. Use provided list formatting. 192.168.10.8 192.168.10.9 env.cluster.nodes.infra.hostname (Optional) Hostnames for infra nodes. Must match the total number of IP addresses for infra nodes. If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN). infra-01 infra-02 Section 6 - Misc Optional Settings env.language What language would you like Red Hat Enterprise Linux to use? In UTF-8 language code. Available languages and their corresponding codes can be found here , in the \"Locale\" column of Table 2.1. en_US.UTF-8 env.timezone Which timezone would you like Red Hat Enterprise Linux to use? A list of available timezone options can be found here . America/New_York env.ansible_key_name (Optional) Name of the SSH key that Ansible will use to connect to hosts. ansible-ocpz env.bridge_name (Optional) Name of the macvtap bridge that will be created on the KVM host. macvtap-net env.pkgs.galaxy A list of Ansible Galaxy collections that will be installed during the setup playbook. The collections listed are required. Feel free to add more as needed, just make sure to follow the same list format. community.general env.pkgs.workstation A list of packages that will be installed on the workstation running Ansible during the setup playbook. Feel free to add more as needed, just make sure to follow the same list format. openssh env.pkgs.kvm A list of packages that will be installed on the KVM Host during the setup_kvm_host playbook. Feel free to add more as needed, just make sure to follow the same list format. qemu-kvm env.pkgs.bastion A list of packages that will be installed on the bastion during the setup_bastion playbook. Feel free to add more as needed, just make sure to follow the same list format. haproxy env.openshift.client Link to the mirror for the OpenShift client from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the client, installer and CoreOS versions (below) match. https://mirror.openshift.com /pub/openshift-v4/s390x/clients /ocp/stable/openshift- client-linux.tar.gz env.openshift.installer Link to the mirror for the OpenShift installer from Red Hat. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the client, installer and CoreOS versions (below) match. https://mirror.openshift.com /pub/openshift-v4/s390x /clients/ocp/stable/openshift- install-linux.tar.gz env.coreos.kernel Link to the mirror of the CoreOS kernel to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the OCP client and installer (above) and other CoreOS components versions match. https://mirror.openshift.com /pub/openshift-v4/s390x /dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-kernel-s390x env.coreos.initramfs Link to the mirror of the CoreOS initramfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the OCP client and installer (above) and other CoreOS components versions match. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0-s390x- live-initramfs.s390x.img env.coreos.rootfs Link to the mirror of the CoreOS rootfs to be used for the bootstrap, control and compute nodes. Feel free to change to a different version, but make sure it is for s390x architecture. Also make sure the OCP client and installer (above) and other CoreOS components versions match. https://mirror.openshift.com /pub/openshift-v4 /s390x/dependencies/rhcos /4.9/latest/rhcos-4.9.0- s390x-live-rootfs.s390x.img env.install_config.api_version Kubernetes API version for the cluster. These install_config variables will be passed to the OCP install_config file. This file is templated in the get_ocp role during the setup_bastion playbook. To make more fine-tuned adjustments to the install_config, you can find it at roles/get_ocp/templates/install-config.yaml.j2 v1 env.install_config.compute.architecture Computing architecture for the compute nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.compute.hyperthreading Enable or disable hyperthreading on compute nodes. Recommended enabled. Enabled env.install_config.control.architecture Computing architecture for the control nodes. Must be s390x for clusters on IBM zSystems. s390x env.install_config.control.hyperthreading Enable or disable hyperthreading on control nodes. Recommended enabled. Enabled env.install_config.cluster_network.cidr IPv4 block in Internal cluster networking in Classless Inter-Domain Routing (CIDR) notation. Recommended to keep as is. 10.128.0.0/14 env.install_config.cluster_network.host_prefix The subnet prefix length to assign to each individual node. For example, if hostPrefix is set to 23 then each node is assigned a /23 subnet out of the given cidr. A hostPrefix value of 23 provides 510 (2^(32 - 23) - 2) pod IP addresses. 23 env.install_config.cluster_network.type The cluster network provider Container Network Interface (CNI) plug-in to install. Either OpenShiftSDN (recommended) or OVNKubernetes. OpenShiftSDN env.install_config.service_network The IP address block for services. The default value is 172.30.0.0/16. The OpenShift SDN and OVN-Kubernetes network providers support only a single IP address block for the service network. An array with an IP address block in CIDR format. 172.30.0.0/16 env.install_config.fips True or False (boolean) for whether or not to use the United States' Federal Information Processing Standards (FIPS). Not yet certified on IBM zSystems. Enclosed in 'single quotes'. 'false' proxy_env.http_proxy (Optional) A proxy URL to use for creating HTTP connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: http://usernamepswd>@ip:port http://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.https_proxy (Optional) A proxy URL to use for creating HTTPS connections outside the cluster. Will be used in the install-config and applied to other Ansible hosts unless set otherwise in no_proxy below. Must follow this pattern: https://username:pswd@ip:port https://ocp-admin:Pa$sw0rd@9.72.10.1:80 proxy_env.no_proxy (Optional) A comma-separated list of destination domain names, IP addresses, or other network CIDRs to exclude from proxying. Preface a domain with . to match subdomains only. For example, .y.com matches x.y.com, but not y.com. Use * to bypass the proxy for all listed destinations. example.com, 192.168.10.1","title":"Step 3: Set Variables"},{"location":"teardown/","text":"Teardown: # If you would like to teardown your VMs, first determine whether you would like to do a full or partial teardown, specified below. Full Teardown : To teardown all the OpenShift KVM guest virtual machines (will not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags full Partial Teardown : To teardown all OpenShift KVM guest virtual machines except the bastion (will also not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags partial To teardown the KVM host, use the delete_partition playbook .","title":"Teardown"},{"location":"teardown/#teardown","text":"If you would like to teardown your VMs, first determine whether you would like to do a full or partial teardown, specified below. Full Teardown : To teardown all the OpenShift KVM guest virtual machines (will not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags full Partial Teardown : To teardown all OpenShift KVM guest virtual machines except the bastion (will also not teardown KVM host) run: ansible-playbook playbooks/teardown.yaml --tags partial To teardown the KVM host, use the delete_partition playbook .","title":"Teardown:"},{"location":"troubleshooting/","text":"Troubleshooting # If you encounter errors while running the main playbook, there are a few things you can do: Double check your variables. Inspect the part that failed by opening the playbook or role at roles/role-name/tasks/main.yaml Google the specific error message. Re-run the role with the verbosity '-v' option to get more debugging information (more v's give more info). For example: ansible-playbook playbooks/setup_bastion.yaml -vvv Teardown the KVM host with the delete_partition.yaml playbook or teardown troublesome KVM guests with the teardown playbooks. Use tags To be more selective with what parts of a playbook are run, use tags. To determine what part of a playbook you would like to run, open the playbook you'd like to run and find the roles parameter. Each role has a corresponding tag. There are also occasionally tags for sections of a playbook or within the role themselves. This is especially helpful for troubleshooting. You can add in tags under the name parameter for individual tasks you'd like to run. Here's an example of using a tag: ansible-playbook playbooks/setup_kvm_host.yaml --tags \"section_2,section_3\" This runs only the parts of the setup_kvm_host playbook marked with tags section_2 and section_3. To use more than one tag, they must be quoted (single or double) and comma-separated (with or without spaces between). E-mail Jacob Emery at jacob.emery@ibm.com If it's a problem with an OpenShift verification step: Open the cockpit to monitor the VMs. In a web browser, go to https://kvm-host-IP-here:9090 Sign-in with your credentials set in the variables file Enable administrative access in the top right. Open the 'Virtual Machines' tab from the left side toolbar. Sometimes it just takes a while, especially if it's lacking resources. Give it some time and then re-reun the playbook/role with tags. If that doesn't work, SSH into the bastion as root (\"ssh root@\\<bastion-ip-address-here>\") and then run, \"export KUBECONFIG=~/ocpinst/auth/kubeconfig\" and then \"oc whoami\" and make sure it ouputs \"system:admin\". Then run the shell command from the role you would like to check on manually: i.e. 'oc get nodes', 'oc get co', etc. Open the .openshift_install.log file for information on what happened and try to debug the issue.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If you encounter errors while running the main playbook, there are a few things you can do: Double check your variables. Inspect the part that failed by opening the playbook or role at roles/role-name/tasks/main.yaml Google the specific error message. Re-run the role with the verbosity '-v' option to get more debugging information (more v's give more info). For example: ansible-playbook playbooks/setup_bastion.yaml -vvv Teardown the KVM host with the delete_partition.yaml playbook or teardown troublesome KVM guests with the teardown playbooks. Use tags To be more selective with what parts of a playbook are run, use tags. To determine what part of a playbook you would like to run, open the playbook you'd like to run and find the roles parameter. Each role has a corresponding tag. There are also occasionally tags for sections of a playbook or within the role themselves. This is especially helpful for troubleshooting. You can add in tags under the name parameter for individual tasks you'd like to run. Here's an example of using a tag: ansible-playbook playbooks/setup_kvm_host.yaml --tags \"section_2,section_3\" This runs only the parts of the setup_kvm_host playbook marked with tags section_2 and section_3. To use more than one tag, they must be quoted (single or double) and comma-separated (with or without spaces between). E-mail Jacob Emery at jacob.emery@ibm.com If it's a problem with an OpenShift verification step: Open the cockpit to monitor the VMs. In a web browser, go to https://kvm-host-IP-here:9090 Sign-in with your credentials set in the variables file Enable administrative access in the top right. Open the 'Virtual Machines' tab from the left side toolbar. Sometimes it just takes a while, especially if it's lacking resources. Give it some time and then re-reun the playbook/role with tags. If that doesn't work, SSH into the bastion as root (\"ssh root@\\<bastion-ip-address-here>\") and then run, \"export KUBECONFIG=~/ocpinst/auth/kubeconfig\" and then \"oc whoami\" and make sure it ouputs \"system:admin\". Then run the shell command from the role you would like to check on manually: i.e. 'oc get nodes', 'oc get co', etc. Open the .openshift_install.log file for information on what happened and try to debug the issue.","title":"Troubleshooting"}]}